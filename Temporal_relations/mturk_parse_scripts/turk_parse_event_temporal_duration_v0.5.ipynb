{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import relevant libraries\n",
    "from collections import defaultdict, deque\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from predpatt import load_conllu\n",
    "from predpatt import PredPatt\n",
    "from predpatt import PredPattOpts\n",
    "from itertools import combinations\n",
    "from collections import Iterable\n",
    "from random import shuffle\n",
    "import csv\n",
    "from factslab.datastructures import ConstituencyTree, DependencyTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Locations:\n",
    "home_dir = \"/Users\"\n",
    "ud_train  =  home_dir + \"/sidvash/Dropbox//facts_lab/veridicality_sid/UD_English/en-ud-train.conllu\"\n",
    "ud_dev  =  home_dir + \"/sidvash/Dropbox/facts_lab/veridicality_sid/UD_English/en-ud-dev.conllu\"\n",
    "ud_test  =  home_dir + \"/sidvash/Dropbox/facts_lab/veridicality_sid/UD_English/en-ud-test.conllu\"\n",
    "ud_data = [ud_train, ud_dev, ud_test]\n",
    "it_happnd = home_dir + \"/sidvash/Dropbox/facts_lab/veridicality_sid/it-happened_eng_ud1.2_07092017.tsv\"\n",
    "\n",
    "#Taken from github - includes doc ids and sent ids\n",
    "ud_train_detailed =  home_dir + \"/sidvash/Dropbox/facts_lab/veridicality_sid/UD_English/en_ewt-ud-train.conllu\"\n",
    "ud_dev_detailed =  home_dir + \"/sidvash/Dropbox/facts_lab/veridicality_sid/UD_English/en_ewt-ud-dev.conllu\"\n",
    "ud_test_detailed =  home_dir + \"/sidvash/Dropbox/facts_lab/veridicality_sid/UD_English/en_ewt-ud-test.conllu\"\n",
    "\n",
    "ud_trees = home_dir + \"/sidvash/Dropbox/facts_lab/UD_data_trees/structures.tsv\"\n",
    "\n",
    "null_str = \"_null_\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Sentence Mappings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['From', 'the', 'AP', 'comes', 'this', 'story', ':']\n"
     ]
    }
   ],
   "source": [
    "struct_dict = {}\n",
    "\n",
    "with open(ud_trees, 'r') as f:\n",
    "    structs_sents = [line.strip().split('\\t') for line in f]\n",
    "\n",
    "for sent_id, tree_list, sent in structs_sents:\n",
    "    struct_dict[sent_id] = DependencyTree.fromstring(tree_list)\n",
    "    struct_dict[sent_id].sentence = sent.split(\" \")\n",
    "    \n",
    "print(struct_dict['en-ud-dev.conllu sent_1'].sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Structures (Dependency Graph object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import DependencyGraph\n",
    "import re\n",
    "ud_path = home_dir + \"/sidvash/Dropbox/facts_lab/veridicality_sid/UD_English/\"\n",
    "\n",
    "def html_ify(s):\n",
    "    '''\n",
    "        Takes care of &quot &lsqb &rsqb &#39\n",
    "    '''\n",
    "    html_string = re.sub(r'\\)', r'&rcrb;', s)\n",
    "    html_string = re.sub(r'\\(', r'&lcrb;', html_string)\n",
    "    return html_string\n",
    "\n",
    "def get_structs(ud_path):\n",
    "    files = ['en-ud-train.conllu', 'en-ud-dev.conllu', 'en-ud-test.conllu']\n",
    "    structures = {}\n",
    "    for file in files:\n",
    "        with open(ud_path + file, 'r') as f:\n",
    "            iden = 0\n",
    "            a = \"\"\n",
    "            words = []\n",
    "            for line in f:\n",
    "                if line != \"\\n\":\n",
    "                    a += line\n",
    "                    words.append(line.split(\"\\t\")[1])\n",
    "                else:\n",
    "                    iden += 1\n",
    "                    a = html_ify(a)\n",
    "                    structure = DependencyGraph(a, top_relation_label='root')\n",
    "                    sent = \" \".join(words)\n",
    "                    sent = html_ify(sent)\n",
    "                    sent_id = file + \" sent_\" + str(iden)\n",
    "                    structures[sent_id] = structure\n",
    "                    a = \"\"\n",
    "                    words = []\n",
    "    return structures\n",
    "\n",
    "structures = get_structs(ud_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Happen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract Sentence IDS of event-happening sentences\n",
    "data = pd.read_csv(it_happnd , sep='\\t')\n",
    "\n",
    "#Select only sentences which did happen\n",
    "happnd = data[data.Happened == \"true\"]\n",
    "\n",
    "#Select only sentences with high confidence\n",
    "happnd = happnd[happnd.Confidence.isin(['4', '3'])]\n",
    "\n",
    "#Select only sentences where Keep = True\n",
    "happnd = happnd[happnd.Keep == True]\n",
    "\n",
    "#Create a set of ID's to filter later\n",
    "happen_set = list(happnd[['Sentence.ID', 'Pred.Token']].values)\n",
    "happen_set = [tuple(x) for x in happen_set]\n",
    "happen_set = set(happen_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for turk_parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_to_html(s):\n",
    "    '''\n",
    "    Make some changes to the input string to make it html readable\n",
    "    '''\n",
    "    #replace special chars as per html format\n",
    "    s = re.sub(r\"\\'\", r\"&#39;\", s)\n",
    "    s = re.sub(r'\\\"', r\"&quot;\", s)\n",
    "    s = re.sub(r\"\\[\", r\"&lsqb;\", s)\n",
    "    s = re.sub(r\"\\]\", r\"&rsqb;\", s)\n",
    "    s = re.sub(r\"\\,\", r\"&#44;\", s)\n",
    "        \n",
    "    return s\n",
    "\n",
    "def replace_to_turk(s):\n",
    "    '''\n",
    "    Make some changes to the input string to make it Turk readable\n",
    "    '''\n",
    "    #replace double quotes to appear twice : except at the start/end of the list\n",
    "    s = re.sub(r'([^\\]])\\\"', r'\\1\"\"', s)\n",
    "    \n",
    "    #replace single quotes at the beginning and end of list\n",
    "    s = re.sub(r\"\\'\\{\", r\"{\", s)  \n",
    "    s = re.sub(r\"\\}\\'\", r\"}\", s)\n",
    "    \n",
    "    #replace two backslash to three\n",
    "    s = re.sub(r\"\\\\\\\\\", r\"\\\\\\\\\\\\\", s)\n",
    "    \n",
    "    #Leave spaces around <span> \n",
    "    s = re.sub(r\"<span\", r\" <span\", s)\n",
    "    s = re.sub(r\" </span>\", r\"</span> \", s)\n",
    "        \n",
    "    return s\n",
    "\n",
    "def load_ud_english(fpath):\n",
    "    \"\"\"Load a file from the UD English corpus\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fpath : str\n",
    "        Path to UD corpus file ending in .conllu\n",
    "    \"\"\"\n",
    "\n",
    "    n = 1\n",
    "\n",
    "    fname = os.path.split(fpath)[1]\n",
    "\n",
    "    parses = defaultdict(list)\n",
    "    sent_ids = []\n",
    "    newdoc_ids = []\n",
    "    \n",
    "    for l in open(fpath):\n",
    "        ident = fname+' '+str(n)\n",
    "        \n",
    "        if re.match(r'\\# newdoc id', l):\n",
    "            newdoc_ids.append(n)\n",
    "            \n",
    "        if re.match(r'^\\d', l):\n",
    "            l_split = l.strip().split()\n",
    "            parses[ident].append(l_split)\n",
    "        \n",
    "        elif parses[ident]:\n",
    "            sent_ids.append(ident)\n",
    "            n += 1\n",
    "\n",
    "    return newdoc_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below Functions are used only if two adjacent sentences are combined into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_preds(pred_tuples):\n",
    "    '''\n",
    "    Input: a list of tuples of (sent_id_num, predicate object)\n",
    "    \n",
    "    Output: filter tuples only with specific pos tags predicates\n",
    "    \n",
    "    '''\n",
    "    ans = []\n",
    "    pos_tags = set([\"ADJ\", \"NOUN\", \"NUM\", \"DET\", \"PROPN\", \"PRON\", \"VERB\", \"AUX\"])\n",
    "    for sent_id, pred_obj in pred_tuples:\n",
    "        if pred_obj.root.tag not in pos_tags:\n",
    "            continue\n",
    "        elif pred_obj.root.tag not in [\"VERB\", \"AUX\"]:\n",
    "            gov_rels = [tok.gov_rel for tok in pred_obj.tokens]\n",
    "            if 'cop' in gov_rels:\n",
    "                ans.append((sent_id, pred_obj))\n",
    "            elif pred_obj.root.tag == 'ADJ':\n",
    "                ans.append((sent_id, pred_obj))\n",
    "        else:\n",
    "            ans.append((sent_id, pred_obj))\n",
    "    return ans\n",
    "\n",
    "def depth_in_tree(idx, dep_obj):\n",
    "    '''\n",
    "    Input: Index of the word in a linear sequence of words\n",
    "    \n",
    "    Output: Depth of that word in the dependency tree\n",
    "    \n",
    "    '''\n",
    "    nodes = dep_obj.nodes\n",
    "    depth = 0\n",
    "    i = idx+1\n",
    "    while nodes[i]['rel'] != 'root':\n",
    "        i = nodes[i]['head']\n",
    "        depth+=1\n",
    "        \n",
    "    return depth\n",
    "            \n",
    "    \n",
    "def find_pivot_predicate(fname, sentid_num, predp_object, structures):\n",
    "    '''\n",
    "    Find the pivot-predicate of a given sentence's id\n",
    "    \n",
    "    Heuristic/Algo:  Follow the root predicate until you find a predicate which doesn't have\n",
    "                any xcomp, ccomp or csubj dependencies.\n",
    "                \n",
    "    '''\n",
    "    preds = filter_preds([(sentid_num, x) for x in predp_object.instances])\n",
    "    tokens = [y.root.position for x, y in preds]\n",
    "    \n",
    "    if tokens:\n",
    "        tokens_covered = set()\n",
    "        \n",
    "        struct_id = fname + \" sent_\" + str(sentid_num)\n",
    "        dep_object = structures[struct_id]\n",
    "        pred_heights = sorted([(x, depth_in_tree(x,dep_object)) for x in tokens], key=lambda x:x[1])\n",
    "        tokens_reverse = [x for x,y in pred_heights][::-1]\n",
    "        \n",
    "        root_idx = tokens.index(pred_heights[0][0])\n",
    "        root_predicate = preds[root_idx]\n",
    "        deps = dep_object.nodes[tokens[root_idx]+1]['deps']\n",
    "        \n",
    "        tokens_covered.add(tokens[root_idx])\n",
    "        tokens_reverse.pop()\n",
    "        \n",
    "        while ('ccomp' in deps) or ('xcomp' in deps) or ('csubj') in deps:\n",
    "            variables = ['ccomp', 'xcomp', 'csubj']\n",
    "            for var in variables:\n",
    "                if var in deps:\n",
    "                    tok_idx = deps[var][0]-1\n",
    "                    if tok_idx in tokens:\n",
    "                        root_idx = tokens.index(tok_idx)\n",
    "                        tokens_covered.add(tokens[root_idx])\n",
    "                        tokens_reverse.pop()\n",
    "                    else:\n",
    "                        if tokens_reverse:\n",
    "                            root_idx = tokens.index(tokens_reverse[-1])\n",
    "                            tokens_covered.add(tokens[root_idx])\n",
    "                            tokens_reverse.pop()\n",
    "                        else:\n",
    "                            return root_predicate\n",
    "                    break\n",
    "                    \n",
    "            deps = dep_object.nodes[tokens[root_idx]+1]['deps']\n",
    "            root_predicate = preds[root_idx]\n",
    "            \n",
    "        return root_predicate \n",
    "\n",
    "    return []\n",
    "\n",
    "def predicate_info(predicate, sent_id):\n",
    "    '''\n",
    "    Input: predicate object\n",
    "    Output: pred_text, token, root_token\n",
    "    \n",
    "    Note: If predicate is copular: pred_text is only upto first 5 words\n",
    "    '''       \n",
    "    #Extend predicate to start from the copula\n",
    "    if predicate.root.tag not in [\"VERB\", \"AUX\"]:\n",
    "        all_pred = predicate.tokens\n",
    "        gov_rels = [tok.gov_rel for tok in all_pred]\n",
    "        if 'cop' in gov_rels:\n",
    "            cop_pos = gov_rels.index('cop')\n",
    "            pred = [x.text for x in all_pred[cop_pos:]]\n",
    "            pred_token = [x.position for x in all_pred[cop_pos:]]\n",
    "            def_pred_token = predicate.root.position  #needed for it_happen set\n",
    "            cop_bool = True  \n",
    "        elif predicate.root.tag == \"ADJ\":\n",
    "            pred_token = [predicate.root.position]\n",
    "            pred = [predicate.root.text]\n",
    "            def_pred_token = predicate.root.position\n",
    "        else:\n",
    "            print(\"Incompatible predicate found\")\n",
    "            \n",
    "    #Else keep the root        \n",
    "    else:\n",
    "        pred_token = [predicate.root.position]\n",
    "        pred = [predicate.root.text]\n",
    "        def_pred_token = predicate.root.position \n",
    "\n",
    "    #Stringify pred and pred_tokens:\n",
    "    #pred_token = \"_\".join(map(str, pred_token))\n",
    "\n",
    "    if len(pred)>5:\n",
    "        pred = pred[:5]\n",
    "        pred = \" \".join(pred) + \"...\"\n",
    "    else:\n",
    "        pred = \" \".join(pred)\n",
    "        \n",
    "    return pred, pred_token, def_pred_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_pred_double(pred_comb, raw_sentence, fname, sentid_num, sentid_num_next, happen_set=[]):\n",
    "    '''\n",
    "    Extract turk_parse dict from input predicate combination \n",
    "    \n",
    "    INputs:\n",
    "    1. pred_all : one list of all predicates in both sentences\n",
    "    2. raw_sentence: a dict of two sentences, with key: sent_id_num\n",
    "    3. sentid_num: 1st sentence in adjacent sentence\n",
    "    4. sentid_num_next: 2nd sentence in adjacent sentence\n",
    "    \n",
    "    '''\n",
    "    token_dict = {}\n",
    "    pred1_obj, pred2_obj = [y for x,y in pred_comb]\n",
    "    sent_id1, sent_id2 = [x for x,y in pred_comb]\n",
    "    \n",
    "    pred1_text, pred1_token, pred1_root_token = predicate_info(pred1_obj, sentid_num)\n",
    "    pred2_text, pred2_token, pred2_root_token = predicate_info(pred2_obj, sentid_num_next)\n",
    "\n",
    "    token_dict['pred1_token'] = \"_\".join(map(str, pred1_token))\n",
    "    token_dict['pred1_text'] = pred1_text\n",
    "    token_dict['pred2_token'] = \"_\".join(map(str, pred2_token))\n",
    "    token_dict['pred2_text'] = pred2_text\n",
    "    token_dict['sentence_id_1'] = fname + \" \" + sent_id1\n",
    "    token_dict['sentence_id_2'] = fname + \" \" + sent_id2\n",
    "    token_dict['pred1_root_token'] = pred1_root_token\n",
    "    token_dict['pred2_root_token'] = pred2_root_token\n",
    "    \n",
    "    ## Raw Sentence:\n",
    "    #Extract each predicate's position and text\n",
    "    pred_sent1 = raw_sentence[sentid_num].copy()\n",
    "    pred_sent2 = raw_sentence[sentid_num_next].copy()\n",
    "    \n",
    "    if sent_id1 == sent_id2 == sentid_num:\n",
    "        acc=0\n",
    "        for ins in pred1_token:\n",
    "            pred_sent1.insert(ins + acc, ' <span class=\\\"predicate1\\\">' + '<sup>1</sup>')\n",
    "            pred_sent1.insert(ins + acc + 2, '</span> ')\n",
    "            acc += 2\n",
    "            \n",
    "        for ins in pred2_token:\n",
    "            pred_sent1.insert(ins + acc, ' <span class=\\\"predicate2\\\">' + '<sup>2</sup>')\n",
    "            pred_sent1.insert(ins + acc + 2, '</span> ')\n",
    "            acc += 2\n",
    "            \n",
    "    elif sent_id1 == sent_id2 == sentid_num_next:\n",
    "        acc=0\n",
    "        for ins in pred1_token:\n",
    "            pred_sent2.insert(ins + acc, ' <span class=\\\"predicate1\\\">' + '<sup>1</sup>')\n",
    "            pred_sent2.insert(ins + acc + 2, '</span> ')\n",
    "            acc += 2\n",
    "            \n",
    "        for ins in pred2_token:\n",
    "            pred_sent2.insert(ins + acc, ' <span class=\\\"predicate2\\\">' + '<sup>2</sup>')\n",
    "            pred_sent2.insert(ins + acc + 2, '</span> ')\n",
    "            acc += 2\n",
    "        \n",
    "    else:\n",
    "        acc=0\n",
    "        for ins in pred1_token:\n",
    "            pred_sent1.insert(ins + acc, ' <span class=\\\"predicate1\\\">' + '<sup>1</sup>')\n",
    "            pred_sent1.insert(ins + acc + 2, '</span> ')\n",
    "            acc += 2\n",
    "            \n",
    "        acc=0\n",
    "        for ins in pred2_token:\n",
    "            pred_sent2.insert(ins + acc, ' <span class=\\\"predicate2\\\">' + '<sup>2</sup>')\n",
    "            pred_sent2.insert(ins + acc + 2, '</span> ')\n",
    "            acc += 2\n",
    "            \n",
    "    pred_sentence = pred_sent1 + pred_sent2\n",
    "    token_dict['sentence'] = \" \".join(pred_sentence)\n",
    "        \n",
    "    return token_dict, pred1_token, pred2_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_list_adjacent(ud_data, happen_set, doc_id_dict, cut_option = True):\n",
    "    '''\n",
    "    Extract a list of JSON objects from the ud data\n",
    "    \n",
    "    #Difference from original: Combining two adjacent sentences to form a single one\n",
    "    \n",
    "    Input: \n",
    "    1. ud data path ending in .conll\n",
    "    2. happen_set: a set of sentence_id where the event did happen\n",
    "    \n",
    "    '''\n",
    "    global_list = []\n",
    "    local_list = []\n",
    "    iden = 1\n",
    "    sent_removed = 0\n",
    "    sent_total = 0\n",
    "    total_preds = 0\n",
    "    filtered_preds = 0\n",
    "    single_preds=0\n",
    "    global_tuples = []\n",
    "    \n",
    "    # Resolve relative clause\n",
    "    options = PredPattOpts(resolve_relcl=True, borrow_arg_for_relcl=True, resolve_conj=False, cut=cut_option)\n",
    "    \n",
    "    #Predicate tags to be used later  \n",
    "    copula_cnts = defaultdict(int)\n",
    "    pred_cnts = defaultdict(int)\n",
    "    auxverb_cnts = defaultdict(int)\n",
    "    ignore_cnts =  defaultdict(int)\n",
    "    copula_indxs = defaultdict(list)\n",
    "    discont_indxs = defaultdict(list)\n",
    "    ###\n",
    "    \n",
    "    for ud_data_path in ud_data:\n",
    "        covered_set = set()\n",
    "        fname = ud_data_path.split(\"/\")[-1]\n",
    "        data_name = ud_data_path.split(\".\")[0].split(\"-\")[-1]\n",
    "        doc_ids = doc_id_dict[data_name]\n",
    "        \n",
    "        with open(ud_data_path) as infile:\n",
    "            data = replace_to_html(infile.read())\n",
    "            parsed = [(PredPatt(ud_parse, opts=options), sent_id) for sent_id, ud_parse in load_conllu(data)]\n",
    "        \n",
    "        total_sents = len(parsed)\n",
    "        \n",
    "        docslist = deque(doc_ids[1:])\n",
    "        nextdoc = docslist.popleft()\n",
    "        \n",
    "        #Iterating through each parsed sentence\n",
    "        for i, parse_sen in enumerate(parsed):   \n",
    "            \n",
    "            total_preds += len(parse_sen[0].instances)\n",
    "            \n",
    "            if i in covered_set:\n",
    "                continue\n",
    "            \n",
    "            if docslist:\n",
    "                if nextdoc in covered_set:\n",
    "                    nextdoc = docslist.popleft()\n",
    "                \n",
    "            pred_object = parse_sen[0] \n",
    "            \n",
    "            \n",
    "            sentid_num = parse_sen[1].split(\"_\")[-1]\n",
    "            ##########################################\n",
    "            #Next item in parsed is not a new document\n",
    "             ##########################################\n",
    "            if (i+2) != nextdoc and (i+1)!= total_sents:\n",
    "                covered_set.add(i)\n",
    "                #covered_set.add(i+1)\n",
    "                \n",
    "                parse_sen_next = parsed[i+1]\n",
    "                pred_object_next = parse_sen_next[0]\n",
    "                sentid_num_next = parse_sen_next[1].split(\"_\")[-1]\n",
    "            \n",
    "                raw_sentence =  {sentid_num : [token.text for token in pred_object.tokens] ,\n",
    "                                sentid_num_next: [token.text for token in pred_object_next.tokens]}\n",
    "                \n",
    "                preds_curr = filter_preds([(sentid_num,pred) for pred in pred_object.instances])\n",
    "                preds_next = filter_preds([(sentid_num_next,pred) for pred in pred_object_next.instances])\n",
    "                \n",
    "                filtered_preds += len(preds_curr)\n",
    "                \n",
    "                pred_combs_curr = combinations(preds_curr,2)\n",
    "                \n",
    "                #Curr_sent combinations\n",
    "                for pred_comb in pred_combs_curr:      \n",
    "                    #token dict from all predicates in both sentences:\n",
    "                    token_dict, pred_token1, pred_token2 = dict_pred_double(pred_comb, raw_sentence, \n",
    "                                                                                  fname, sentid_num, \n",
    "                                                                                    sentid_num_next)\n",
    "                    global_tuples.append((token_dict, pred_token1, pred_token2))\n",
    "                    sent_total+=1\n",
    "                \n",
    "                #Root pred of curr_sent with preds of next_sent:\n",
    "                pivot_curr_pred = find_pivot_predicate(fname, sentid_num, pred_object, structures)\n",
    "                if pivot_curr_pred:\n",
    "                    for tupl in preds_next:\n",
    "                        pred_comb = [pivot_curr_pred, tupl]\n",
    "                        token_dict, pred_token1, pred_token2 = dict_pred_double(pred_comb, raw_sentence, \n",
    "                                                                                      fname, sentid_num, \n",
    "                                                                                        sentid_num_next)\n",
    "                        global_tuples.append((token_dict, pred_token1, pred_token2))\n",
    "                        sent_total+=1\n",
    "                \n",
    "            ##########################################\n",
    "            #Next item in parsed is from a new document\n",
    "             ##########################################\n",
    "            else: \n",
    "                if docslist:\n",
    "                    nextdoc = docslist.popleft()\n",
    "                covered_set.add(i)\n",
    "                \n",
    "        print(\"{} finished\".format(data_name))\n",
    "        print(\"\\n\")\n",
    "    #Append the last remaining sentences into the global list\n",
    "    global_list.append(local_list)\n",
    "    \n",
    "    print(\"Total number of sent_tokens found: {}\".format(sent_total))\n",
    "    print(\"Total number of predicates found: {}\".format(total_preds))\n",
    "    print(\"Total number of filtered predicates: {}\".format(filtered_preds))\n",
    "    print(\"Number of sentences removed due to non-event: {}\".format(sent_removed))\n",
    "    \n",
    "                    \n",
    "    return global_tuples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createHITs(global_tuples, offset_max=12):\n",
    "    '''\n",
    "    Create HITs based on input global tuples\n",
    "    '''\n",
    "    shuffle(global_tuples)\n",
    "    total_sents = 0\n",
    "    iden=0\n",
    "    global_list = [] \n",
    "    local_list = []\n",
    "    global_sents = 0\n",
    "    global_ids = []\n",
    "    total_len = len(global_tuples)\n",
    "    rem = total_len%offset_max\n",
    "    \n",
    "    for j, tupl in enumerate(global_tuples):\n",
    "        \n",
    "        token_dict, pred_token1, pred_token2 = tupl\n",
    "            \n",
    "        total_sents += 1\n",
    "        global_sents +=1\n",
    "            \n",
    "        if rem < offset_max//2:\n",
    "            if j == total_len-(rem+1):\n",
    "                offset_max+=rem\n",
    "        \n",
    "        if total_sents >= offset_max:\n",
    "            iden += 1\n",
    "            token_dict['id'] = iden\n",
    "            local_list.append(json.dumps(token_dict))\n",
    "            global_ids.append(iden)\n",
    "            global_list.append(local_list)\n",
    "            local_list = []\n",
    "            iden=0\n",
    "            total_sents=0\n",
    "            \n",
    "        else:\n",
    "            iden += 1\n",
    "            token_dict['id'] = iden\n",
    "            local_list.append(json.dumps(token_dict))\n",
    "            \n",
    "    #Append the last remaining sentences into the global list if rem was not used.\n",
    "    if local_list:\n",
    "        global_list.append(local_list)\n",
    "    print(\"Total number of predicate-pairs: {}\".format(global_sents))\n",
    "    print(\"Average number of sentences per HIT {}\".format(np.mean([len(x) for x in global_list])))\n",
    "    print(\"Max number of sentences per HIT {}\".format(np.max([len(x) for x in global_list])))\n",
    "    print(\"Min number of sentences per HIT {}\".format(np.min([len(x) for x in global_list])))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    return global_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining adjacent sentences into a single one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ids = {}\n",
    "doc_ids['train'] = load_ud_english(ud_train_detailed)\n",
    "doc_ids['dev'] = load_ud_english(ud_dev_detailed)\n",
    "doc_ids['test'] = load_ud_english(ud_test_detailed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train finished\n",
      "\n",
      "\n",
      "Total number of sent_tokens found: 59593\n",
      "Total number of predicates found: 27390\n",
      "Total number of filtered predicates: 25413\n",
      "Number of sentences removed due to non-event: 0\n",
      "dev finished\n",
      "\n",
      "\n",
      "Total number of sent_tokens found: 5638\n",
      "Total number of predicates found: 3345\n",
      "Total number of filtered predicates: 2782\n",
      "Number of sentences removed due to non-event: 0\n",
      "test finished\n",
      "\n",
      "\n",
      "Total number of sent_tokens found: 5137\n",
      "Total number of predicates found: 3200\n",
      "Total number of filtered predicates: 2637\n",
      "Number of sentences removed due to non-event: 0\n"
     ]
    }
   ],
   "source": [
    "tups_train = extract_list_adjacent([ud_train], happen_set, doc_ids)\n",
    "#tups_dev_test = extract_list_adjacent([ud_dev, ud_test], happen_set, doc_ids)\n",
    "\n",
    "tups_dev = extract_list_adjacent([ud_dev], happen_set, doc_ids)\n",
    "tups_test = extract_list_adjacent([ud_test], happen_set, doc_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of predicate-pairs: 59593\n",
      "Average number of sentences per HIT 4.999832200687977\n",
      "Max number of sentences per HIT 5\n",
      "Min number of sentences per HIT 3\n",
      "\n",
      "\n",
      "Total number of predicate-pairs: 10775\n",
      "Average number of sentences per HIT 5.0\n",
      "Max number of sentences per HIT 5\n",
      "Min number of sentences per HIT 5\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Length of train HIT: 11919\n",
      "Length of dev_test HIT: 2155\n"
     ]
    }
   ],
   "source": [
    "gl_list_train = createHITs(tups_train, offset_max=5)\n",
    "gl_list_dev_test = createHITs(tups_dev_test, offset_max=5)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Length of train HIT: {}\".format(len(gl_list_train)))\n",
    "print(\"Length of dev_test HIT: {}\".format(len(gl_list_dev_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a csv file for MTurk:\n",
    "with open('event_temporal_duration_turk_file_train.csv', 'w+') as file_handler:\n",
    "    file_handler.write(\"var_arrays\\n\")\n",
    "    for item in gl_list_train:\n",
    "        local_str = \"\\\"\" + str(item) + \"\\\"\\n\"\n",
    "        file_handler.write(replace_to_turk(local_str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a csv file for MTurk:\n",
    "with open('event_temporal_duration_turk_file_dev_test.csv', 'w+') as file_handler:\n",
    "    file_handler.write(\"var_arrays\\n\")\n",
    "    for item in gl_list_dev_test:\n",
    "        local_str = \"\\\"\" + str(item) + \"\\\"\\n\"\n",
    "        file_handler.write(replace_to_turk(local_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rough Cost Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "\n",
    "# def nCr(n,r):\n",
    "#     f = math.factorial\n",
    "#     if n<r:\n",
    "#         return 0\n",
    "#     else:\n",
    "#         return f(n) // f(r) // f(n-r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P1 = 1\n",
    "# P2 = 0\n",
    "# global_list_train_within = []\n",
    "# global_list_train_between = []\n",
    "# for i, tup in enumerate(tups_train):\n",
    "#     ns1 = len(tup[0]['pred1'].split(\",\"))\n",
    "#     ns2 = len(tup[0]['pred2'].split(\",\"))\n",
    "#     x3 = P1*ns2\n",
    "#     x4 = P2*ns1\n",
    "#     #global_list_train.append([ns1,ns2,x3,x4])\n",
    "#     global_list_train_within.append(nCr(ns1,2))\n",
    "# #     if i % 2:\n",
    "#     global_list_train_between.extend([x3,x4])\n",
    "    \n",
    "# global_list_dev_test_within = []\n",
    "# global_list_dev_test_between = []\n",
    "# for i, tup in enumerate(tups_dev_test):\n",
    "#     ns1 = len(tup[0]['pred1'].split(\",\"))\n",
    "#     ns2 = len(tup[0]['pred1'].split(\",\"))\n",
    "#     x3 = P1*ns2\n",
    "#     x4 = P2*ns1\n",
    "#     #global_list_dev_test.append([ns1,ns2,x3,x4])\n",
    "#     global_list_dev_test_within.append(nCr(ns1, 2))\n",
    "# #     if i % 2:\n",
    "#     global_list_dev_test_between.extend([x3,x4])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tups_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #sum([sum(x) for x in global_list_train])\n",
    "# sum(global_list_train_within)+sum(global_list_train_between)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (sum(global_list_dev_test_within)+sum(global_list_dev_test_between))*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .4*.1*1.2*(sum(global_list_train_within)+sum(global_list_train_between)+(sum(global_list_dev_test_within)+sum(global_list_dev_test_between))*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .4*.1*1.2*3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
