{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import relevant libraries\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from predpatt import load_conllu\n",
    "from predpatt import PredPatt\n",
    "import csv\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Data Locations:\n",
    "ud_train  =  \"/Users/sidvash/facts_lab/veridicality_sid/UD_English/en-ud-train.conllu\"\n",
    "ud_dev  =  \"/Users/sidvash/facts_lab/veridicality_sid/UD_English/en-ud-dev.conllu\"\n",
    "ud_test  =  \"/Users/sidvash/facts_lab/veridicality_sid/UD_English/en-ud-test.conllu\"\n",
    "ud_data = [ud_train, ud_dev, ud_test]\n",
    "it_happnd = \"/Users/sidvash/facts_lab/veridicality_sid/it-happened_eng_ud1.2_07092017.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Extract Sentence IDS of event-happening sentences\n",
    "data = pd.read_csv(it_happnd , sep='\\t')\n",
    "\n",
    "#Select only sentences which did happen\n",
    "happnd = data[data.Happened == \"true\"]\n",
    "\n",
    "#Select only sentences with high confidence\n",
    "happnd = happnd[happnd.Confidence.isin(['4', '3'])]\n",
    "\n",
    "#Select only sentences where Keep = True\n",
    "happnd = happnd[happnd.Keep == True]\n",
    "\n",
    "#Create a set of ID's to filter later\n",
    "happen_set = list(happnd[['Sentence.ID', 'Pred.Token']].values)\n",
    "happen_set = [tuple(x) for x in happen_set]\n",
    "happen_set = set(happen_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_to_html(s):\n",
    "    '''\n",
    "    Make some changes to the input string to make it html readable\n",
    "    '''\n",
    "    #replace special chars as per html format\n",
    "    s = re.sub(r\"\\'\", r\"&#39;\", s)\n",
    "    s = re.sub(r'\\\"', r\"&quot;\", s)\n",
    "    s = re.sub(r\"\\[\", r\"&lsqb;\", s)\n",
    "    s = re.sub(r\"\\]\", r\"&rsqb;\", s)\n",
    "        \n",
    "    return s\n",
    "\n",
    "def replace_to_turk(s):\n",
    "    '''\n",
    "    Make some changes to the input string to make it Turk readable\n",
    "    '''\n",
    "    #replace double quotes to appear twice : except at the start/end of the list\n",
    "    s = re.sub(r'([^\\]])\\\"', r'\\1\"\"', s)\n",
    "    \n",
    "    #replace single quotes at the beginning and end of list\n",
    "    s = re.sub(r\"\\'\\{\", r\"{\", s)  \n",
    "    s = re.sub(r\"\\}\\'\", r\"}\", s)\n",
    "    \n",
    "    #replace two backslash to three\n",
    "    s = re.sub(r\"\\\\\\\\\", r\"\\\\\\\\\\\\\", s)\n",
    "    \n",
    "    #Leave spaces around <span> \n",
    "    s = re.sub(r\"<span\", r\" <span\", s)\n",
    "    s = re.sub(r\" </span>\", r\"</span> \", s)\n",
    "        \n",
    "    return s\n",
    "\n",
    "def combinations(l):\n",
    "    '''\n",
    "    Input: List containing some elements\n",
    "    \n",
    "    Returns: nC2 combinations of items in the list, \n",
    "            where n = length of list\n",
    "    '''\n",
    "    ans = []\n",
    "    \n",
    "    for idx in range(len(l)-1):\n",
    "        for item2 in l[(idx+1):]:\n",
    "            ans.append((l[idx],item2))\n",
    "            \n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_list_multiple(ud_data, happen_set):\n",
    "    '''\n",
    "    Extract a list of JSON objects from the ud data\n",
    "    \n",
    "    Input: \n",
    "    1. ud data path ending in .conll\n",
    "    2. happen_set: a set of sentence_id where the event did happen\n",
    "    \n",
    "    '''\n",
    "    global_list = []\n",
    "    local_list = []\n",
    "    iden = 1\n",
    "    sent_removed = 0\n",
    "    sent_total = 0\n",
    "    out_list = []\n",
    "    \n",
    "    for ud_data_path in ud_data:\n",
    "        fname = ud_data_path.split(\"/\")[-1]\n",
    "\n",
    "        with open(ud_data_path) as infile:\n",
    "            data = replace_to_html(infile.read())\n",
    "            parsed = [(PredPatt(ud_parse), sent_id) for sent_id, ud_parse in load_conllu(data)]\n",
    "        \n",
    "        random.shuffle(parsed)\n",
    "        #Iterating through each parsed sentence\n",
    "        for parse_sen in parsed:\n",
    "            pred_object = parse_sen[0]            \n",
    "            raw_sentence = [token.text for token in pred_object.tokens]\n",
    "            pred_combs = combinations(pred_object.instances)\n",
    "            sentid_num = parse_sen[1].split(\"_\")[-1]\n",
    "            \n",
    "            if len(pred_combs) == 0:\n",
    "                continue\n",
    "            #Iterating through each predicate combination\n",
    "            for pred_comb in pred_combs:\n",
    "                #Extract each predicate's position and text\n",
    "                pred_sentence = raw_sentence.copy()\n",
    "                pred1 = pred_comb[0].root.text\n",
    "                pred2 = pred_comb[1].root.text\n",
    "                pred_token1 = pred_comb[0].root.position\n",
    "                pred_token2 = pred_comb[1].root.position\n",
    "                \n",
    "                #Insert predicate span into the sentence\n",
    "                pred_sentence.insert(pred_token1, '<span class=\\\"predicate\\\">')\n",
    "                pred_sentence.insert(pred_token1 + 2, '</span>')\n",
    "                pred_sentence.insert(pred_token2 + 2, '<span class=\\\"predicate\\\">')\n",
    "                pred_sentence.insert(pred_token2 + 4, '</span>')\n",
    "                \n",
    "                #Insert items into local dict\n",
    "                token_dict = {}\n",
    "                token_dict['pred_token1'] = str(pred_token1)\n",
    "                token_dict['pred1'] = pred1\n",
    "                token_dict['pred_token2'] = str(pred_token2)\n",
    "                token_dict['pred2'] = pred2\n",
    "                token_dict['sentence'] = \" \".join(pred_sentence)\n",
    "                token_dict['sentence_id'] = fname + \" \" + sentid_num\n",
    "                token_dict['id'] = iden\n",
    "            \n",
    "                #create tuple to check if it exists in the event_happened master set\n",
    "                sent_total += 1\n",
    "                sent_tuple1 = (token_dict['sentence_id'], pred_token1+1)\n",
    "                sent_tuple2 = (token_dict['sentence_id'], pred_token2+1)\n",
    "                #remove if either predicate didn't happen\n",
    "                if (sent_tuple1 not in happen_set) or (sent_tuple1 not in happen_set):\n",
    "                    sent_removed += 1\n",
    "                    continue\n",
    "                \n",
    "                out_tuple = (token_dict['sentence_id'], token_dict['pred_token1'], \n",
    "                             token_dict['pred_token2'],token_dict['sentence'] )\n",
    "                out_list.append(out_tuple)\n",
    "                \n",
    "                \n",
    "                iden += 1\n",
    "\n",
    "                if iden == 11:\n",
    "                    iden = 1 \n",
    "\n",
    "                if len(local_list) == 10:\n",
    "                    global_list.append(local_list)\n",
    "                    local_list = []\n",
    "                    local_list.append(json.dumps(token_dict))\n",
    "                    \n",
    "                else:\n",
    "                    local_list.append(json.dumps(token_dict))\n",
    "        \n",
    "    \n",
    "    #Append the last remaining sentences into the global list\n",
    "    global_list.append(local_list)\n",
    "    \n",
    "    print(\"Total number of sentences found: {}\".format(sent_total))\n",
    "    print(\"Number of sentences removed: {}\".format(sent_removed))\n",
    "                    \n",
    "    return global_list, out_list\n",
    "\n",
    "\n",
    "def extract_list(ud_data, happen_set):\n",
    "    '''\n",
    "    Extract a list of JSON objects from the ud data\n",
    "    \n",
    "    Input: \n",
    "    1. ud data path ending in .conll\n",
    "    2. happen_set: a set of sentence_id where the event did happen\n",
    "    \n",
    "    '''\n",
    "    global_list = []\n",
    "    local_list = []\n",
    "    iden = 1\n",
    "    sent_removed = 0\n",
    "    sent_total = 0\n",
    "    \n",
    "    for ud_data_path in ud_data:\n",
    "        fname = ud_data_path.split(\"/\")[-1]\n",
    "\n",
    "        with open(ud_data_path) as infile:\n",
    "            data = replace_to_html(infile.read())\n",
    "            parsed = [(PredPatt(ud_parse), sent_id) for sent_id, ud_parse in load_conllu(data)]\n",
    "        random.shuffle(parsed)\n",
    "        \n",
    "        for parse_sen in parsed:\n",
    "\n",
    "            for predicate in parse_sen[0].instances:\n",
    "                raw_sentence = [token.text for token in parse_sen[0].tokens]\n",
    "                pred_token = predicate.root.position\n",
    "                pred = predicate.root.text\n",
    "                #print(raw_sentence)\n",
    "                #print(pred_token)\n",
    "                #print(pred)\n",
    "\n",
    "                token_dict = {}\n",
    "                pred_sentence = raw_sentence.copy()\n",
    "                pred_sentence.insert(pred_token, '<span class=\\\"predicate\\\">')\n",
    "                pred_sentence.insert(pred_token + 2, '</span>')\n",
    "                sentid_num = parse_sen[1].split(\"_\")[-1]\n",
    "\n",
    "                token_dict['pred_token'] = str(pred_token)\n",
    "                token_dict['sentence'] = \" \".join(pred_sentence)\n",
    "                token_dict['pred'] = pred\n",
    "                token_dict['sentence_id'] = fname + \" \" + sentid_num\n",
    "                token_dict['id'] = iden\n",
    "                \n",
    "                #create tuple to check if it exists in the event_happened master set\n",
    "                sent_total += 1\n",
    "                sent_tuple = (token_dict['sentence_id'], pred_token)  #changed from original\n",
    "                if sent_tuple not in happen_set:\n",
    "                    sent_removed += 1\n",
    "                    continue\n",
    "\n",
    "                iden += 1\n",
    "\n",
    "                if iden == 11:\n",
    "                    iden = 1 \n",
    "\n",
    "                if len(local_list) == 10:\n",
    "                    global_list.append(local_list)\n",
    "                    local_list = []\n",
    "                    local_list.append(json.dumps(token_dict))\n",
    "                    \n",
    "                else:\n",
    "                    local_list.append(json.dumps(token_dict))\n",
    "        \n",
    "    \n",
    "    #Append the last remaining sentences into the global list\n",
    "    global_list.append(local_list)\n",
    "    \n",
    "    print(\"Total number of sentences found: {}\".format(sent_total))\n",
    "    print(\"Number of sentences removed : {}\".format(sent_removed))\n",
    "                    \n",
    "    return global_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sentences found: 31867\n",
      "Number of sentences removed: 16174\n"
     ]
    }
   ],
   "source": [
    "gl_list, out_list = extract_list_multiple(ud_data, happen_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2311 sentences filtered\n"
     ]
    }
   ],
   "source": [
    "#Filter sentences with while, when, until:\n",
    "word_list = ['when', 'until', 'while']\n",
    "\n",
    "filter_list = []\n",
    "\n",
    "for sent_tuple in out_list:\n",
    "    if any(word for word in word_list if word in sent_tuple[3]):\n",
    "        filter_list.append(sent_tuple)\n",
    "        \n",
    "print(\"{} sentences filtered\".format(len(filter_list)))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the subset dataset of sentences to be filtered later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Dataframe to look at the subset of sentences\n",
    "filtered_data = pd.DataFrame(filter_list, columns=['sentence_id', 'pred_token1', 'pred_token2', 'sentence'])\n",
    "filtered_data.to_csv(\"happnd_event_types_subset.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the manually selected sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 filtered sentence pairs\n"
     ]
    }
   ],
   "source": [
    "#Change floats datatypes to int\n",
    "selected_sent = pd.read_csv('happnd_event_test100.csv')  #taken from google drive\n",
    "selected_sent.fillna(0, inplace=True)\n",
    "selected_sent[\"pred_token1\"] = selected_sent[\"pred_token1\"].map(lambda x: int(x))\n",
    "selected_sent[\"pred_token2\"] = selected_sent[\"pred_token2\"].map(lambda x: int(x))\n",
    "selected_sent[\"keep\"] = selected_sent[\"keep\"].map(lambda x: int(x))\n",
    "filtered = selected_sent[selected_sent.keep == 1]\n",
    "\n",
    "#Create a set of ID's to filter later\n",
    "filter_happen_set = list(filtered[['sentence_id', 'pred_token1', 'pred_token2']].values)\n",
    "filter_happen_set = [tuple(x) for x in filter_happen_set]\n",
    "print(\"{} filtered sentence pairs\".format(len(filter_happen_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating new happen_set to be extracted for HIT Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of new set 200\n"
     ]
    }
   ],
   "source": [
    "new_set = []\n",
    "for x,y,z in filter_happen_set:\n",
    "    new_set.append((x,y))\n",
    "    new_set.append((x,z))\n",
    "new_set = set(new_set)\n",
    "print(\"Length of new set {}\".format(len(new_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sentences found: 28658\n",
      "Number of sentences removed : 28458\n"
     ]
    }
   ],
   "source": [
    "gl_list = extract_list(ud_data, new_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create a csv file for MTurk:\n",
    "with open('test_event_type_200.csv', 'w+') as file_handler:\n",
    "    file_handler.write(\"var_arrays\\n\")\n",
    "    for item in gl_list:\n",
    "        local_str = \"\\\"\" + str(item) + \"\\\"\\n\"\n",
    "        file_handler.write(replace_to_turk(local_str))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
