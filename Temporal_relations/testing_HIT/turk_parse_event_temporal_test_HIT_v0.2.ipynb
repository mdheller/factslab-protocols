{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import relevant libraries\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from predpatt import load_conllu\n",
    "from predpatt import PredPatt\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Data Locations:\n",
    "ud_train  =  \"/Users/sidvash/facts_lab/veridicality_sid/UD_English/en-ud-train.conllu\"\n",
    "ud_dev  =  \"/Users/sidvash/facts_lab/veridicality_sid/UD_English/en-ud-dev.conllu\"\n",
    "ud_test  =  \"/Users/sidvash/facts_lab/veridicality_sid/UD_English/en-ud-test.conllu\"\n",
    "ud_data = [ud_train, ud_dev, ud_test]\n",
    "it_happnd = \"/Users/sidvash/facts_lab/veridicality_sid/it-happened_eng_ud1.2_07092017.tsv\"\n",
    "pilot_data_upd = \"pilot_sent_token_data.csv\"\n",
    "pilot_data_file = \"pilot_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Pilot data extracted from pilot protocol1 annotations \n",
    "df_glmer = pd.read_csv(pilot_data_upd)\n",
    "\n",
    "#Extract Sentence IDS of event-happening sentences\n",
    "data = pd.read_csv(it_happnd , sep='\\t')\n",
    "\n",
    "#Event-type pilot data\n",
    "pilot_data = pd.read_csv(pilot_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_token</th>\n",
       "      <th>glmer_intercept_inst</th>\n",
       "      <th>glmer_intercept_start</th>\n",
       "      <th>glmer_intercept_end</th>\n",
       "      <th>instant_final</th>\n",
       "      <th>start_final</th>\n",
       "      <th>end_final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>en-ud-dev.conllu 110_11</td>\n",
       "      <td>-0.759728</td>\n",
       "      <td>1.194579</td>\n",
       "      <td>0.585228</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>en-ud-dev.conllu 110_18</td>\n",
       "      <td>0.230165</td>\n",
       "      <td>1.194579</td>\n",
       "      <td>1.122885</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>en-ud-dev.conllu 13_14</td>\n",
       "      <td>0.429219</td>\n",
       "      <td>1.748161</td>\n",
       "      <td>1.656412</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>en-ud-dev.conllu 13_6</td>\n",
       "      <td>-0.259216</td>\n",
       "      <td>1.748161</td>\n",
       "      <td>1.656412</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>en-ud-dev.conllu 1379_1</td>\n",
       "      <td>-0.259216</td>\n",
       "      <td>1.053771</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>en-ud-dev.conllu 1379_8</td>\n",
       "      <td>0.429219</td>\n",
       "      <td>1.053771</td>\n",
       "      <td>1.656412</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>en-ud-dev.conllu 1415_10</td>\n",
       "      <td>0.230165</td>\n",
       "      <td>1.194579</td>\n",
       "      <td>0.585228</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>en-ud-dev.conllu 1415_6</td>\n",
       "      <td>-0.759728</td>\n",
       "      <td>1.194579</td>\n",
       "      <td>0.585228</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>en-ud-dev.conllu 1705_19</td>\n",
       "      <td>-0.759728</td>\n",
       "      <td>1.745843</td>\n",
       "      <td>0.585228</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>en-ud-dev.conllu 1705_6</td>\n",
       "      <td>-0.759728</td>\n",
       "      <td>2.339925</td>\n",
       "      <td>2.242671</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 sent_token  glmer_intercept_inst  glmer_intercept_start  \\\n",
       "0   en-ud-dev.conllu 110_11             -0.759728               1.194579   \n",
       "1   en-ud-dev.conllu 110_18              0.230165               1.194579   \n",
       "2    en-ud-dev.conllu 13_14              0.429219               1.748161   \n",
       "3     en-ud-dev.conllu 13_6             -0.259216               1.748161   \n",
       "4   en-ud-dev.conllu 1379_1             -0.259216               1.053771   \n",
       "5   en-ud-dev.conllu 1379_8              0.429219               1.053771   \n",
       "6  en-ud-dev.conllu 1415_10              0.230165               1.194579   \n",
       "7   en-ud-dev.conllu 1415_6             -0.759728               1.194579   \n",
       "8  en-ud-dev.conllu 1705_19             -0.759728               1.745843   \n",
       "9   en-ud-dev.conllu 1705_6             -0.759728               2.339925   \n",
       "\n",
       "   glmer_intercept_end  instant_final  start_final  end_final  \n",
       "0             0.585228          False         True       True  \n",
       "1             1.122885           True         True       True  \n",
       "2             1.656412           True         True       True  \n",
       "3             1.656412          False         True       True  \n",
       "4             0.408542          False         True       True  \n",
       "5             1.656412           True         True       True  \n",
       "6             0.585228           True         True       True  \n",
       "7             0.585228          False         True       True  \n",
       "8             0.585228          False         True       True  \n",
       "9             2.242671          False         True       True  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_glmer['instant_final'] = np.where(df_glmer['glmer_intercept_inst']>=0, True, False)\n",
    "df_glmer['start_final'] = np.where(df_glmer['glmer_intercept_start']>=0, True, False)\n",
    "df_glmer['end_final'] = np.where(df_glmer['glmer_intercept_end']>=0, True, False)\n",
    "\n",
    "#If instant, convert start, end to False\n",
    "df_glmer.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_glmer[df_glmer['sent_token']=='en-ud-dev.conllu 110_11'].instant_final.values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting instant, start and end values for each sent_token id and storing in dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "instant_dict = {}\n",
    "start_dict = {}\n",
    "end_dict = {}\n",
    "\n",
    "sent_tokens = list(df_glmer.sent_token.values)\n",
    "for x in sent_tokens:\n",
    "    sent_id, token_id = x.split(\"_\")\n",
    "    instant_dict[(sent_id, int(token_id))] = str(df_glmer[df_glmer['sent_token']==x].instant_final.values[0])\n",
    "    start_dict[(sent_id, int(token_id))] = str(df_glmer[df_glmer['sent_token']==x].start_final.values[0])\n",
    "    end_dict[(sent_id, int(token_id))] = str(df_glmer[df_glmer['sent_token']==x].end_final.values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sent_token ids where both \"has_start\" and \"has_end\" is False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['en-ud-train.conllu 11844_7', 'en-ud-train.conllu 12377_42',\n",
       "       'en-ud-train.conllu 301_1', 'en-ud-train.conllu 5948_12',\n",
       "       'en-ud-train.conllu 5948_17', 'en-ud-train.conllu 6173_6',\n",
       "       'en-ud-train.conllu 6387_2', 'en-ud-train.conllu 65_57',\n",
       "       'en-ud-train.conllu 867_49', 'en-ud-train.conllu 9491_22'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_glmer[(df_glmer.start_final == False ) & \n",
    "              (df_glmer.end_final == False )].sent_token.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select sentence-token ids where at least \"has_start\" or \"has_end\" is True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDs to be removed: \n",
      "{('en-ud-train.conllu 12377', 42), ('en-ud-train.conllu 867', 49), ('en-ud-train.conllu 9491', 22), ('en-ud-train.conllu 11844', 7), ('en-ud-train.conllu 5948', 12), ('en-ud-train.conllu 6387', 2), ('en-ud-train.conllu 5948', 17), ('en-ud-train.conllu 65', 57), ('en-ud-train.conllu 6173', 6), ('en-ud-train.conllu 301', 1)}\n",
      "\n",
      "\n",
      "No. of Selected token-ids: 190\n"
     ]
    }
   ],
   "source": [
    "#Ids to be removed\n",
    "temp_ids = list(df_glmer[(df_glmer.start_final == False ) &\n",
    "                           (df_glmer.end_final == False )].sent_token.values)\n",
    "\n",
    "remove_set = set(tuple([x.split(\"_\")[0], int(x.split(\"_\")[1])])  for x in temp_ids)\n",
    "print(\"IDs to be removed: \")\n",
    "print(remove_set)\n",
    "print(\"\\n\")\n",
    "\n",
    "#All ids\n",
    "temp_total_ids = list(pilot_data.sent_token.unique())\n",
    "total_set = set(tuple([x.split(\"_\")[0], int(x.split(\"_\")[1])])  for x in temp_total_ids)\n",
    "\n",
    "#Ids to be selected\n",
    "final_select_set = total_set.difference(remove_set)\n",
    "print(\"No. of Selected token-ids: {}\".format(len(final_select_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(total_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_to_html(s):\n",
    "    '''\n",
    "    Make some changes to the input string to make it html readable\n",
    "    '''\n",
    "    #replace special chars as per html format\n",
    "    s = re.sub(r\"\\'\", r\"&#39;\", s)\n",
    "    s = re.sub(r'\\\"', r\"&quot;\", s)\n",
    "    s = re.sub(r\"\\[\", r\"&lsqb;\", s)\n",
    "    s = re.sub(r\"\\]\", r\"&rsqb;\", s)\n",
    "        \n",
    "    return s\n",
    "\n",
    "def replace_to_turk(s):\n",
    "    '''\n",
    "    Make some changes to the input string to make it Turk readable\n",
    "    '''\n",
    "    #replace double quotes to appear twice : except at the start/end of the list\n",
    "    s = re.sub(r'([^\\]])\\\"', r'\\1\"\"', s)\n",
    "    \n",
    "    #replace single quotes at the beginning and end of list\n",
    "    s = re.sub(r\"\\'\\{\", r\"{\", s)  \n",
    "    s = re.sub(r\"\\}\\'\", r\"}\", s)\n",
    "    \n",
    "    #replace two backslash to three\n",
    "    s = re.sub(r\"\\\\\\\\\", r\"\\\\\\\\\\\\\", s)\n",
    "    \n",
    "    #Leave spaces around <span> \n",
    "    s = re.sub(r\"<span\", r\" <span\", s)\n",
    "    s = re.sub(r\" </span>\", r\"</span> \", s)\n",
    "        \n",
    "    return s\n",
    "\n",
    "def combinations(l):\n",
    "    '''\n",
    "    Input: List containing some elements\n",
    "    \n",
    "    Returns: nC2 combinations of items in the list, \n",
    "            where n = length of list\n",
    "    '''\n",
    "    ans = []\n",
    "    \n",
    "    for idx in range(len(l)-1):\n",
    "        for item2 in l[(idx+1):]:\n",
    "            ans.append((l[idx],item2))\n",
    "            \n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_list(ud_data, happen_set, instant_dict, start_dict, end_dict):\n",
    "    '''\n",
    "    Extract a list of JSON objects from the ud data\n",
    "    \n",
    "    Input: \n",
    "    1. ud data path ending in .conll\n",
    "    2. happen_set: a set of sentence_id where the event did happen\n",
    "    \n",
    "    '''\n",
    "    global_list = []\n",
    "    local_list = []\n",
    "    iden = 1\n",
    "    sent_removed = 0\n",
    "    sent_total = 0\n",
    "    out_list = []\n",
    "    \n",
    "    for ud_data_path in ud_data:\n",
    "        fname = ud_data_path.split(\"/\")[-1]\n",
    "\n",
    "        with open(ud_data_path) as infile:\n",
    "            data = replace_to_html(infile.read())\n",
    "            parsed = [(PredPatt(ud_parse), sent_id) for sent_id, ud_parse in load_conllu(data)]\n",
    "        \n",
    "        #Iterating through each parsed sentence\n",
    "        for parse_sen in parsed:\n",
    "            pred_object = parse_sen[0]            \n",
    "            raw_sentence = [token.text for token in pred_object.tokens]\n",
    "            pred_combs = combinations(pred_object.instances)\n",
    "            sentid_num = parse_sen[1].split(\"_\")[-1]\n",
    "            \n",
    "            if len(pred_combs) == 0:\n",
    "                continue\n",
    "            #Iterating through each predicate combination\n",
    "            for pred_comb in pred_combs:\n",
    "                #Extract each predicate's position and text\n",
    "                pred_sentence = raw_sentence.copy()\n",
    "                pred1 = pred_comb[0].root.text\n",
    "                pred2 = pred_comb[1].root.text\n",
    "                pred_token1 = pred_comb[0].root.position\n",
    "                pred_token2 = pred_comb[1].root.position\n",
    "                \n",
    "                #Insert predicate span into the sentence\n",
    "                pred_sentence.insert(pred_token1, '<span class=\\\"predicate\\\">')\n",
    "                pred_sentence.insert(pred_token1 + 2, '</span>')\n",
    "                pred_sentence.insert(pred_token2 + 2, '<span class=\\\"predicate\\\">')\n",
    "                pred_sentence.insert(pred_token2 + 4, '</span>')\n",
    "                \n",
    "                #Insert items into local dict\n",
    "                token_dict = {}\n",
    "                token_dict['pred_token1'] = str(pred_token1)\n",
    "                token_dict['pred1'] = pred1\n",
    "                token_dict['pred_token2'] = str(pred_token2)\n",
    "                token_dict['pred2'] = pred2\n",
    "                token_dict['sentence'] = \" \".join(pred_sentence)\n",
    "                token_dict['sentence_id'] = fname + \" \" + sentid_num\n",
    "                token_dict['id'] = iden\n",
    "                \n",
    "            \n",
    "                #create tuple to check if it exists in the event_happened master set\n",
    "                sent_total += 1\n",
    "                sent_tuple1 = (token_dict['sentence_id'], pred_token1)\n",
    "                sent_tuple2 = (token_dict['sentence_id'], pred_token2)\n",
    "                \n",
    "                \n",
    "                #remove if either predicate didn't happen\n",
    "                if (sent_tuple1 not in happen_set) or (sent_tuple2 not in happen_set):\n",
    "                    sent_removed += 1\n",
    "                    continue\n",
    "                    \n",
    "                #Instant, start and end values for each sent_token id\n",
    "                token_dict['instant_pred1'] =  instant_dict[sent_tuple1]\n",
    "                token_dict['instant_pred2'] =  instant_dict[sent_tuple2]\n",
    "                token_dict['start_pred1'] =  start_dict[sent_tuple1]\n",
    "                token_dict['start_pred2'] =  start_dict[sent_tuple2]\n",
    "                token_dict['end_pred1'] =  end_dict[sent_tuple1]\n",
    "                token_dict['end_pred2'] =  end_dict[sent_tuple2]\n",
    "                \n",
    "                out_tuple = (token_dict['sentence_id'], token_dict['pred_token1'], \n",
    "                             token_dict['pred_token2'],token_dict['sentence'] )\n",
    "                out_list.append(out_tuple)\n",
    "                \n",
    "                \n",
    "                iden += 1\n",
    "\n",
    "                if iden == 11:\n",
    "                    iden = 1 \n",
    "\n",
    "                if len(local_list) == 10:\n",
    "                    global_list.append(local_list)\n",
    "                    local_list = []\n",
    "                    local_list.append(json.dumps(token_dict))\n",
    "                    \n",
    "                else:\n",
    "                    local_list.append(json.dumps(token_dict))\n",
    "        \n",
    "    \n",
    "    #Append the last remaining sentences into the global list\n",
    "    global_list.append(local_list)  #--ignoring for pilot as using multiples of 10\n",
    "    \n",
    "    print(\"Total number of sentences found: {}\".format(sent_total))\n",
    "    print(\"Number of sentences removed due to non-event: {}\".format(sent_removed))\n",
    "                    \n",
    "    return global_list, out_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sentences found: 31867\n",
      "Number of sentences removed due to non-event: 31767\n"
     ]
    }
   ],
   "source": [
    "#use \"final_select_set\" instead of \"total_set\" if you want to \n",
    "#exclude sentences with neither a start point not an end-point.\n",
    "\n",
    "gl_list, out_list = extract_list(ud_data, total_set, instant_dict, start_dict, end_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of HITs: 10\n"
     ]
    }
   ],
   "source": [
    "print(\"No. of HITs: {}\".format(len(gl_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create a csv file for MTurk:\n",
    "with open('test_protocol2.csv', 'w+') as file_handler:\n",
    "    file_handler.write(\"var_arrays\\n\")\n",
    "    for item in gl_list:\n",
    "        local_str = \"\\\"\" + str(item) + \"\\\"\\n\"\n",
    "        file_handler.write(replace_to_turk(local_str))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
