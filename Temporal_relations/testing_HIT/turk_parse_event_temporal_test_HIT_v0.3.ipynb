{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Revamped version with all predicates and copulas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import relevant libraries\n",
    "from collections import defaultdict, deque\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from predpatt import load_conllu\n",
    "from predpatt import PredPatt\n",
    "from predpatt import PredPattOpts\n",
    "from itertools import combinations\n",
    "from collections import Iterable\n",
    "from random import shuffle\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Extract old pilot sentence ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Locations:\n",
    "pilot_data_upd = \"pilot_sent_token_data.csv\"\n",
    "pilot_data_file = \"pilot_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pilot data extracted from pilot protocol1 annotations \n",
    "df_pilot1 = pd.read_csv(pilot_data_upd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "set_ids_pilot1 = set([x.split(\"_\")[0] for x in df_pilot1.sent_token.values])\n",
    "print(len(set_ids_pilot1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Run Turk_Parse as original "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Locations:\n",
    "ud_train  =  \"/Users/sidvash/facts_lab/veridicality_sid/UD_English/en-ud-train.conllu\"\n",
    "ud_dev  =  \"/Users/sidvash/facts_lab/veridicality_sid/UD_English/en-ud-dev.conllu\"\n",
    "ud_test  =  \"/Users/sidvash/facts_lab/veridicality_sid/UD_English/en-ud-test.conllu\"\n",
    "ud_data = [ud_train, ud_dev, ud_test]\n",
    "it_happnd = \"/Users/sidvash/facts_lab/veridicality_sid/it-happened_eng_ud1.2_07092017.tsv\"\n",
    "\n",
    "#Taken from github - includes doc ids and sent ids\n",
    "ud_train_detailed =  \"/Users/sidvash/facts_lab/veridicality_sid/UD_English/en_ewt-ud-train.conllu\"\n",
    "ud_dev_detailed =  \"/Users/sidvash/facts_lab/veridicality_sid/UD_English/en_ewt-ud-dev.conllu\"\n",
    "ud_test_detailed =  \"/Users/sidvash/facts_lab/veridicality_sid/UD_English/en_ewt-ud-test.conllu\"\n",
    "\n",
    "null_str = \"_null_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_to_html(s):\n",
    "    '''\n",
    "    Make some changes to the input string to make it html readable\n",
    "    '''\n",
    "    #replace special chars as per html format\n",
    "    s = re.sub(r\"\\'\", r\"&#39;\", s)\n",
    "    s = re.sub(r'\\\"', r\"&quot;\", s)\n",
    "    s = re.sub(r\"\\[\", r\"&lsqb;\", s)\n",
    "    s = re.sub(r\"\\]\", r\"&rsqb;\", s)\n",
    "    s = re.sub(r\"\\,\", r\"&#44;\", s)\n",
    "        \n",
    "    return s\n",
    "\n",
    "def replace_to_turk(s):\n",
    "    '''\n",
    "    Make some changes to the input string to make it Turk readable\n",
    "    '''\n",
    "    #replace double quotes to appear twice : except at the start/end of the list\n",
    "    s = re.sub(r'([^\\]])\\\"', r'\\1\"\"', s)\n",
    "    \n",
    "    #replace single quotes at the beginning and end of list\n",
    "    s = re.sub(r\"\\'\\{\", r\"{\", s)  \n",
    "    s = re.sub(r\"\\}\\'\", r\"}\", s)\n",
    "    \n",
    "    #replace two backslash to three\n",
    "    s = re.sub(r\"\\\\\\\\\", r\"\\\\\\\\\\\\\", s)\n",
    "    \n",
    "    #Leave spaces around <span> \n",
    "    s = re.sub(r\"<span\", r\" <span\", s)\n",
    "    s = re.sub(r\" </span>\", r\"</span> \", s)\n",
    "        \n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ud_english(fpath):\n",
    "    \"\"\"Load a file from the UD English corpus\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fpath : str\n",
    "        Path to UD corpus file ending in .conllu\n",
    "    \"\"\"\n",
    "\n",
    "    n = 1\n",
    "\n",
    "    fname = os.path.split(fpath)[1]\n",
    "\n",
    "    parses = defaultdict(list)\n",
    "    sent_ids = []\n",
    "    newdoc_ids = []\n",
    "    \n",
    "    for l in open(fpath):\n",
    "        ident = fname+' '+str(n)\n",
    "        \n",
    "        if re.match(r'\\# newdoc id', l):\n",
    "            newdoc_ids.append(n)\n",
    "            \n",
    "        if re.match(r'^\\d', l):\n",
    "            l_split = l.strip().split()\n",
    "            parses[ident].append(l_split)\n",
    "        \n",
    "        elif parses[ident]:\n",
    "            sent_ids.append(ident)\n",
    "            n += 1\n",
    "\n",
    "    return newdoc_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copula_predicates(predicate):\n",
    "    pos_tags = [\"ADJ\", \"NOUN\", \"NUM\", \"DET\", \"PROPN\", \"PRON\", \"VERB\", \"AUX\"] \n",
    "    #Ignore predicates that do not have one of the pos tags\n",
    "    if predicate.root.tag not in pos_tags:\n",
    "        return None\n",
    "        \n",
    "    #Extend predicate to start from the copula\n",
    "    if predicate.root.tag not in [\"VERB\", \"AUX\"]:\n",
    "        all_pred = predicate.tokens\n",
    "        gov_rels = [tok.gov_rel for tok in all_pred]\n",
    "        if 'cop' in gov_rels:\n",
    "            cop_pos = gov_rels.index('cop')\n",
    "            pred = [x.text for x in all_pred[cop_pos:]]\n",
    "            pred_token = [x.position for x in all_pred[cop_pos:]]\n",
    "            def_pred_token = predicate.root.position  #needed for it_happen set\n",
    "            cop_bool = True                \n",
    "        else:\n",
    "            return None\n",
    "            \n",
    "    #Else keep the root        \n",
    "    else:\n",
    "        pred_token = [predicate.root.position]\n",
    "        pred = [predicate.root.text]\n",
    "        def_pred_token = predicate.root.position \n",
    "\n",
    "    #Stringify pred and pred_tokens:\n",
    "    #pred_token = \"_\".join(map(str, pred_token))\n",
    "\n",
    "    if len(pred)>5:\n",
    "        pred = pred[:5]\n",
    "        pred = \" \".join(pred) + \"...\"\n",
    "    else:\n",
    "        pred = \" \".join(pred)\n",
    "        \n",
    "    return pred, pred_token, def_pred_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_pred_all_double(preds_all, raw_sentence, fname, sentid_num, sentid_num_next, happen_set=[]):\n",
    "    '''\n",
    "    Extract turk_parse dict from input all predicates \n",
    "    \n",
    "    INputs:\n",
    "    1. pred_all : one list of all predicates in both sentences\n",
    "    2. raw_sentence: a dict of two sentences, with key: sent_id_num\n",
    "    3. sentid_num: 1st sentence in adjacent sentence\n",
    "    4. sentid_num_next: 2nd sentence in adjacent sentence\n",
    "    \n",
    "    '''\n",
    "    token_dict = {}\n",
    "    \n",
    "    filter_preds_all = preds_all\n",
    "    \n",
    "    #Extract each predicate's position and text\n",
    "    pred_sent1 = raw_sentence[sentid_num].copy()\n",
    "    pred_sent2 = raw_sentence[sentid_num_next].copy()\n",
    "        \n",
    "    #Predicate sentence 1:\n",
    "    pred1_tuple = [] \n",
    "    for s_id, pred in filter_preds_all:\n",
    "        if s_id==sentid_num:\n",
    "            temp = copula_predicates(pred)\n",
    "            if temp:\n",
    "                pred1_tuple.append(temp)\n",
    "            \n",
    "    if pred1_tuple:\n",
    "        pred1 = [pred_txt for pred_txt, pred_token, pred_root_token in pred1_tuple]\n",
    "        pred_root_token1 = [pred_root_token for pred_txt, pred_token, pred_root_token in pred1_tuple]\n",
    "        \n",
    "        pred_token1 = []\n",
    "        for pred_txt, pred_token, pred_root_token in pred1_tuple:\n",
    "            if isinstance(pred_token, Iterable):\n",
    "                pred_token = \"_\".join(map(str, pred_token))\n",
    "                pred_token1.append(pred_token)\n",
    "            else:\n",
    "                pred_token1.append(pred_token)\n",
    "\n",
    "\n",
    "        acc=0\n",
    "        for i,ins in enumerate(pred_token1):\n",
    "            if isinstance(ins, int):\n",
    "                pred_sent1.insert(ins + acc, ' <span class=\\\"predicate' + str(i+1) + '\\\">' + '<sup>' + \n",
    "                                  str(i+1) + '</sup>')\n",
    "                pred_sent1.insert(ins + acc + 2, '</span> ')\n",
    "                acc += 2\n",
    "            else:\n",
    "                int_ins = [int(x) for x in ins.split(\"_\")]\n",
    "                for pos in int_ins:\n",
    "                    pred_sent1.insert(pos + acc, ' <span class=\\\"predicate' + str(i+1) + '\\\">'+ '<sup>' + \n",
    "                                  str(i+1) + '</sup>')\n",
    "                    pred_sent1.insert(pos + acc + 2, '</span> ')\n",
    "                    acc += 2\n",
    "    else:\n",
    "        pred1 = pred_root_token1 = pred_token1 = []\n",
    "        \n",
    "    i=len(pred1)\n",
    "    #Predicate sentence 2:\n",
    "    pred2_tuple = [] \n",
    "    for s_id, pred in filter_preds_all:\n",
    "        if s_id==sentid_num_next:\n",
    "            temp = copula_predicates(pred)\n",
    "            if temp:\n",
    "                pred2_tuple.append(temp)\n",
    "    \n",
    "    if pred2_tuple:\n",
    "        pred2 = [pred_txt for pred_txt, pred_token, pred_root_token in pred2_tuple]\n",
    "        pred_root_token2 = [pred_root_token for pred_txt, pred_token, pred_root_token in pred2_tuple]\n",
    "        pred_token2 = []\n",
    "\n",
    "        for pred_txt, pred_token, pred_root_token in pred2_tuple:\n",
    "            if isinstance(pred_token, Iterable):\n",
    "                pred_token = \"_\".join(map(str, pred_token))\n",
    "                pred_token2.append(pred_token)\n",
    "            else:\n",
    "                pred_token2.append(pred_token)\n",
    "\n",
    "\n",
    "        acc=0\n",
    "        for j,ins in enumerate(pred_token2):\n",
    "            if isinstance(ins, int):\n",
    "                pred_sent2.insert(ins + acc, ' <span class=\\\"predicate' + str(i+j+1) + '\\\">' + '<sup>' + \n",
    "                                  str(i+j+1) + '</sup>')\n",
    "                \n",
    "                pred_sent2.insert(ins + acc + 2, '</span> ')\n",
    "                acc += 2\n",
    "            else:\n",
    "                int_ins = [int(x) for x in ins.split(\"_\")]\n",
    "                for pos in int_ins:\n",
    "                    pred_sent2.insert(pos + acc, ' <span class=\\\"predicate'+ str(i+j+1) + '\\\">'+ '<sup>' + \n",
    "                                  str(i+j+1) + '</sup>')\n",
    "                    pred_sent2.insert(pos + acc + 2, '</span> ')\n",
    "                    acc += 2\n",
    "    else:\n",
    "        pred2 = pred_root_token2 = pred_token2 = []\n",
    "        \n",
    "    pred_sentence = pred_sent1 + pred_sent2\n",
    "    num_preds = len(pred1) + len(pred2)\n",
    "    \n",
    "    token_dict['sentence'] = \" \".join(pred_sentence)\n",
    "    \n",
    "    if pred_token1:\n",
    "        pred_token1 = \",\".join(map(str, pred_token1))\n",
    "        pred_root_token1 = \",\".join(map(str, pred_root_token1))\n",
    "    else:\n",
    "        pred_token1 = null_str\n",
    "        pred_root_token1 = null_str\n",
    "        \n",
    "    if pred_token2:\n",
    "        pred_token2 = \",\".join(map(str, pred_token2))\n",
    "        pred_root_token2 = \",\".join(map(str, pred_root_token2))\n",
    "    else:\n",
    "        pred_token2 = null_str\n",
    "        pred_root_token2 = null_str\n",
    "    \n",
    "    if pred1:\n",
    "        pred1 = \",\".join(pred1)\n",
    "    else:\n",
    "        pred1=null_str\n",
    "    \n",
    "    if pred2:\n",
    "        pred2 = \",\".join(pred2)\n",
    "    else:\n",
    "        pred2 = null_str\n",
    "    \n",
    "    #print(token_dict['sentence'])\n",
    "    #print(\"\\n\")\n",
    "    token_dict['pred_token1'] = pred_token1\n",
    "    token_dict['pred1'] = pred1\n",
    "    token_dict['pred_token2'] = pred_token2\n",
    "    token_dict['pred2'] = pred2\n",
    "    token_dict['sentence_id_1'] = fname + \" \" + sentid_num \n",
    "    token_dict['sentence_id_2'] = fname + \" \" + sentid_num_next \n",
    "    token_dict['pred_root_token1'] = pred_root_token1\n",
    "    token_dict['pred_root_token2'] = pred_root_token2\n",
    "    \n",
    "    return token_dict, pred_token1, pred_token2, num_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_list_adjacent(ud_data, doc_id_dict, set_ids_pilot1):\n",
    "    '''\n",
    "    Extract a list of JSON objects from the ud data\n",
    "    \n",
    "    #Difference from original: Combining two adjacent sentences to form a single one\n",
    "    \n",
    "    Input: \n",
    "    1. ud data path ending in .conll\n",
    "    2. happen_set: a set of sentence_id where the event did happen\n",
    "    \n",
    "    '''\n",
    "    global_list = []\n",
    "    local_list = []\n",
    "    iden = 1\n",
    "    sent_removed = 0\n",
    "    sent_total = 0\n",
    "    total_preds = 0\n",
    "    single_preds=0\n",
    "    global_tuples = []\n",
    "    \n",
    "    # Resolve relative clause\n",
    "    options = PredPattOpts(resolve_relcl=True, borrow_arg_for_relcl=True, resolve_conj=False)\n",
    "    \n",
    "    #Predicate tags to be used later  \n",
    "    copula_cnts = defaultdict(int)\n",
    "    pred_cnts = defaultdict(int)\n",
    "    auxverb_cnts = defaultdict(int)\n",
    "    ignore_cnts =  defaultdict(int)\n",
    "    copula_indxs = defaultdict(list)\n",
    "    discont_indxs = defaultdict(list)\n",
    "    ###\n",
    "    \n",
    "    for ud_data_path in ud_data:\n",
    "        covered_set = set()\n",
    "        fname = ud_data_path.split(\"/\")[-1]\n",
    "        data_name = ud_data_path.split(\".\")[0].split(\"-\")[-1]\n",
    "        doc_ids = doc_id_dict[data_name]\n",
    "        \n",
    "        with open(ud_data_path) as infile:\n",
    "            data = replace_to_html(infile.read())\n",
    "            parsed = [(PredPatt(ud_parse, opts=options), sent_id) for sent_id, ud_parse in load_conllu(data)]\n",
    "        \n",
    "        total_sents = len(parsed)\n",
    "        \n",
    "        docslist = deque(doc_ids[1:])\n",
    "        nextdoc = docslist.popleft()\n",
    "        \n",
    "        #Iterating through each parsed sentence\n",
    "        for i, parse_sen in enumerate(parsed):   \n",
    "            if i in covered_set:\n",
    "                continue\n",
    "            \n",
    "            if docslist:\n",
    "                if nextdoc in covered_set:\n",
    "                    nextdoc = docslist.popleft()\n",
    "                \n",
    "            pred_object = parse_sen[0]      \n",
    "            sentid_num = parse_sen[1].split(\"_\")[-1]\n",
    "            \n",
    "            \n",
    "            sentence_id_temp = fname + \" \" + sentid_num \n",
    "            \n",
    "            if sentence_id_temp not in set_ids_pilot1:\n",
    "                continue\n",
    "            \n",
    "            ##########################################\n",
    "            #Next item in parsed is not a new document\n",
    "             ##########################################\n",
    "            if (i+2) != nextdoc and (i+1)!= total_sents:\n",
    "                covered_set.add(i)\n",
    "                #covered_set.add(i+1)\n",
    "                \n",
    "                parse_sen_next = parsed[i+1]\n",
    "                pred_object_next = parse_sen_next[0]\n",
    "                sentid_num_next = parse_sen_next[1].split(\"_\")[-1]\n",
    "            \n",
    "                raw_sentence =  {sentid_num : [token.text for token in pred_object.tokens] ,\n",
    "                                sentid_num_next: [token.text for token in pred_object_next.tokens]}\n",
    "                \n",
    "                preds_curr = [(sentid_num,pred) for pred in pred_object.instances]\n",
    "                preds_next = [(sentid_num_next,pred) for pred in pred_object_next.instances]\n",
    "                \n",
    "                preds_all = preds_curr + preds_next\n",
    "                \n",
    "                sent_total+=len(preds_all)\n",
    "                            \n",
    "                #token dict from all predicates in both sentences:\n",
    "                token_dict, pred_token1, pred_token2, num_preds = dict_pred_all_double(preds_all, raw_sentence, \n",
    "                                                                                  fname, sentid_num, \n",
    "                                                                                    sentid_num_next)\n",
    "                \n",
    "                global_tuples.append((token_dict, pred_token1, pred_token2, num_preds))\n",
    "                \n",
    "                    \n",
    "            ##########################################\n",
    "            #Next item in parsed is from a new document\n",
    "             ##########################################\n",
    "            else: \n",
    "                if docslist:\n",
    "                    nextdoc = docslist.popleft()\n",
    "                covered_set.add(i)\n",
    "                \n",
    "        print(\"{} finished\".format(data_name))\n",
    "        print(\"\\n\")\n",
    "    #Append the last remaining sentences into the global list\n",
    "    global_list.append(local_list)\n",
    "    \n",
    "    print(\"Total number of sent_tokens found: {}\".format(sent_total))\n",
    "    print(\"Number of sentences removed due to non-event: {}\".format(sent_removed))\n",
    "    \n",
    "                    \n",
    "    return global_tuples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createHITs(global_tuples, offset_max=40):\n",
    "    '''\n",
    "    Create HITs based on input global tuples\n",
    "    '''\n",
    "    shuffle(global_tuples)\n",
    "    total_preds = 0\n",
    "    iden=0\n",
    "    global_list = []\n",
    "    local_list = []\n",
    "    global_preds = 0\n",
    "    global_ids = []\n",
    "    for token_dict, pred_token1, pred_token2, num_preds in global_tuples:\n",
    "        if num_preds< 2:\n",
    "            continue\n",
    "\n",
    "        total_preds += num_preds\n",
    "        global_preds +=num_preds \n",
    "        \n",
    "        if total_preds >= offset_max:\n",
    "            total_preds=0\n",
    "            global_ids.append(iden)\n",
    "            global_list.append(local_list)\n",
    "            local_list = []\n",
    "            iden=1\n",
    "            token_dict['id'] = iden\n",
    "            local_list.append(json.dumps(token_dict))\n",
    "            \n",
    "        else:\n",
    "            iden += 1\n",
    "            token_dict['id'] = iden\n",
    "            local_list.append(json.dumps(token_dict))\n",
    "            \n",
    "    #Append the last remaining sentences into the global list\n",
    "    #global_list.append(local_list) # Removing only for the pilot\n",
    "    print(\"Total number of predicates: {}\".format(global_preds))\n",
    "    print(\"Average number of sentences per HIT {}\".format(np.mean(global_ids)))\n",
    "    print(\"Max number of sentences per HIT {}\".format(np.max(global_ids)))\n",
    "    print(\"Min number of sentences per HIT {}\".format(np.min(global_ids)))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    return global_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining adjacent sentences into a single one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ids = {}\n",
    "doc_ids['train'] = load_ud_english(ud_train_detailed)\n",
    "doc_ids['dev'] = load_ud_english(ud_dev_detailed)\n",
    "doc_ids['test'] = load_ud_english(ud_test_detailed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train finished\n",
      "\n",
      "\n",
      "dev finished\n",
      "\n",
      "\n",
      "Total number of sent_tokens found: 740\n",
      "Number of sentences removed due to non-event: 0\n"
     ]
    }
   ],
   "source": [
    "tups_train = extract_list_adjacent([ud_train, ud_dev], doc_ids, set_ids_pilot1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of predicates: 721\n",
      "Average number of sentences per HIT 6.125\n",
      "Max number of sentences per HIT 8\n",
      "Min number of sentences per HIT 4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Length of HIT: 16\n"
     ]
    }
   ],
   "source": [
    "gl_list = createHITs(tups_train)\n",
    "print(\"\\n\")\n",
    "print(\"Length of HIT: {}\".format(len(gl_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a csv file for MTurk:\n",
    "with open('test_protocol2.csv', 'w+') as file_handler:\n",
    "    file_handler.write(\"var_arrays\\n\")\n",
    "    for item in gl_list:\n",
    "        local_str = \"\\\"\" + str(item) + \"\\\"\\n\"\n",
    "        file_handler.write(replace_to_turk(local_str))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
