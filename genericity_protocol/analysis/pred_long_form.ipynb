{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "from scipy.stats import pearsonr as pearson\n",
    "from scipy.stats import spearmanr as spearman\n",
    "from math import isnan\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2394 320 1772 228\n"
     ]
    }
   ],
   "source": [
    "train_file = \"results_train_jun25.csv\"\n",
    "raw_train_file = pd.read_csv(train_file)\n",
    "raw_train_file.columns = [c.replace('.', '_') for c in raw_train_file.columns]\n",
    "\n",
    "train_file_c = \"results_train_aug09.csv\"\n",
    "raw_train_file_c = pd.read_csv(train_file_c)\n",
    "raw_train_file_c.columns = [c.replace('.', '_') for c in raw_train_file_c.columns]\n",
    "\n",
    "devte_file = \"results_devte_jun25.csv\"\n",
    "raw_devte_file = pd.read_csv(devte_file)\n",
    "raw_devte_file.columns = [c.replace('.', '_') for c in raw_devte_file.columns]\n",
    "\n",
    "devte_file_c = \"results_devte_aug09.csv\"\n",
    "raw_devte_file_c = pd.read_csv(devte_file_c)\n",
    "raw_devte_file_c.columns = [c.replace('.', '_') for c in raw_devte_file_c.columns]\n",
    "\n",
    "print(len(raw_train_file), len(raw_train_file_c), len(raw_devte_file), len(raw_devte_file_c))\n",
    "\n",
    "\n",
    "raw_data_file = raw_train_file.append(raw_devte_file, ignore_index=True)\n",
    "raw_data_file_c = raw_train_file_c.append(raw_devte_file_c, ignore_index=True)\n",
    "\n",
    "\n",
    "tmp_file = 'pred_root_token.tsv'\n",
    "tmp = pd.read_csv(tmp_file, sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dataframe(data):\n",
    "    '''\n",
    "    Input: Pandas csv dataframe obtained from MTurk\n",
    "    \n",
    "    Output: Pandas dataframe levelled by (User x Sentenced_ID)\n",
    "    '''\n",
    "    data[\"dicts\"] = data[\"Input_var_arrays\"].map(lambda x: json.loads(x))\n",
    "    global_list = []\n",
    "    \n",
    "    for row in data.itertuples():\n",
    "        for idx, local_dict in enumerate(row.dicts):\n",
    "            temp_dict = local_dict.copy()\n",
    "            var_dyn = \"Answer_pred_dyn\" + str(idx + 1)\n",
    "            var_dyn_c = \"Answer_dyn_conf\" + str(idx + 1)\n",
    "            var_part = \"Answer_pred_part\" + str(idx + 1)\n",
    "            var_part_c = \"Answer_part_conf\" + str(idx + 1)\n",
    "            var_hyp = \"Answer_pred_hyp\" + str(idx + 1)\n",
    "            var_hyp_c = \"Answer_hyp_conf\" + str(idx + 1)\n",
    "            temp_dict['part'] = getattr(row, var_part)\n",
    "            temp_dict['part_conf'] = getattr(row, var_part_c)\n",
    "            temp_dict['dyn'] = getattr(row, var_dyn)\n",
    "            temp_dict['dyn_conf'] = getattr(row, var_dyn_c)\n",
    "            temp_dict['hyp'] = getattr(row, var_hyp)\n",
    "            temp_dict['hyp_conf'] = getattr(row, var_hyp_c)\n",
    "            temp_dict['worker_id'] = row.WorkerId\n",
    "            temp_dict['hit_id'] = row.HITId\n",
    "            temp_dict['status'] = row.AssignmentStatus\n",
    "            global_list.append(temp_dict)\n",
    "    \n",
    "    return pd.DataFrame(global_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5480 40460\n"
     ]
    }
   ],
   "source": [
    "raw_data = extract_dataframe(raw_data_file)\n",
    "raw_data = raw_data[raw_data['status']!='Rejected']\n",
    "raw_data = raw_data.reset_index(drop=True)\n",
    "raw_data['sent_pred'] = raw_data['sent_id'].map(lambda x : x) + \"_\" +\\\n",
    "                           raw_data['pred_token'].map(lambda x: str(x))\n",
    "raw_data['pred_root_token'] = None\n",
    "tmp['sent_pred'] = tmp['sent_id'].map(lambda x : x) + \"_\" +\\\n",
    "                           tmp['pred_token'].map(lambda x: str(x))\n",
    "\n",
    "# predicate root token information was missing from initial data\n",
    "for i, _ in raw_data.iterrows():\n",
    "    raw_data.at[i, 'pred_root_token'] = tmp.loc[tmp['sent_pred'] == \\\n",
    "                                              raw_data.at[i, 'sent_pred'], 'pos'].values[0]\n",
    "    \n",
    "raw_data['sent_pred_root'] = raw_data['sent_id'].map(lambda x : x) + \"_\" +\\\n",
    "                           raw_data['pred_root_token'].map(lambda x: str(x))\n",
    "\n",
    "# Rearrange the columns\n",
    "cols = ['hit_id', 'worker_id','sent_id', 'sent_pred', 'predicate', 'pred_token', \n",
    "        'pred_root_token','part','part_conf', 'dyn','dyn_conf','hyp','hyp_conf']\n",
    "data = raw_data[cols]\n",
    "\n",
    "# Incorporate new annotations\n",
    "raw_data_c = extract_dataframe(raw_data_file_c)\n",
    "raw_data_c = raw_data_c[raw_data_c['status']!='Rejected']\n",
    "raw_data_c = raw_data_c.reset_index(drop=True)\n",
    "raw_data_c['sent_pred'] = raw_data_c['sent_id'].map(lambda x : x) + \"_\" +\\\n",
    "                           raw_data_c['pred_token'].map(lambda x: str(x))\n",
    "raw_data_c.rename(columns={'pred_root_pos':'pred_root_token'}, inplace=True)\n",
    "\n",
    "raw_data_c['sent_pred_root'] = raw_data_c['sent_id'].map(lambda x : x) + \"_\" +\\\n",
    "                               raw_data_c['pred_root_token'].map(lambda x: str(x))\n",
    "data_c = raw_data_c[cols]\n",
    "\n",
    "print(len(data_c), len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data_c = data_c[data_c['sent_id'][6:11] == \"train\"]\n",
    "data = data.append(data_c, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create long form dataset\n",
    "long_cols = ['Split', 'Annotator.ID','Sentence.ID','Pred.Root.Token', 'Pred.Tokens','Predicate',\n",
    "             'Is.Particular', 'Part.Confidence', 'Is.Dynamic','Dyn.Confidence',\n",
    "             'Is.Hypothetical','Hyp.Confidence']\n",
    "\n",
    "long_data = data.copy()\n",
    "long_data = long_data.rename(columns={'worker_id':'Annotator.ID', 'sent_id':'Sentence.ID',\n",
    "                                      'pred_root_token':'Pred.Root.Token', 'pred_token':'Pred.Tokens',\n",
    "                                      'predicate':'Predicate', 'part':'Is.Particular', \n",
    "                                      'part_conf':'Part.Confidence', 'dyn':'Is.Dynamic', \n",
    "                                      'dyn_conf':'Dyn.Confidence', 'hyp':'Is.Hypothetical', \n",
    "                                      'hyp_conf':'Hyp.Confidence'})\n",
    "\n",
    "long_data['Split'] = long_data['Sentence.ID'].str[6:11]\n",
    "long_data['Split'] = long_data['Split'].map(lambda x: x.rstrip('.c'))\n",
    "\n",
    "ann_hash = {}\n",
    "annid = 0\n",
    "for ann in set(long_data['Annotator.ID'].values):\n",
    "    annid += 1\n",
    "    ann_hash[ann] = annid\n",
    "long_data['Annotator.ID'] = long_data['Annotator.ID'].map(ann_hash)\n",
    "long_data = long_data[long_cols]\n",
    "long_data.to_csv('long_data.tsv', sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26740\n",
      "9\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Majority vote in devtest with custom ridit score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = [\"part\", \"dyn\", \"hyp\"]\n",
    "attr_map = {\"part\": \"Is.Particular\", \"dyn\": \"Is.Dynamic\", \"hyp\": \"Is.Hypothetical\"}\n",
    "attr_conf = {\"part\": \"Part.Confidence\", \"dyn\": \"Dyn.Confidence\",\n",
    "             \"hyp\": \"Hyp.Confidence\"}\n",
    "from statistics import mode\n",
    "data = long_data[long_data['Split'] != 'test']\n",
    "\n",
    "response = [\"Is.Particular\", \"Is.Hypothetical\", \"Is.Dynamic\"]\n",
    "response_conf = [\"Part.Confidence\", \"Hyp.Confidence\", \"Dyn.Confidence\"]\n",
    "\n",
    "# Convert responses to 1s and 0s\n",
    "for resp in response:\n",
    "    data[resp] = data[resp].astype(int)\n",
    "\n",
    "for resp in response_conf:\n",
    "    data[resp] = data.groupby('Annotator.ID')[resp].apply(lambda x: x.rank() / (len(x) + 1.))\n",
    "data_dev = data[data['Split'] == 'dev']\n",
    "col = data_dev['Sentence.ID'] + \"_\" + data_dev['Pred.Root.Token'].map(str)\n",
    "data_dev = data_dev.assign(SentenceIDToken=col.values)\n",
    "sent_ids = list(set(data_dev['SentenceIDToken'].tolist()))\n",
    "data_dev_reduced = pd.DataFrame()\n",
    "i = 0\n",
    "print(len(sent_ids))\n",
    "for sent_id in sent_ids:\n",
    "    i += 1\n",
    "    print(i, end=\"\\r\")\n",
    "    new_df = data_dev[data_dev['SentenceIDToken'] == sent_id]\n",
    "    sample = new_df.iloc[0]\n",
    "    \n",
    "    for attr in attributes:\n",
    "        answers = new_df[attr_map[attr]].tolist()\n",
    "        if all(x == answers[0] for x in answers):\n",
    "            mode_ans = answers[0]\n",
    "            new_conf = sum(new_df[attr_conf[attr]].tolist()) / 3\n",
    "        else:\n",
    "            mode_ans = mode(answers)\n",
    "            new_df[new_df[attr_map[attr]] != mode_ans][attr_conf[attr]] = 1 - new_df[new_df[attr_map[attr]] != mode_ans][attr_conf[attr]]\n",
    "            new_conf = sum(new_df[attr_conf[attr]].tolist()) / 3\n",
    "\n",
    "        sample[attr_map[attr]] = mode_ans\n",
    "        sample[attr_conf[attr]] = new_conf\n",
    "\n",
    "    data_dev_reduced = data_dev_reduced.append(sample)\n",
    "data_dev_reduced.to_csv('pred_data_dev.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
