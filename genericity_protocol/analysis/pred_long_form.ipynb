{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "from scipy.stats import pearsonr as pearson\n",
    "from scipy.stats import spearmanr as spearman\n",
    "from math import isnan\n",
    "from collections import Counter\n",
    "from os.path import expanduser\n",
    "pd.set_option('chained_assignment',None)         # Turn off warnings\n",
    "pd.set_option('display.max_columns', None)       # Turns off pandas truncating data\n",
    "\n",
    "home = expanduser('~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2394 320 1772 228\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Read all the various csv files\n",
    "train_file = home + \"/Research/protocols/data/pred_results_train_jun25.csv\"\n",
    "raw_train_file = pd.read_csv(train_file)\n",
    "raw_train_file.columns = [c.replace('.', '_') for c in raw_train_file.columns]\n",
    "\n",
    "train_file_c = home + \"/Research/protocols/data/pred_results_train_aug09.csv\"\n",
    "raw_train_file_c = pd.read_csv(train_file_c)\n",
    "raw_train_file_c.columns = [c.replace('.', '_') for c in raw_train_file_c.columns]\n",
    "\n",
    "devte_file = home + \"/Research/protocols/data/pred_results_devte_jun25.csv\"\n",
    "raw_devte_file = pd.read_csv(devte_file)\n",
    "raw_devte_file.columns = [c.replace('.', '_') for c in raw_devte_file.columns]\n",
    "\n",
    "devte_file_c = home + \"/Research/protocols/data/pred_results_devte_aug09.csv\"\n",
    "raw_devte_file_c = pd.read_csv(devte_file_c)\n",
    "raw_devte_file_c.columns = [c.replace('.', '_') for c in raw_devte_file_c.columns]\n",
    "\n",
    "print(len(raw_train_file), len(raw_train_file_c), len(raw_devte_file), len(raw_devte_file_c))\n",
    "\n",
    "raw_data_file = raw_train_file.append(raw_devte_file, ignore_index=True, sort=False)\n",
    "raw_data_file_c = raw_train_file_c.append(raw_devte_file_c, ignore_index=True, sort=False)\n",
    "\n",
    "tmp_file = home + \"/Research/protocols/data/pred_root_token.tsv\"\n",
    "tmp = pd.read_csv(tmp_file, sep = \"\\t\")\n",
    "print(raw_data_file.columns.values.tolist().sort() == raw_data_file_c.columns.values.tolist().sort())\n",
    "\n",
    "rerun_file = home + \"/Research/protocols/data/pred_results_sep18.csv\"\n",
    "rerun_data_file = pd.read_csv(rerun_file)\n",
    "rerun_data_file.columns = [c.replace('.', '_') for c in rerun_data_file.columns]\n",
    "\n",
    "rerun_file_2 = home + \"/Research/protocols/data/pred_results_oct2.csv\"\n",
    "rerun_data_file_2 = pd.read_csv(rerun_file_2)\n",
    "rerun_data_file_2.columns = [c.replace('.', '_') for c in rerun_data_file.columns]\n",
    "\n",
    "rerun_data_file = rerun_data_file.append(rerun_data_file_2, ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dataframe(data):\n",
    "    '''\n",
    "    Input: Pandas csv dataframe obtained from MTurk\n",
    "    \n",
    "    Output: Pandas dataframe levelled by (User x Sentenced_ID)\n",
    "    '''\n",
    "    data[\"dicts\"] = data[\"Input_var_arrays\"].map(lambda x: json.loads(x))\n",
    "    global_list = []\n",
    "    \n",
    "    for row in data.itertuples():\n",
    "        for idx, local_dict in enumerate(row.dicts):\n",
    "            temp_dict = local_dict.copy()\n",
    "            var_dyn = \"Answer_pred_dyn\" + str(idx + 1)\n",
    "            var_dyn_c = \"Answer_dyn_conf\" + str(idx + 1)\n",
    "            var_part = \"Answer_pred_part\" + str(idx + 1)\n",
    "            var_part_c = \"Answer_part_conf\" + str(idx + 1)\n",
    "            var_hyp = \"Answer_pred_hyp\" + str(idx + 1)\n",
    "            var_hyp_c = \"Answer_hyp_conf\" + str(idx + 1)\n",
    "            temp_dict['part'] = getattr(row, var_part)\n",
    "            temp_dict['part_conf'] = getattr(row, var_part_c)\n",
    "            temp_dict['dyn'] = getattr(row, var_dyn)\n",
    "            temp_dict['dyn_conf'] = getattr(row, var_dyn_c)\n",
    "            temp_dict['hyp'] = getattr(row, var_hyp)\n",
    "            temp_dict['hyp_conf'] = getattr(row, var_hyp_c)\n",
    "            temp_dict['worker_id'] = row.WorkerId\n",
    "            temp_dict['hit_id'] = row.HITId\n",
    "            temp_dict['status'] = row.AssignmentStatus\n",
    "            global_list.append(temp_dict)\n",
    "    \n",
    "    return pd.DataFrame(global_list)\n",
    "\n",
    "def extract_dataframe_1(data):\n",
    "    '''\n",
    "    Input: Pandas csv dataframe obtained from MTurk\n",
    "    \n",
    "    Output: Pandas dataframe levelled by (User x Sentenced_ID)\n",
    "    '''\n",
    "    data[\"dicts\"] = data[\"var_arrays\"].map(lambda x: json.loads(x))\n",
    "    global_list = []\n",
    "    \n",
    "    for row in data.itertuples():\n",
    "        for idx, local_dict in enumerate(row.dicts):\n",
    "            temp_dict = local_dict.copy()\n",
    "            global_list.append(temp_dict)\n",
    "    \n",
    "    return pd.DataFrame(global_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dyn' 'dyn_conf' 'hit_id' 'hyp' 'hyp_conf' 'id' 'part' 'part_conf'\n",
      " 'pred_root_pos' 'pred_sentence' 'pred_token' 'predicate' 'raw_sentence'\n",
      " 'sent_id' 'status' 'worker_id']\n",
      "6430 4320 2110\n",
      "40460\n",
      "5470\n"
     ]
    }
   ],
   "source": [
    "# Remove rejected HITS and bad HITS\n",
    "rerun_data_file = rerun_data_file[rerun_data_file.AssignmentStatus != \"Rejected\"]\n",
    "rerun_data = extract_dataframe(rerun_data_file)\n",
    "print(rerun_data.columns.values)\n",
    "rerun_data_1 = rerun_data[pd.isna(rerun_data.pred_root_pos)]\n",
    "rerun_data_2 = rerun_data[~pd.isna(rerun_data.pred_root_pos)]\n",
    "print(len(rerun_data), len(rerun_data_1), len(rerun_data_2))\n",
    "\n",
    "raw_data = extract_dataframe(raw_data_file)\n",
    "hits = pd.read_csv(home + '/Research/protocols/data/pred_hits_rerun.tsv', sep=\"\\t\")\n",
    "raw_data = raw_data[~raw_data.worker_id.isin(hits.annotator.values.tolist())]\n",
    "raw_data = raw_data.append(rerun_data_1, ignore_index=True, sort=False)\n",
    "raw_data = raw_data[raw_data['status']!='Rejected']\n",
    "raw_data = raw_data.reset_index(drop=True)\n",
    "print(len(raw_data))\n",
    "                   \n",
    "raw_data_c = extract_dataframe(raw_data_file_c)\n",
    "raw_data_c = raw_data_c[~raw_data_c.worker_id.isin(hits.annotator.values.tolist())]\n",
    "raw_data_c = raw_data_c.append(rerun_data_2, ignore_index=True, sort=False)\n",
    "raw_data_c = raw_data_c[raw_data_c['status']!='Rejected']\n",
    "raw_data_c = raw_data_c.reset_index(drop=True)\n",
    "print(len(raw_data_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5470 40460\n"
     ]
    }
   ],
   "source": [
    "raw_data['pred_token'] = raw_data['pred_token'].apply(lambda x: \",\".join([str(int(y) + 1) for y in x.split(',')]))\n",
    "raw_data['sent_pred'] = raw_data['sent_id'] + \"_\" + raw_data['pred_token']\n",
    "\n",
    "tmp['pred_token'] = tmp['pred_token'].apply(lambda x: \",\".join([str(int(y) + 1) for y in x.split(',')]))\n",
    "tmp['pos'] = pd.to_numeric(tmp['pos'])\n",
    "tmp['pos'] += 1\n",
    "tmp['sent_pred'] = tmp['sent_id'] + \"_\" + tmp['pred_token']\n",
    "tmp = tmp.set_index('sent_pred')\n",
    "\n",
    "# predicate root token information was missing from initial data\n",
    "raw_data['pred_root_token'] = raw_data['sent_pred'].apply(lambda x: tmp.loc[x, 'pos'])\n",
    "raw_data['sent_pred_root'] = raw_data['sent_id'] + \"_\" + raw_data['pred_root_token'].map(lambda x: str(x))\n",
    "\n",
    "# Incorporate new annotations\n",
    "raw_data_c['pred_token'] = raw_data_c['pred_token'].apply(lambda x: \",\".join([str(int(y) + 1) for y in x.split(',')]))\n",
    "raw_data_c['sent_pred'] = raw_data_c['sent_id'] + \"_\" + raw_data_c['pred_token']\n",
    "raw_data_c.rename(columns={'pred_root_pos':'pred_root_token'}, inplace=True)\n",
    "raw_data_c['pred_root_token'] = pd.to_numeric(raw_data_c['pred_root_token'])\n",
    "raw_data_c['pred_root_token'] += 1\n",
    "raw_data_c['sent_pred_root'] = raw_data_c['sent_id'] + \"_\" + raw_data_c['pred_root_token'].map(lambda x: str(x))\n",
    "\n",
    "# Rearrange the columns\n",
    "cols = ['hit_id', 'worker_id','sent_id', 'sent_pred', 'sent_pred_root', 'predicate', 'pred_token', \n",
    "        'pred_root_token','part','part_conf', 'dyn','dyn_conf','hyp','hyp_conf']\n",
    "data = raw_data[cols]\n",
    "data_c = raw_data_c[cols]\n",
    "\n",
    "print(len(data_c), len(data))\n",
    "\n",
    "data = data.append(data_c, ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45930, 15)\n",
      "(45900, 15)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "ud_path = \"/Users/venkat/Downloads/UD_English-r1.2/\"\n",
    "\n",
    "files = ['en-ud-train.conllu', 'en-ud-dev.conllu', 'en-ud-test.conllu']\n",
    "lemmas = {}\n",
    "for file in files:\n",
    "    with open(ud_path + file, 'r') as f:\n",
    "        iden = 0\n",
    "        a = \"\"\n",
    "        words = []\n",
    "        for line in f:\n",
    "            if line != \"\\n\":\n",
    "                words.append(line.split(\"\\t\")[2])\n",
    "            else:\n",
    "                iden += 1\n",
    "                sent_id = file + \" sent_\" + str(iden)\n",
    "                lemmas[sent_id] = words\n",
    "                words = []\n",
    "# print(data.loc[139, ['sent_id', 'pred_root_token']])\n",
    "# print(lemmas['en-ud-train.conllu sent_904'][13])\n",
    "# print(data.loc[139, 'pred_root_token'])\n",
    "data.loc[:, 'lemma'] = data.apply(lambda x: lemmas[x['sent_id']][int(x['pred_root_token']) - 1], axis=1)\n",
    "\n",
    "print(data.shape)\n",
    "# duplicate_train_sents = ['en-ud-train.conllu sent_12541_4', 'en-ud-train.conllu sent_12541_13,14',\n",
    "#                          'en-ud-train.conllu sent_12518_14', 'en-ud-train.conllu sent_12518_21',\n",
    "#                          'en-ud-train.conllu sent_12518_36', 'en-ud-train.conllu sent_12518_61',\n",
    "#                          'en-ud-train.conllu sent_12519_6', 'en-ud-train.conllu sent_12521_4',\n",
    "#                          'en-ud-train.conllu sent_12527_4']\n",
    "\n",
    "# duplicate_devte_sents = ['en-ud-test.conllu sent_2075_3', 'en-ud-test.conllu sent_2026_8',\n",
    "#                          'en-ud-test.conllu sent_2026_11', 'en-ud-test.conllu sent_2036_9', \n",
    "#                          'en-ud-test.conllu sent_2039_8', 'en-ud-test.conllu sent_2045_2',\n",
    "#                          'en-ud-test.conllu sent_2057_24']\n",
    "\n",
    "duplicate_train_sents = ['en-ud-train.conllu sent_12541_5', 'en-ud-train.conllu sent_12541_14,15',\n",
    "                         'en-ud-train.conllu sent_12518_15', 'en-ud-train.conllu sent_12518_22',\n",
    "                         'en-ud-train.conllu sent_12518_37', 'en-ud-train.conllu sent_12518_62',\n",
    "                         'en-ud-train.conllu sent_12519_7', 'en-ud-train.conllu sent_12521_5',\n",
    "                         'en-ud-train.conllu sent_12527_5']\n",
    "\n",
    "duplicate_devte_sents = ['en-ud-test.conllu sent_2075_4', 'en-ud-test.conllu sent_2026_9',\n",
    "                         'en-ud-test.conllu sent_2026_12', 'en-ud-test.conllu sent_2036_10', \n",
    "                         'en-ud-test.conllu sent_2039_9', 'en-ud-test.conllu sent_2045_3',\n",
    "                         'en-ud-test.conllu sent_2057_25']\n",
    "for a in duplicate_devte_sents:\n",
    "    data.drop(data[data['sent_pred'] == a].index[0:3], inplace=True)\n",
    "\n",
    "for a in duplicate_train_sents:\n",
    "    data.drop(data[data['sent_pred'] == a].index[0], inplace=True)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from predpatt import load_conllu\n",
    "from predpatt import PredPatt\n",
    "from predpatt import PredPattOpts\n",
    "from os.path import expanduser\n",
    "\n",
    "files = ['/UD_English-r1.2/en-ud-train.conllu',\n",
    "         '/UD_English-r1.2/en-ud-dev.conllu',\n",
    "         '/UD_English-r1.2/en-ud-test.conllu']\n",
    "\n",
    "parsed = []\n",
    "\n",
    "options = PredPattOpts(resolve_relcl=True, borrow_arg_for_relcl=True, resolve_conj=False, cut=False)  # Resolve relative clause\n",
    "options_ = PredPattOpts(resolve_relcl=True, borrow_arg_for_relcl=True, resolve_conj=False, cut=True)  # Resolve relative clause\n",
    "prons_incl = [\"you\", \"they\", \"yourself\", \"themselves\", \"them\", \"themself\",\n",
    "              \"theirself\", \"theirselves\"]\n",
    "for file in files:\n",
    "    path = home + '/Downloads' + file\n",
    "    with open(path, 'r') as infile:\n",
    "        data_in = infile.read()\n",
    "        parsed += [(file[17:] + \" \" + sent_id, PredPatt(ud_parse, opts=options)) for\n",
    "            sent_id, ud_parse in load_conllu(data_in)]\n",
    "\n",
    "id_to_span = {}\n",
    "for sent_id, parse_sen in parsed:\n",
    "    sent_preds = []\n",
    "    for predicate in parse_sen.instances:\n",
    "        sent_check = [pr.position for pr in sent_preds]\n",
    "        if predicate.position not in sent_check:\n",
    "            sent_preds.append(predicate)\n",
    "\n",
    "    for predicate in sent_preds:\n",
    "        if predicate.root.tag not in [\"ADJ\", \"NOUN\", \"NUM\", \"DET\", \"PROPN\", \"PRON\", \"VERB\", \"AUX\"]:\n",
    "            continue\n",
    "        if predicate.root.tag not in [\"VERB\", \"AUX\"]:\n",
    "            gov_rels = [tok.gov_rel for tok in predicate.tokens]\n",
    "            all_pred = [t for t in predicate.tokens]\n",
    "            if 'cop' in gov_rels:\n",
    "                cop_pos = gov_rels.index('cop')\n",
    "                pred = [x.text for x in all_pred[cop_pos:]]\n",
    "                pred_token = [(x.position + 1) for x in all_pred[cop_pos:]]\n",
    "            else:\n",
    "                if predicate.root.tag == \"ADJ\":\n",
    "                    pred = [predicate.root.text]\n",
    "                    pred_token = [predicate.root.position + 1]\n",
    "                else:\n",
    "                    continue\n",
    "        else:\n",
    "            pred = [predicate.root.text]\n",
    "            pred_token = [predicate.root.position + 1]\n",
    "        arguments = predicate.arguments\n",
    "        predpatt_id = sent_id + \"_\" + \",\".join(map(str, pred_token))\n",
    "        pred_span = \",\".join(map(str, [(t.position + 1) for t in predicate.tokens]))\n",
    "        args_context_root = \",\".join(map(str, [(t.root.position + 1) for t in arguments]))\n",
    "        args_context_span = \";\".join(map(str, [[(t.position + 1) for t in a.tokens] for a in arguments])).replace(' ','').replace('[', '').replace(']', '')\n",
    "        id_to_span[predpatt_id] = (pred_span, args_context_root, args_context_span)\n",
    "\n",
    "data.loc[:, 'pred_span'] = data.apply(lambda x: id_to_span.get(x['sent_pred'], (\"null\", \"null\", \"null\"))[0], axis=1)\n",
    "data.loc[:, 'arg_context_root'] = data.apply(lambda x: id_to_span.get(x['sent_pred'], (\"null\", \"null\", \"null\"))[1], axis=1)\n",
    "data.loc[:, 'arg_context_span'] = data.apply(lambda x: id_to_span.get(x['sent_pred'], (\"null\", \"null\", \"null\"))[2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.rename(columns={'hit_id': 'HIT.ID', 'worker_id':'Annotator.ID',\n",
    "                            'sent_id':'Sentence.ID', 'sent_pred': 'Sentence.Ann.Token',\n",
    "                            'sent_pred_root': 'Sentence.Pred.Token',\n",
    "                            'pred_root_token':'Pred.Token', 'pred_token':'Ann.Token',\n",
    "                            'pred_span': 'Pred.Span', 'arg_context_root': 'Arg.Token',\n",
    "                            'arg_context_span': 'Arg.Span',\n",
    "                            'predicate':'Pred.Word', 'lemma':'Pred.Lemma',\n",
    "                            'part':'Is.Particular', 'part_conf':'Part.Confidence',\n",
    "                            'dyn':'Is.Dynamic', 'dyn_conf':'Dyn.Confidence',\n",
    "                            'hyp':'Is.Hypothetical', 'hyp_conf':'Hyp.Confidence'})\n",
    "data.loc[:, 'Split'] = data.loc[:, 'Sentence.ID'].str[6:11]\n",
    "data['Split'] = data['Split'].map(lambda x: x.rstrip('.c'))\n",
    "cols = ['Split', 'HIT.ID', 'Annotator.ID','Sentence.ID', 'Pred.Token','Ann.Token',\n",
    "        'Pred.Span', 'Arg.Token', 'Arg.Span', 'Sentence.Ann.Token', 'Sentence.Pred.Token',\n",
    "        'Pred.Word', 'Pred.Lemma', 'Is.Particular', 'Part.Confidence', 'Is.Dynamic',\n",
    "        'Dyn.Confidence', 'Is.Hypothetical','Hyp.Confidence']\n",
    "data = data[cols]\n",
    "data.to_csv(home + '/Research/protocols/data/FINAL_pred_raw_data.tsv', sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45900\n"
     ]
    }
   ],
   "source": [
    "# Create long form dataset\n",
    "import re\n",
    "long_cols = ['Split', 'Annotator.ID','Sentence.ID','Pred.Token', 'Ann.Token',\n",
    "             'Pred.Span', 'Arg.Token', 'Arg.Span', 'Pred.Word', 'Pred.Lemma',\n",
    "             'Is.Particular', 'Part.Confidence', 'Is.Dynamic','Dyn.Confidence',\n",
    "             'Is.Hypothetical', 'Hyp.Confidence']\n",
    "\n",
    "long_data = data.copy()\n",
    "# long_data['Sentence.ID'] = data['Sentence.ID'].map(lambda x: re.findall(r'\\d+', x)[0])\n",
    "\n",
    "ann_hash = {}\n",
    "annid = 0\n",
    "for ann in set(long_data['Annotator.ID'].values):\n",
    "    annid += 1\n",
    "    ann_hash[ann] = annid\n",
    "long_data['Annotator.ID'] = long_data['Annotator.ID'].map(ann_hash)\n",
    "long_data = long_data[long_cols]\n",
    "print(len(long_data))\n",
    "long_data.to_csv(home + '/Research/protocols/data/FINAL_pred_long_data.tsv', sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
