{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "from scipy.stats import pearsonr as pearson\n",
    "from scipy.stats import spearmanr as spearman\n",
    "from math import isnan\n",
    "from collections import Counter\n",
    "pd.set_option('chained_assignment',None)         # Turn off those dumb annoying warnings\n",
    "pd.set_option('display.max_columns', None)       # Turns off pandas truncating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2394 320 1772 228\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Read all the various csv files\n",
    "train_file = \"../../../data/pred_results_train_jun25.csv\"\n",
    "raw_train_file = pd.read_csv(train_file)\n",
    "raw_train_file.columns = [c.replace('.', '_') for c in raw_train_file.columns]\n",
    "\n",
    "train_file_c = \"../../../data/pred_results_train_aug09.csv\"\n",
    "raw_train_file_c = pd.read_csv(train_file_c)\n",
    "raw_train_file_c.columns = [c.replace('.', '_') for c in raw_train_file_c.columns]\n",
    "\n",
    "devte_file = \"../../../data/pred_results_devte_jun25.csv\"\n",
    "raw_devte_file = pd.read_csv(devte_file)\n",
    "raw_devte_file.columns = [c.replace('.', '_') for c in raw_devte_file.columns]\n",
    "\n",
    "devte_file_c = \"../../../data/pred_results_devte_aug09.csv\"\n",
    "raw_devte_file_c = pd.read_csv(devte_file_c)\n",
    "raw_devte_file_c.columns = [c.replace('.', '_') for c in raw_devte_file_c.columns]\n",
    "\n",
    "print(len(raw_train_file), len(raw_train_file_c), len(raw_devte_file), len(raw_devte_file_c))\n",
    "\n",
    "\n",
    "raw_data_file = raw_train_file.append(raw_devte_file, ignore_index=True)\n",
    "raw_data_file_c = raw_train_file_c.append(raw_devte_file_c, ignore_index=True)\n",
    "\n",
    "\n",
    "tmp_file = '../../../data/pred_root_token.tsv'\n",
    "tmp = pd.read_csv(tmp_file, sep = \"\\t\")\n",
    "print(raw_data_file.columns.values.tolist().sort() == raw_data_file_c.columns.values.tolist().sort())\n",
    "\n",
    "rerun_file = \"../../../data/pred_results_sep18.csv\"\n",
    "rerun_data_file = pd.read_csv(rerun_file)\n",
    "rerun_data_file.columns = [c.replace('.', '_') for c in rerun_data_file.columns]\n",
    "\n",
    "rerun_file_2 = \"../../../data/pred_results_oct2.csv\"\n",
    "rerun_data_file_2 = pd.read_csv(rerun_file_2)\n",
    "rerun_data_file_2.columns = [c.replace('.', '_') for c in rerun_data_file.columns]\n",
    "\n",
    "rerun_data_file = rerun_data_file.append(rerun_data_file_2, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dataframe(data):\n",
    "    '''\n",
    "    Input: Pandas csv dataframe obtained from MTurk\n",
    "    \n",
    "    Output: Pandas dataframe levelled by (User x Sentenced_ID)\n",
    "    '''\n",
    "    data[\"dicts\"] = data[\"Input_var_arrays\"].map(lambda x: json.loads(x))\n",
    "    global_list = []\n",
    "    \n",
    "    for row in data.itertuples():\n",
    "        for idx, local_dict in enumerate(row.dicts):\n",
    "            temp_dict = local_dict.copy()\n",
    "            var_dyn = \"Answer_pred_dyn\" + str(idx + 1)\n",
    "            var_dyn_c = \"Answer_dyn_conf\" + str(idx + 1)\n",
    "            var_part = \"Answer_pred_part\" + str(idx + 1)\n",
    "            var_part_c = \"Answer_part_conf\" + str(idx + 1)\n",
    "            var_hyp = \"Answer_pred_hyp\" + str(idx + 1)\n",
    "            var_hyp_c = \"Answer_hyp_conf\" + str(idx + 1)\n",
    "            temp_dict['part'] = getattr(row, var_part)\n",
    "            temp_dict['part_conf'] = getattr(row, var_part_c)\n",
    "            temp_dict['dyn'] = getattr(row, var_dyn)\n",
    "            temp_dict['dyn_conf'] = getattr(row, var_dyn_c)\n",
    "            temp_dict['hyp'] = getattr(row, var_hyp)\n",
    "            temp_dict['hyp_conf'] = getattr(row, var_hyp_c)\n",
    "            temp_dict['worker_id'] = row.WorkerId\n",
    "            temp_dict['hit_id'] = row.HITId\n",
    "            temp_dict['status'] = row.AssignmentStatus\n",
    "            global_list.append(temp_dict)\n",
    "    \n",
    "    return pd.DataFrame(global_list)\n",
    "\n",
    "def extract_dataframe_1(data):\n",
    "    '''\n",
    "    Input: Pandas csv dataframe obtained from MTurk\n",
    "    \n",
    "    Output: Pandas dataframe levelled by (User x Sentenced_ID)\n",
    "    '''\n",
    "    data[\"dicts\"] = data[\"var_arrays\"].map(lambda x: json.loads(x))\n",
    "    global_list = []\n",
    "    \n",
    "    for row in data.itertuples():\n",
    "        for idx, local_dict in enumerate(row.dicts):\n",
    "            temp_dict = local_dict.copy()\n",
    "            global_list.append(temp_dict)\n",
    "    \n",
    "    return pd.DataFrame(global_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dyn' 'dyn_conf' 'hit_id' 'hyp' 'hyp_conf' 'id' 'part' 'part_conf'\n",
      " 'pred_root_pos' 'pred_sentence' 'pred_token' 'predicate' 'raw_sentence'\n",
      " 'sent_id' 'status' 'worker_id']\n",
      "6430 4320 2110\n"
     ]
    }
   ],
   "source": [
    "# Remove rejected HITS and bad HITS\n",
    "rerun_data_file = rerun_data_file[rerun_data_file.AssignmentStatus != \"Rejected\"]\n",
    "rerun_data = extract_dataframe(rerun_data_file)\n",
    "print(rerun_data.columns.values)\n",
    "rerun_data_1 = rerun_data[pd.isna(rerun_data.pred_root_pos)]\n",
    "rerun_data_2 = rerun_data[~pd.isna(rerun_data.pred_root_pos)]\n",
    "print(len(rerun_data), len(rerun_data_1), len(rerun_data_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/venkat/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py:6211: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5470 40460\n"
     ]
    }
   ],
   "source": [
    "raw_data = extract_dataframe(raw_data_file)\n",
    "hits = pd.read_csv('pred_hits_rerun.tsv', sep=\"\\t\")\n",
    "raw_data = raw_data[~raw_data.worker_id.isin(hits.annotator.values.tolist())]\n",
    "raw_data = raw_data.append(rerun_data_1, ignore_index=True)\n",
    "raw_data = raw_data[raw_data['status']!='Rejected']\n",
    "raw_data = raw_data.reset_index(drop=True)\n",
    "raw_data['sent_pred'] = raw_data['sent_id'].map(lambda x : x) + \"_\" +\\\n",
    "                           raw_data['pred_token'].map(lambda x: str(x))\n",
    "raw_data['pred_root_token'] = None\n",
    "tmp['sent_pred'] = tmp['sent_id'].map(lambda x : x) + \"_\" +\\\n",
    "                           tmp['pred_token'].map(lambda x: str(x))\n",
    "\n",
    "# predicate root token information was missing from initial data\n",
    "for i, _ in raw_data.iterrows():\n",
    "    raw_data.at[i, 'pred_root_token'] = tmp.loc[tmp['sent_pred'] == \\\n",
    "                                              raw_data.at[i, 'sent_pred'], 'pos'].values[0]\n",
    "    \n",
    "raw_data['sent_pred_root'] = raw_data['sent_id'].map(lambda x : x) + \"_\" +\\\n",
    "                           raw_data['pred_root_token'].map(lambda x: str(x))\n",
    "\n",
    "# Rearrange the columns\n",
    "cols = ['hit_id', 'worker_id','sent_id', 'sent_pred', 'sent_pred_root', 'predicate', 'pred_token', \n",
    "        'pred_root_token','part','part_conf', 'dyn','dyn_conf','hyp','hyp_conf']\n",
    "data = raw_data[cols]\n",
    "\n",
    "# Incorporate new annotations\n",
    "raw_data_c = extract_dataframe(raw_data_file_c)\n",
    "raw_data_c = raw_data_c[~raw_data_c.worker_id.isin(hits.annotator.values.tolist())]\n",
    "raw_data_c = raw_data_c.append(rerun_data_2, ignore_index=True)\n",
    "raw_data_c = raw_data_c[raw_data_c['status']!='Rejected']\n",
    "raw_data_c = raw_data_c.reset_index(drop=True)\n",
    "raw_data_c['sent_pred'] = raw_data_c['sent_id'].map(lambda x : x) + \"_\" +\\\n",
    "                           raw_data_c['pred_token'].map(lambda x: str(x))\n",
    "raw_data_c.rename(columns={'pred_root_pos':'pred_root_token'}, inplace=True)\n",
    "\n",
    "raw_data_c['sent_pred_root'] = raw_data_c['sent_id'].map(lambda x : x) + \"_\" +\\\n",
    "                               raw_data_c['pred_root_token'].map(lambda x: str(x))\n",
    "data_c = raw_data_c[cols]\n",
    "\n",
    "print(len(data_c), len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data_c = data_c[data_c['sent_id'][6:11] == \"train\"]\n",
    "data = data.append(data_c, ignore_index=True)\n",
    "\n",
    "import re\n",
    "ud_path = \"/Users/venkat/Downloads/UD_English-r1.2/\"\n",
    "\n",
    "files = ['en-ud-train.conllu', 'en-ud-dev.conllu', 'en-ud-test.conllu']\n",
    "lemmas = {}\n",
    "for file in files:\n",
    "    with open(ud_path + file, 'r') as f:\n",
    "        iden = 0\n",
    "        a = \"\"\n",
    "        words = []\n",
    "        for line in f:\n",
    "            if line != \"\\n\":\n",
    "                words.append(line.split(\"\\t\")[2])\n",
    "            else:\n",
    "                iden += 1\n",
    "                sent_id = file + \" sent_\" + str(iden)\n",
    "                lemmas[sent_id] = words\n",
    "                words = []\n",
    "\n",
    "data.loc[:, 'lemma'] = data.apply(lambda x: lemmas[x.loc['sent_id']][int(x.loc['pred_root_token'])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from predpatt import load_conllu\n",
    "from predpatt import PredPatt\n",
    "from predpatt import PredPattOpts\n",
    "from os.path import expanduser\n",
    "\n",
    "files = ['/UD_English-r1.2/en-ud-train.conllu',\n",
    "         '/UD_English-r1.2/en-ud-dev.conllu',\n",
    "         '/UD_English-r1.2/en-ud-test.conllu']\n",
    "home = expanduser(\"~/Downloads/\")\n",
    "parsed = []\n",
    "\n",
    "options = PredPattOpts(resolve_relcl=True, borrow_arg_for_relcl=True, resolve_conj=False, cut=False)  # Resolve relative clause\n",
    "options_ = PredPattOpts(resolve_relcl=True, borrow_arg_for_relcl=True, resolve_conj=False, cut=True)  # Resolve relative clause\n",
    "prons_incl = [\"you\", \"they\", \"yourself\", \"themselves\", \"them\", \"themself\",\n",
    "              \"theirself\", \"theirselves\"]\n",
    "for file in files:\n",
    "    path = home + file\n",
    "    with open(path, 'r') as infile:\n",
    "        data_in = infile.read()\n",
    "        parsed += [(file[17:] + \" \" + sent_id, PredPatt(ud_parse, opts=options)) for\n",
    "            sent_id, ud_parse in load_conllu(data_in)]\n",
    "        parsed += [(file[17:] + \" \" + sent_id, PredPatt(ud_parse, opts=options_)) for\n",
    "            sent_id, ud_parse in load_conllu(data_in)]\n",
    "id_to_span = {}\n",
    "for sent_id, parse_sen in parsed:\n",
    "    sent_preds = []\n",
    "    for predicate in parse_sen.instances:\n",
    "        sent_check = [pr.position for pr in sent_preds]\n",
    "        if predicate.position not in sent_check:\n",
    "            sent_preds.append(predicate)\n",
    "\n",
    "    for predicate in sent_preds:\n",
    "        if predicate.root.tag not in [\"ADJ\", \"NOUN\", \"NUM\", \"DET\", \"PROPN\", \"PRON\", \"VERB\", \"AUX\"]:\n",
    "            continue\n",
    "        if predicate.root.tag not in [\"VERB\", \"AUX\"]:\n",
    "            gov_rels = [tok.gov_rel for tok in predicate.tokens]\n",
    "            all_pred = [t for t in predicate.tokens]\n",
    "            if 'cop' in gov_rels:\n",
    "                cop_pos = gov_rels.index('cop')\n",
    "                pred = [x.text for x in all_pred[cop_pos:]]\n",
    "                pred_token = [x.position for x in all_pred[cop_pos:]]\n",
    "            else:\n",
    "                if predicate.root.tag == \"ADJ\":\n",
    "                    pred = [predicate.root.text]\n",
    "                    pred_token = [predicate.root.position]\n",
    "                else:\n",
    "                    continue\n",
    "        else:\n",
    "            pred = [predicate.root.text]\n",
    "            pred_token = [predicate.root.position]\n",
    "        arguments = predicate.arguments\n",
    "        predpatt_id = sent_id + \"_\" + \",\".join(map(str, pred_token))\n",
    "        pred_span = \",\".join(map(str, [t.position for t in predicate.tokens]))\n",
    "        args_context = \",\".join(map(str, [t.root.position for t in arguments]))\n",
    "        id_to_span[predpatt_id] = (pred_span, args_context)\n",
    "\n",
    "data.loc[:, 'pred_span'] = data.apply(lambda x: id_to_span.get(x['sent_pred'], (\"null\", \"null\"))[0], axis=1)\n",
    "data.loc[:, 'arg_context'] = data.apply(lambda x: id_to_span.get(x['sent_pred'], (\"null\", \"null\"))[1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hit_id</th>\n",
       "      <th>worker_id</th>\n",
       "      <th>sent_id</th>\n",
       "      <th>sent_pred</th>\n",
       "      <th>sent_pred_root</th>\n",
       "      <th>predicate</th>\n",
       "      <th>pred_token</th>\n",
       "      <th>pred_root_token</th>\n",
       "      <th>part</th>\n",
       "      <th>part_conf</th>\n",
       "      <th>dyn</th>\n",
       "      <th>dyn_conf</th>\n",
       "      <th>hyp</th>\n",
       "      <th>hyp_conf</th>\n",
       "      <th>lemma</th>\n",
       "      <th>pred_span</th>\n",
       "      <th>arg_context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [hit_id, worker_id, sent_id, sent_pred, sent_pred_root, predicate, pred_token, pred_root_token, part, part_conf, dyn, dyn_conf, hyp, hyp_conf, lemma, pred_span, arg_context]\n",
       "Index: []"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['arg_context']==\"null\"]\n",
    "# options = PredPattOpts(resolve_relcl=True, borrow_arg_for_relcl=True, resolve_conj=False, cut=True)  # Resolve relative clause\n",
    "# prons_incl = [\"you\", \"they\", \"yourself\", \"themselves\", \"them\", \"themself\",\n",
    "#               \"theirself\", \"theirselves\"]\n",
    "# for file in files:\n",
    "#     path = home + file\n",
    "#     with open(path, 'r') as infile:\n",
    "#         data_in = infile.read()\n",
    "#         parsed += [(file[17:] + \" \" + sent_id, PredPatt(ud_parse, opts=options)) for\n",
    "#             sent_id, ud_parse in load_conllu(data_in)]\n",
    "\n",
    "# for sent_id, parse_sen in parsed:\n",
    "#     sent_preds = []\n",
    "#     for predicate in parse_sen.instances:\n",
    "#         sent_check = [pr.position for pr in sent_preds]\n",
    "#         if predicate.position not in sent_check:\n",
    "#             sent_preds.append(predicate)\n",
    "\n",
    "#     for predicate in sent_preds:\n",
    "#         if predicate.root.tag not in [\"ADJ\", \"NOUN\", \"NUM\", \"DET\", \"PROPN\", \"PRON\", \"VERB\", \"AUX\"]:\n",
    "#             continue\n",
    "#         if predicate.root.tag not in [\"VERB\", \"AUX\"]:\n",
    "#             gov_rels = [tok.gov_rel for tok in predicate.tokens]\n",
    "#             all_pred = [t for t in predicate.tokens]\n",
    "#             if 'cop' in gov_rels:\n",
    "#                 cop_pos = gov_rels.index('cop')\n",
    "#                 pred = [x.text for x in all_pred[cop_pos:]]\n",
    "#                 pred_token = [x.position for x in all_pred[cop_pos:]]\n",
    "#             else:\n",
    "#                 if predicate.root.tag == \"ADJ\":\n",
    "#                     pred = [predicate.root.text]\n",
    "#                     pred_token = [predicate.root.position]\n",
    "#                 else:\n",
    "#                     continue\n",
    "#         else:\n",
    "#             pred = [predicate.root.text]\n",
    "#             pred_token = [predicate.root.position]\n",
    "#         arguments = predicate.arguments\n",
    "#         predpatt_id = sent_id + \"_\" + \",\".join(map(str, pred_token))\n",
    "#         pred_span = \",\".join(map(str, [t.position for t in predicate.tokens]))\n",
    "#         args_context = \",\".join(map(str, [t.root.position for t in arguments]))\n",
    "#         id_to_span[predpatt_id] = (pred_span, args_context)\n",
    "\n",
    "# data.loc[:, 'pred_span'] = data.apply(lambda x: id_to_span.get(x['sent_pred'], (\"null\", \"null\"))[0], axis=1)\n",
    "# data.loc[:, 'arg_context'] = data.apply(lambda x: id_to_span.get(x['sent_pred'], (\"null\", \"null\"))[1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.rename(columns={'hit_id': 'HIT.ID', 'worker_id':'Annotator.ID', 'sent_id':'Sentence.ID', 'sent_pred': 'Sentence.Pred.Tokens', 'sent_pred_root': 'Sentence.Pred.Root.Token',\n",
    "                            'pred_root_token':'Pred.Root.Token', 'pred_token':'Pred.Tokens', 'pred_span': 'Pred.Span', 'arg_context': 'Argument.Context',\n",
    "                            'predicate':'Predicate', 'lemma':'Predicate.Lemma',\n",
    "                            'part':'Is.Particular', 'part_conf':'Part.Confidence',\n",
    "                            'dyn':'Is.Dynamic', 'dyn_conf':'Dyn.Confidence',\n",
    "                            'hyp':'Is.Hypothetical', 'hyp_conf':'Hyp.Confidence'})\n",
    "data.loc[:, 'Split'] = data.loc[:, 'Sentence.ID'].str[6:11]\n",
    "data['Split'] = data['Split'].map(lambda x: x.rstrip('.c'))\n",
    "cols = ['Split', 'HIT.ID', 'Annotator.ID','Sentence.ID', 'Pred.Root.Token','Pred.Tokens', 'Pred.Span', 'Argument.Context' ,'Sentence.Pred.Tokens', 'Sentence.Pred.Root.Token',\n",
    "        'Predicate', 'Predicate.Lemma', 'Is.Particular', 'Part.Confidence', 'Is.Dynamic','Dyn.Confidence',\n",
    "        'Is.Hypothetical','Hyp.Confidence']\n",
    "data = data[cols]\n",
    "data.to_csv('../../../data/pred_raw_data.tsv', sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45930\n"
     ]
    }
   ],
   "source": [
    "# Create long form dataset\n",
    "import re\n",
    "long_cols = ['Split', 'Annotator.ID','Sentence.ID','Pred.Root.Token', 'Pred.Tokens', 'Pred.Span', 'Argument.Context', 'Predicate',\n",
    "             'Predicate.Lemma', 'Is.Particular', 'Part.Confidence', 'Is.Dynamic','Dyn.Confidence',\n",
    "             'Is.Hypothetical','Hyp.Confidence']\n",
    "\n",
    "long_data = data.copy()\n",
    "# long_data = long_data.rename(columns={'worker_id':'Annotator.ID', 'sent_id':'Sentence.ID',\n",
    "#                                       'pred_root_token':'Pred.Root.Token', 'pred_token':'Pred.Span',\n",
    "#                                       'predicate':'Predicate', 'lemma':'Predicate.Lemma',\n",
    "#                                       'part':'Is.Particular', 'part_conf':'Part.Confidence',\n",
    "#                                       'dyn':'Is.Dynamic', 'dyn_conf':'Dyn.Confidence',\n",
    "#                                       'hyp':'Is.Hypothetical', 'hyp_conf':'Hyp.Confidence'})\n",
    "\n",
    "\n",
    "long_data['Sentence.ID'] = data['Sentence.ID'].map(lambda x: re.findall(r'\\d+', x)[0])\n",
    "\n",
    "ann_hash = {}\n",
    "annid = 0\n",
    "for ann in set(long_data['Annotator.ID'].values):\n",
    "    annid += 1\n",
    "    ann_hash[ann] = annid\n",
    "long_data['Annotator.ID'] = long_data['Annotator.ID'].map(ann_hash)\n",
    "long_data = long_data[long_cols]\n",
    "print(len(long_data))\n",
    "long_data.to_csv('../../../data/pred_long_data.tsv', sep=\"\\t\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
