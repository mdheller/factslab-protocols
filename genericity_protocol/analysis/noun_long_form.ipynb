{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "from scipy.stats import pearsonr as pearson\n",
    "from scipy.stats import spearmanr as spearman\n",
    "from math import isnan\n",
    "from collections import Counter\n",
    "pd.set_option('chained_assignment',None)         # Turn off those dumb annoying warnings\n",
    "pd.set_option('display.max_columns', None)       # Turns off pandas truncating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all the various csv files \n",
    "train_file = \"../../../data/noun_results_train_jun25.csv\"\n",
    "raw_train_file = pd.read_csv(train_file)\n",
    "raw_train_file.columns = [c.replace('.', '_') for c in raw_train_file.columns]\n",
    "\n",
    "train_file_c = \"../../../data/noun_results_train_aug09.csv\"\n",
    "raw_train_file_c = pd.read_csv(train_file_c)\n",
    "raw_train_file_c.columns = [c.replace('.', '_') for c in raw_train_file_c.columns]\n",
    "\n",
    "devte_file = \"../../../data/noun_results_devte_jun25.csv\"\n",
    "raw_devte_file = pd.read_csv(devte_file)\n",
    "raw_devte_file.columns = [c.replace('.', '_') for c in raw_devte_file.columns]\n",
    "\n",
    "devte_file_c = \"../../../data/noun_results_devte_aug09.csv\"\n",
    "raw_devte_file_c = pd.read_csv(devte_file_c)\n",
    "raw_devte_file_c.columns = [c.replace('.', '_') for c in raw_devte_file_c.columns]\n",
    "\n",
    "print(len(raw_train_file), len(raw_train_file_c), len(raw_devte_file), len(raw_devte_file_c))\n",
    "\n",
    "\n",
    "raw_data_file = raw_train_file.append(raw_devte_file, ignore_index=True)\n",
    "raw_data_file_c = raw_train_file_c.append(raw_devte_file_c, ignore_index=True)\n",
    "raw_data_file = raw_data_file.append(raw_data_file_c, ignore_index=True)\n",
    "print(len(raw_data_file))\n",
    "\n",
    "hits = pd.read_csv('noun_hits_rerun.tsv', sep=\"\\t\")\n",
    "hits_to_rerun = raw_data_file[raw_data_file.WorkerId.isin(hits.annotator.values.tolist())]\n",
    "\n",
    "rerun_file = \"../../../data/noun_results_sep18.csv\"\n",
    "rerun_data = pd.read_csv(rerun_file)\n",
    "rerun_data.columns = [c.replace('.', '_') for c in rerun_data.columns]\n",
    "\n",
    "rerun_file_2 = \"../../../data/noun_results_oct2.csv\"\n",
    "rerun_data_2 = pd.read_csv(rerun_file_2)\n",
    "rerun_data_2.columns = [c.replace('.', '_') for c in rerun_data_2.columns]\n",
    "\n",
    "rerun_data = rerun_data.append(rerun_data_2, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dataframe(data):\n",
    "    '''\n",
    "    Input: Pandas csv dataframe obtained from MTurk\n",
    "    \n",
    "    Output: Pandas dataframe levelled by (User x Sentenced_ID)\n",
    "    '''\n",
    "    data[\"dicts\"] = data[\"Input_var_arrays\"].map(lambda x: json.loads(x))\n",
    "    global_list = []\n",
    "    \n",
    "    for row in data.itertuples():\n",
    "        for idx, local_dict in enumerate(row.dicts):\n",
    "            temp_dict = local_dict.copy()\n",
    "            var_part = \"Answer_noun_part\" + str(idx + 1)\n",
    "            var_part_c = \"Answer_noun_part_certainty\" + str(idx + 1)\n",
    "            var_kind = \"Answer_noun_class\" + str(idx + 1)\n",
    "            var_kind_c = \"Answer_noun_class_certainty\" + str(idx + 1)\n",
    "            var_abs = \"Answer_noun_abs\" + str(idx + 1)\n",
    "            var_abs_c = \"Answer_noun_abs_certainty\" + str(idx + 1)\n",
    "            temp_dict['part'] = getattr(row, var_part)\n",
    "            temp_dict['part_conf'] = getattr(row, var_part_c)\n",
    "            temp_dict['kind'] = getattr(row, var_kind)\n",
    "            temp_dict['kind_conf'] = getattr(row, var_kind_c)\n",
    "            temp_dict['abs'] = getattr(row, var_abs)\n",
    "            temp_dict['abs_conf'] = getattr(row, var_abs_c)\n",
    "            temp_dict['worker_id'] = row.WorkerId\n",
    "            temp_dict['hit_id'] = row.HITId\n",
    "            temp_dict['status'] = row.AssignmentStatus\n",
    "            global_list.append(temp_dict)\n",
    "    \n",
    "    return pd.DataFrame(global_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rejected HITS and bad HITS\n",
    "raw_data_file = raw_data_file.append(rerun_data, ignore_index=True)\n",
    "raw_data_file = raw_data_file[raw_data_file.AssignmentStatus != \"Rejected\"]\n",
    "raw_data_file = raw_data_file[~raw_data_file.WorkerId.isin(hits.annotator.values.tolist())]\n",
    "raw_data = extract_dataframe(raw_data_file)\n",
    "raw_data = raw_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column for unique identification of each annotation\n",
    "raw_data.loc[:, 'sent_noun'] = raw_data['sent_id'].map(lambda x : x) + \"_\" +\\\n",
    "                               raw_data['noun_token'].map(lambda x: str(x))\n",
    "\n",
    "# Rearrange the columns\n",
    "cols = ['hit_id', 'worker_id','sent_noun', 'noun', 'sent_id','noun_token','part','part_conf',\n",
    "        'kind','kind_conf','abs','abs_conf']\n",
    "data = raw_data[cols]\n",
    "\n",
    "# Lemma extraction\n",
    "import re\n",
    "ud_path = \"/Users/venkat/Downloads/UD_English-r1.2/\"\n",
    "\n",
    "files = ['en-ud-train.conllu', 'en-ud-dev.conllu', 'en-ud-test.conllu']\n",
    "lemmas = {}\n",
    "for file in files:\n",
    "    with open(ud_path + file, 'r') as f:\n",
    "        iden = 0\n",
    "        a = \"\"\n",
    "        words = []\n",
    "        for line in f:\n",
    "            if line != \"\\n\":\n",
    "                words.append(line.split(\"\\t\")[2])\n",
    "            else:\n",
    "                iden += 1\n",
    "                sent_id = file + \" sent_\" + str(iden)\n",
    "                lemmas[sent_id] = words\n",
    "                words = []\n",
    "\n",
    "data.loc[:, 'lemma'] = data.apply(lambda x: lemmas[x.loc['sent_id']][int(x.loc['noun_token'])], axis=1)\n",
    "\n",
    "data = data.rename(columns={'hit_id': 'HIT.ID', 'worker_id':'Annotator.ID', \n",
    "                            'sent_id':'Sentence.ID', 'sent_noun': 'Sentence.Noun.Token',\n",
    "                            'noun_token':'Noun.Token', 'lemma': 'Noun.Lemma',\n",
    "                            'noun':'Noun', 'part':'Is.Particular', \n",
    "                            'part_conf':'Part.Confidence', 'kind':'Is.Kind', \n",
    "                            'kind_conf':'Kind.Confidence', 'abs':'Is.Abstract', \n",
    "                            'abs_conf':'Abs.Confidence'})\n",
    "\n",
    "data.loc[:, 'Split'] = data.loc[:, 'Sentence.ID'].str[6:11]\n",
    "data['Split'] = data['Split'].map(lambda x: x.rstrip('.c'))\n",
    "cols = ['Split', 'HIT.ID', 'Annotator.ID','Sentence.ID', 'Noun.Token', 'Sentence.Noun.Token',\n",
    "        'Noun', 'Noun.Lemma', 'Is.Particular', 'Part.Confidence', 'Is.Kind','Kind.Confidence',\n",
    "        'Is.Abstract','Abs.Confidence']\n",
    "data = data[cols]\n",
    "data.to_csv('../../../data/noun_raw_data.tsv', sep=\"\\t\", index=False)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to long form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "long_cols = ['Split', 'Annotator.ID','Sentence.ID', 'Noun.Token','Noun', 'Noun.Lemma',\n",
    "             'Is.Particular', 'Part.Confidence', 'Is.Kind','Kind.Confidence',\n",
    "             'Is.Abstract','Abs.Confidence']\n",
    "\n",
    "long_data = data.copy()\n",
    "long_data = long_data.rename(columns={'worker_id':'Annotator.ID', 'sent_id':'Sentence.ID',\n",
    "                                      'noun_token':'Noun.Token', 'lemma': 'Noun.Lemma',\n",
    "                                      'noun':'Noun', 'part':'Is.Particular', \n",
    "                                      'part_conf':'Part.Confidence', 'kind':'Is.Kind', \n",
    "                                      'kind_conf':'Kind.Confidence', 'abs':'Is.Abstract', \n",
    "                                      'abs_conf':'Abs.Confidence'})\n",
    "\n",
    "long_data['Split'] = long_data['Sentence.ID'].str[6:11]\n",
    "long_data['Split'] = long_data['Split'].map(lambda x: x.rstrip('.c'))\n",
    "long_data['Sentence.ID'] = data['Sentence.ID'].map(lambda x: re.findall(r'\\d+', x)[0])\n",
    "\n",
    "ann_hash = {}\n",
    "annid = 0\n",
    "for ann in set(long_data['Annotator.ID'].values):\n",
    "    annid += 1\n",
    "    ann_hash[ann] = annid\n",
    "long_data['Annotator.ID'] = long_data['Annotator.ID'].map(ann_hash)\n",
    "long_data = long_data[long_cols]\n",
    "print(len(long_data))\n",
    "long_data.to_csv('../../../data/noun_long_data.tsv', sep=\"\\t\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
