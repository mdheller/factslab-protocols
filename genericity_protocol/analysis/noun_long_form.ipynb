{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from scipy.stats import pearsonr as pearson\n",
    "from scipy.stats import spearmanr as spearman\n",
    "from math import isnan\n",
    "from collections import Counter\n",
    "from os.path import expanduser\n",
    "pd.set_option('chained_assignment',None)         # Turn off those dumb annoying warnings\n",
    "pd.set_option('display.max_columns', None)       # Turns off pandas truncating data\n",
    "\n",
    "home = expanduser('~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2985 26 2189 18\n",
      "5218\n"
     ]
    }
   ],
   "source": [
    "# Read all the various csv files \n",
    "train_file = home + \"/Research/protocols/data/noun_results_train_jun25.csv\"\n",
    "raw_train_file = pd.read_csv(train_file)\n",
    "raw_train_file.columns = [c.replace('.', '_') for c in raw_train_file.columns]\n",
    "\n",
    "train_file_c = home + \"/Research/protocols/data/noun_results_train_aug09.csv\"\n",
    "raw_train_file_c = pd.read_csv(train_file_c)\n",
    "raw_train_file_c.columns = [c.replace('.', '_') for c in raw_train_file_c.columns]\n",
    "\n",
    "devte_file = home + \"/Research/protocols/data/noun_results_devte_jun25.csv\"\n",
    "raw_devte_file = pd.read_csv(devte_file)\n",
    "raw_devte_file.columns = [c.replace('.', '_') for c in raw_devte_file.columns]\n",
    "\n",
    "devte_file_c = home + \"/Research/protocols/data/noun_results_devte_aug09.csv\"\n",
    "raw_devte_file_c = pd.read_csv(devte_file_c)\n",
    "raw_devte_file_c.columns = [c.replace('.', '_') for c in raw_devte_file_c.columns]\n",
    "\n",
    "print(len(raw_train_file), len(raw_train_file_c), len(raw_devte_file), len(raw_devte_file_c))\n",
    "\n",
    "\n",
    "raw_data_file = raw_train_file.append(raw_devte_file, ignore_index=True)\n",
    "raw_data_file_c = raw_train_file_c.append(raw_devte_file_c, ignore_index=True)\n",
    "raw_data_file = raw_data_file.append(raw_data_file_c, ignore_index=True)\n",
    "print(len(raw_data_file))\n",
    "\n",
    "hits = pd.read_csv(home + \"/Research/protocols/data/noun_hits_rerun.tsv\", sep=\"\\t\")\n",
    "hits_to_rerun = raw_data_file[raw_data_file.WorkerId.isin(hits.annotator.values.tolist())]\n",
    "\n",
    "rerun_file = home + \"/Research/protocols/data/noun_results_sep18.csv\"\n",
    "rerun_data = pd.read_csv(rerun_file)\n",
    "rerun_data.columns = [c.replace('.', '_') for c in rerun_data.columns]\n",
    "\n",
    "rerun_file_2 = home + \"/Research/protocols/data/noun_results_oct2.csv\"\n",
    "rerun_data_2 = pd.read_csv(rerun_file_2)\n",
    "rerun_data_2.columns = [c.replace('.', '_') for c in rerun_data_2.columns]\n",
    "\n",
    "rerun_data = rerun_data.append(rerun_data_2, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dataframe(data):\n",
    "    '''\n",
    "    Input: Pandas csv dataframe obtained from MTurk\n",
    "    \n",
    "    Output: Pandas dataframe levelled by (User x Sentenced_ID)\n",
    "    '''\n",
    "    data[\"dicts\"] = data[\"Input_var_arrays\"].map(lambda x: json.loads(x))\n",
    "    global_list = []\n",
    "    \n",
    "    for row in data.itertuples():\n",
    "        for idx, local_dict in enumerate(row.dicts):\n",
    "            temp_dict = local_dict.copy()\n",
    "            var_part = \"Answer_noun_part\" + str(idx + 1)\n",
    "            var_part_c = \"Answer_noun_part_certainty\" + str(idx + 1)\n",
    "            var_kind = \"Answer_noun_class\" + str(idx + 1)\n",
    "            var_kind_c = \"Answer_noun_class_certainty\" + str(idx + 1)\n",
    "            var_abs = \"Answer_noun_abs\" + str(idx + 1)\n",
    "            var_abs_c = \"Answer_noun_abs_certainty\" + str(idx + 1)\n",
    "            temp_dict['part'] = getattr(row, var_part)\n",
    "            temp_dict['part_conf'] = getattr(row, var_part_c)\n",
    "            temp_dict['kind'] = getattr(row, var_kind)\n",
    "            temp_dict['kind_conf'] = getattr(row, var_kind_c)\n",
    "            temp_dict['abs'] = getattr(row, var_abs)\n",
    "            temp_dict['abs_conf'] = getattr(row, var_abs_c)\n",
    "            temp_dict['worker_id'] = row.WorkerId\n",
    "            temp_dict['hit_id'] = row.HITId\n",
    "            temp_dict['status'] = row.AssignmentStatus\n",
    "            global_list.append(temp_dict)\n",
    "    \n",
    "    return pd.DataFrame(global_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rejected HITS and bad HITS\n",
    "raw_data_file = raw_data_file.append(rerun_data, ignore_index=True)\n",
    "raw_data_file = raw_data_file[raw_data_file.AssignmentStatus != \"Rejected\"]\n",
    "raw_data_file = raw_data_file[~raw_data_file.WorkerId.isin(hits.annotator.values.tolist())]\n",
    "raw_data = extract_dataframe(raw_data_file)\n",
    "raw_data = raw_data.reset_index(drop=True)\n",
    "\n",
    "raw_data['noun_token'] = pd.to_numeric(raw_data['noun_token'])\n",
    "raw_data['noun_token'] += 1                 # Changinging from 0-index to 1-index\n",
    "\n",
    "# Add a column for unique identification of each annotation\n",
    "raw_data.loc[:, 'sent_noun'] = raw_data['sent_id'] + \"_\" + raw_data['noun_token'].map(lambda x: str(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove duplicate annotations(due to padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en-ud-train.conllu sent_12541_2 2\n",
      "en-ud-train.conllu sent_12541_10 2\n",
      "en-ud-train.conllu sent_12541_22 2\n",
      "en-ud-train.conllu sent_12542_2 2\n",
      "en-ud-train.conllu sent_12542_17 2\n",
      "en-ud-train.conllu sent_12542_19 2\n",
      "en-ud-train.conllu sent_12543_13 2\n",
      "en-ud-train.conllu sent_12543_18 2\n",
      "en-ud-train.conllu sent_12543_19 2\n",
      "en-ud-test.conllu sent_2075_3 6\n",
      "en-ud-train.conllu sent_12104_20 2\n",
      "en-ud-train.conllu sent_12200_21 2\n",
      "en-ud-train.conllu sent_12217_8 2\n",
      "en-ud-train.conllu sent_12253_10 2\n",
      "en-ud-train.conllu sent_12281_27 2\n",
      "en-ud-train.conllu sent_12365_20 2\n",
      "en-ud-test.conllu sent_919_28 6\n",
      "en-ud-test.conllu sent_989_19 6\n",
      "en-ud-test.conllu sent_1004_20 6\n",
      "en-ud-test.conllu sent_1022_27 6\n",
      "en-ud-test.conllu sent_1025_21 6\n",
      "en-ud-test.conllu sent_1060_24 6\n",
      "en-ud-test.conllu sent_1339_12 6\n",
      "en-ud-test.conllu sent_1464_57 6\n"
     ]
    }
   ],
   "source": [
    "sid_counts = Counter(raw_data['sent_noun'])\n",
    "for a in sid_counts.keys():\n",
    "    if a[:11]=='en-ud-train':\n",
    "        if sid_counts[a] != 1:\n",
    "            print(a, sid_counts[a])\n",
    "    else:\n",
    "        if sid_counts[a] != 3:\n",
    "            print(a, sid_counts[a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51410, 18)\n",
      "(51368, 13)\n"
     ]
    }
   ],
   "source": [
    "# duplicate_train_sents = ['en-ud-train.conllu sent_12541_1', 'en-ud-train.conllu sent_12541_9', \n",
    "#                          'en-ud-train.conllu sent_12541_21', 'en-ud-train.conllu sent_12542_1',\n",
    "#                          'en-ud-train.conllu sent_12542_16', 'en-ud-train.conllu sent_12542_18', \n",
    "#                          'en-ud-train.conllu sent_12543_12', 'en-ud-train.conllu sent_12543_17', \n",
    "#                          'en-ud-train.conllu sent_12543_18', 'en-ud-train.conllu sent_12104_19', \n",
    "#                          'en-ud-train.conllu sent_12200_20', 'en-ud-train.conllu sent_12217_7',\n",
    "#                          'en-ud-train.conllu sent_12253_9', 'en-ud-train.conllu sent_12281_26', \n",
    "#                          'en-ud-train.conllu sent_12365_19']\n",
    "# duplicate_dev_sents = ['en-ud-test.conllu sent_2075_2', 'en-ud-test.conllu sent_919_27',\n",
    "#                        'en-ud-test.conllu sent_989_18', 'en-ud-test.conllu sent_1004_19',\n",
    "#                        'en-ud-test.conllu sent_1022_26', 'en-ud-test.conllu sent_1025_20',\n",
    "#                        'en-ud-test.conllu sent_1060_23', 'en-ud-test.conllu sent_1339_11',\n",
    "#                        'en-ud-test.conllu sent_1464_56']\n",
    "\n",
    "duplicate_train_sents = ['en-ud-train.conllu sent_12541_2', 'en-ud-train.conllu sent_12541_10', \n",
    "                         'en-ud-train.conllu sent_12541_22', 'en-ud-train.conllu sent_12542_2',\n",
    "                         'en-ud-train.conllu sent_12542_17', 'en-ud-train.conllu sent_12542_19', \n",
    "                         'en-ud-train.conllu sent_12543_13', 'en-ud-train.conllu sent_12543_18', \n",
    "                         'en-ud-train.conllu sent_12543_19', 'en-ud-train.conllu sent_12104_20', \n",
    "                         'en-ud-train.conllu sent_12200_21', 'en-ud-train.conllu sent_12217_8',\n",
    "                         'en-ud-train.conllu sent_12253_10', 'en-ud-train.conllu sent_12281_27', \n",
    "                         'en-ud-train.conllu sent_12365_20']\n",
    "duplicate_dev_sents = ['en-ud-test.conllu sent_2075_3', 'en-ud-test.conllu sent_919_28',\n",
    "                       'en-ud-test.conllu sent_989_19', 'en-ud-test.conllu sent_1004_20',\n",
    "                       'en-ud-test.conllu sent_1022_27', 'en-ud-test.conllu sent_1025_21',\n",
    "                       'en-ud-test.conllu sent_1060_24', 'en-ud-test.conllu sent_1339_12',\n",
    "                       'en-ud-test.conllu sent_1464_57']\n",
    "\n",
    "print(raw_data.shape)\n",
    "for a in duplicate_dev_sents:\n",
    "    raw_data.drop(raw_data[raw_data['sent_noun'] == a].index[0:3], inplace=True)\n",
    "\n",
    "for a in duplicate_train_sents:\n",
    "    raw_data.drop(raw_data[raw_data['sent_noun'] == a].index[0], inplace=True)\n",
    "# Rearrange the columns\n",
    "cols = ['hit_id', 'worker_id','sent_noun', 'noun', 'sent_id','noun_token','part','part_conf',\n",
    "        'kind','kind_conf','abs','abs_conf']\n",
    "data = raw_data[cols]\n",
    "\n",
    "# Lemma extraction\n",
    "import re\n",
    "ud_path = \"/Users/venkat/Downloads/UD_English-r1.2/\"\n",
    "\n",
    "files = ['en-ud-train.conllu', 'en-ud-dev.conllu', 'en-ud-test.conllu']\n",
    "lemmas = {}\n",
    "for file in files:\n",
    "    with open(ud_path + file, 'r') as f:\n",
    "        iden = 0\n",
    "        a = \"\"\n",
    "        words = []\n",
    "        for line in f:\n",
    "            if line != \"\\n\":\n",
    "                words.append(line.split(\"\\t\")[2])\n",
    "            else:\n",
    "                iden += 1\n",
    "                sent_id = file + \" sent_\" + str(iden)\n",
    "                lemmas[sent_id] = words\n",
    "                words = []\n",
    "\n",
    "data.loc[:, 'lemma'] = data.apply(lambda x: lemmas[x.loc['sent_id']][int(x.loc['noun_token']) - 1], axis=1)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the Argument and Predicate Span & Root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from predpatt import load_conllu\n",
    "from predpatt import PredPatt\n",
    "from predpatt import PredPattOpts\n",
    "from os.path import expanduser\n",
    "files = ['/UD_English-r1.2/en-ud-train.conllu',\n",
    "         '/UD_English-r1.2/en-ud-dev.conllu',\n",
    "         '/UD_English-r1.2/en-ud-test.conllu']\n",
    "\n",
    "parsed = []\n",
    "\n",
    "options = PredPattOpts(resolve_relcl=True, borrow_arg_for_relcl=True, resolve_conj=False, cut=True)  # Resolve relative clause\n",
    "prons_incl = [\"you\", \"they\", \"yourself\", \"themselves\", \"them\", \"themself\",\n",
    "              \"theirself\", \"theirselves\"]\n",
    "for file in files:\n",
    "    path = home + '/Downloads' + file\n",
    "    with open(path, 'r') as infile:\n",
    "        data_in = infile.read()\n",
    "        parsed += [(file[17:] + \" \" + sent_id, PredPatt(ud_parse, opts=options)) for\n",
    "            sent_id, ud_parse in load_conllu(data_in)]\n",
    "id_to_span = {}\n",
    "for sent_id, parse_sen in parsed:\n",
    "    sent_args = []\n",
    "    sent_pred_poss = []\n",
    "    for predicate in parse_sen.instances:\n",
    "        if predicate.root.tag in [\"NOUN\"]:\n",
    "            all_args = predicate.arguments + [predicate]\n",
    "        else:\n",
    "            all_args = predicate.arguments\n",
    "        sent_check = [x.position for x in sent_args]\n",
    "        for each_arg in all_args:\n",
    "            if each_arg.position not in sent_check:\n",
    "                sent_args.append(each_arg)\n",
    "                sent_pred_poss.append((str(predicate.root.position + 1), \",\".join(map(str, [(t.position + 1) for t in predicate.tokens]))))\n",
    "    for ij, argument in enumerate(sent_args):\n",
    "        if argument.root.tag in [\"DET\", \"NUM\", \"NOUN\", \"PROPN\", \"PRON\"]:\n",
    "            if argument.root.tag == \"PRON\":\n",
    "                if argument.root.text.lower() not in prons_incl:\n",
    "                    continue\n",
    "        noun_span = [(t.position + 1) for t in argument.tokens]      # 0-index to 1-index\n",
    "        pred_context_root = sent_pred_poss[ij][0]\n",
    "        pred_context_span = sent_pred_poss[ij][1]\n",
    "        predpatt_id = sent_id + \"_\" + str(argument.root.position + 1)\n",
    "        id_to_span[predpatt_id] = (\",\".join(map(str, noun_span)), pred_context_root, pred_context_span)\n",
    "\n",
    "data.loc[:, 'noun_span'] = data.apply(lambda x: id_to_span[x['sent_noun']][0], axis=1)\n",
    "data.loc[:, 'pred_context_root'] = data.apply(lambda x: id_to_span[x['sent_noun']][1], axis=1)\n",
    "data.loc[:, 'pred_context_span'] = data.apply(lambda x: id_to_span[x['sent_noun']][2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51368\n"
     ]
    }
   ],
   "source": [
    "data = data.rename(columns={'hit_id': 'HIT.ID', 'worker_id':'Annotator.ID', \n",
    "                            'sent_id':'Sentence.ID', 'sent_noun': 'Sentence.Arg.Token',\n",
    "                            'noun_token':'Arg.Token', 'noun_span': 'Arg.Span',\n",
    "                            'lemma': 'Arg.Lemma', 'pred_context_root': 'Pred.Token',\n",
    "                            'pred_context_span': 'Pred.Span',\n",
    "                            'noun':'Arg.Word', 'part':'Is.Particular', \n",
    "                            'part_conf':'Part.Confidence', 'kind':'Is.Kind', \n",
    "                            'kind_conf':'Kind.Confidence', 'abs':'Is.Abstract', \n",
    "                            'abs_conf':'Abs.Confidence'})\n",
    "\n",
    "data.loc[:, 'Split'] = data.loc[:, 'Sentence.ID'].str[6:11]\n",
    "data['Split'] = data['Split'].map(lambda x: x.rstrip('.c'))\n",
    "cols = ['Split', 'HIT.ID', 'Annotator.ID','Sentence.ID', 'Arg.Token', 'Arg.Span',\n",
    "        'Sentence.Arg.Token', 'Pred.Token', 'Pred.Span', 'Arg.Word', 'Arg.Lemma',\n",
    "        'Is.Particular','Part.Confidence', 'Is.Kind','Kind.Confidence', 'Is.Abstract',\n",
    "        'Abs.Confidence']\n",
    "data = data[cols]\n",
    "data.to_csv(home + '/Research/protocols/data/FINAL_arg_raw_data.tsv', sep=\"\\t\", index=False)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to long form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51368\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "long_cols = ['Split', 'Annotator.ID','Sentence.ID', 'Arg.Token', 'Arg.Span', 'Pred.Token',\n",
    "             'Pred.Span', 'Arg.Word', 'Arg.Lemma', 'Is.Particular', 'Part.Confidence', 'Is.Kind',\n",
    "             'Kind.Confidence', 'Is.Abstract','Abs.Confidence']\n",
    "\n",
    "long_data = data.copy()\n",
    "# long_data['Sentence.ID'] = data['Sentence.ID'].map(lambda x: re.findall(r'\\d+', x)[0])\n",
    "\n",
    "ann_hash = {}\n",
    "annid = 0\n",
    "for ann in set(long_data['Annotator.ID'].values):\n",
    "    annid += 1\n",
    "    ann_hash[ann] = annid\n",
    "long_data['Annotator.ID'] = long_data['Annotator.ID'].map(ann_hash)\n",
    "long_data = long_data[long_cols]\n",
    "print(len(long_data))\n",
    "long_data.to_csv(home + '/Research/protocols/data/FINAL_arg_long_data.tsv', sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37146\n"
     ]
    }
   ],
   "source": [
    "long_data['Unique.ID'] = long_data.apply(lambda x: str(x['Sentence.ID']) + \"_\" + str(x[\"Arg.Token\"]), axis=1)\n",
    "print(len(long_data['Unique.ID'].unique()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
