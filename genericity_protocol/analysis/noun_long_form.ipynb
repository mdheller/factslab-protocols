{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "from scipy.stats import pearsonr as pearson\n",
    "from scipy.stats import spearmanr as spearman\n",
    "from math import isnan\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2985 26 2189 18\n",
      "5218\n"
     ]
    }
   ],
   "source": [
    "train_file = \"results_train_jun25.csv\"\n",
    "raw_train_file = pd.read_csv(train_file)\n",
    "raw_train_file.columns = [c.replace('.', '_') for c in raw_train_file.columns]\n",
    "\n",
    "train_file_c = \"results_train_aug09.csv\"\n",
    "raw_train_file_c = pd.read_csv(train_file_c)\n",
    "raw_train_file_c.columns = [c.replace('.', '_') for c in raw_train_file_c.columns]\n",
    "\n",
    "devte_file = \"results_devte_jun25.csv\"\n",
    "raw_devte_file = pd.read_csv(devte_file)\n",
    "raw_devte_file.columns = [c.replace('.', '_') for c in raw_devte_file.columns]\n",
    "\n",
    "devte_file_c = \"results_devte_aug09.csv\"\n",
    "raw_devte_file_c = pd.read_csv(devte_file_c)\n",
    "raw_devte_file_c.columns = [c.replace('.', '_') for c in raw_devte_file_c.columns]\n",
    "\n",
    "print(len(raw_train_file), len(raw_train_file_c), len(raw_devte_file), len(raw_devte_file_c))\n",
    "\n",
    "\n",
    "raw_data_file = raw_train_file.append(raw_devte_file, ignore_index=True)\n",
    "raw_data_file_c = raw_train_file_c.append(raw_devte_file_c, ignore_index=True)\n",
    "raw_data_file = raw_data_file.append(raw_data_file_c, ignore_index=True)\n",
    "print(len(raw_data_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dataframe(data):\n",
    "    '''\n",
    "    Input: Pandas csv dataframe obtained from MTurk\n",
    "    \n",
    "    Output: Pandas dataframe levelled by (User x Sentenced_ID)\n",
    "    '''\n",
    "    data[\"dicts\"] = data[\"Input_var_arrays\"].map(lambda x: json.loads(x))\n",
    "    global_list = []\n",
    "    \n",
    "    for row in data.itertuples():\n",
    "        for idx, local_dict in enumerate(row.dicts):\n",
    "            temp_dict = local_dict.copy()\n",
    "            var_part = \"Answer_noun_part\" + str(idx + 1)\n",
    "            var_part_c = \"Answer_noun_part_certainty\" + str(idx + 1)\n",
    "            var_kind = \"Answer_noun_class\" + str(idx + 1)\n",
    "            var_kind_c = \"Answer_noun_class_certainty\" + str(idx + 1)\n",
    "            var_abs = \"Answer_noun_abs\" + str(idx + 1)\n",
    "            var_abs_c = \"Answer_noun_abs_certainty\" + str(idx + 1)\n",
    "            temp_dict['part'] = getattr(row, var_part)\n",
    "            temp_dict['part_conf'] = getattr(row, var_part_c)\n",
    "            temp_dict['kind'] = getattr(row, var_kind)\n",
    "            temp_dict['kind_conf'] = getattr(row, var_kind_c)\n",
    "            temp_dict['abs'] = getattr(row, var_abs)\n",
    "            temp_dict['abs_conf'] = getattr(row, var_abs_c)\n",
    "            temp_dict['worker_id'] = row.WorkerId\n",
    "            temp_dict['hit_id'] = row.HITId\n",
    "            temp_dict['status'] = row.AssignmentStatus\n",
    "            global_list.append(temp_dict)\n",
    "    \n",
    "    return pd.DataFrame(global_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51410\n"
     ]
    }
   ],
   "source": [
    "raw_data = extract_dataframe(raw_data_file)\n",
    "raw_data = raw_data[raw_data['status']!='Rejected']\n",
    "raw_data = raw_data.reset_index(drop=True)\n",
    "raw_data['sent_noun'] = raw_data['sent_id'].map(lambda x : x) + \"_\" +\\\n",
    "                           raw_data['noun_token'].map(lambda x: str(x))\n",
    "# Rearrange the columns\n",
    "cols = ['hit_id', 'worker_id','sent_noun', 'noun', 'sent_id','noun_token','part','part_conf',\n",
    "        'kind','kind_conf','abs','abs_conf']\n",
    "data = raw_data[cols]\n",
    "\n",
    "long_cols = ['Split', 'Annotator.ID','Sentence.ID', 'Noun.Token','Noun',\n",
    "             'Is.Particular', 'Part.Confidence', 'Is.Kind','Kind.Confidence',\n",
    "             'Is.Abstract','Abs.Confidence']\n",
    "\n",
    "long_data = data.copy()\n",
    "long_data = long_data.rename(columns={'worker_id':'Annotator.ID', 'sent_id':'Sentence.ID',\n",
    "                                      'noun_token':'Noun.Token',\n",
    "                                      'noun':'Noun', 'part':'Is.Particular', \n",
    "                                      'part_conf':'Part.Confidence', 'kind':'Is.Kind', \n",
    "                                      'kind_conf':'Kind.Confidence', 'abs':'Is.Abstract', \n",
    "                                      'abs_conf':'Abs.Confidence'})\n",
    "\n",
    "long_data['Split'] = long_data['Sentence.ID'].str[6:11]\n",
    "long_data['Split'] = long_data['Split'].map(lambda x: x.rstrip('.c'))\n",
    "\n",
    "ann_hash = {}\n",
    "annid = 0\n",
    "for ann in set(long_data['Annotator.ID'].values):\n",
    "    annid += 1\n",
    "    ann_hash[ann] = annid\n",
    "long_data['Annotator.ID'] = long_data['Annotator.ID'].map(ann_hash)\n",
    "long_data = long_data[long_cols]\n",
    "print(len(long_data))\n",
    "# long_data.to_csv('noun_long_data.tsv', sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Majority scoring on dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/venkat/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n",
      "/Users/venkat/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3584\n",
      "1\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/venkat/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/venkat/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/venkat/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3584\r"
     ]
    }
   ],
   "source": [
    "attributes = [\"part\", \"kind\", \"abs\"]\n",
    "attr_map = {\"part\": \"Is.Particular\", \"kind\": \"Is.Kind\", \"abs\": \"Is.Abstract\"}\n",
    "attr_conf = {\"part\": \"Part.Confidence\", \"kind\": \"Kind.Confidence\",\n",
    "             \"abs\": \"Abs.Confidence\"}\n",
    "from statistics import mode\n",
    "data = long_data[long_data['Split'] != 'test']\n",
    "\n",
    "response = [\"Is.Particular\", \"Is.Kind\", \"Is.Abstract\"]\n",
    "response_conf = [\"Part.Confidence\", \"Kind.Confidence\", \"Abs.Confidence\"]\n",
    "\n",
    "# Convert responses to 1s and 0s\n",
    "for resp in response:\n",
    "    data[resp] = data[resp].astype(int)\n",
    "\n",
    "for resp in response_conf:\n",
    "    data[resp] = data.groupby('Annotator.ID')[resp].apply(lambda x: x.rank() / (len(x) + 1.))\n",
    "data_dev = data[data['Split'] == 'dev']\n",
    "col = data_dev['Sentence.ID'] + \"_\" + data_dev['Noun.Token'].map(str)\n",
    "data_dev = data_dev.assign(SentenceIDToken=col.values)\n",
    "sent_ids = list(set(data_dev['SentenceIDToken'].tolist()))\n",
    "data_dev_reduced = pd.DataFrame()\n",
    "i = 0\n",
    "print(len(sent_ids))\n",
    "for sent_id in sent_ids:\n",
    "    i += 1\n",
    "    print(i, end=\"\\r\")\n",
    "    new_df = data_dev[data_dev['SentenceIDToken'] == sent_id]\n",
    "    sample = new_df.iloc[0]\n",
    "    \n",
    "    for attr in attributes:\n",
    "        answers = new_df[attr_map[attr]].tolist()\n",
    "        if all(x == answers[0] for x in answers):\n",
    "            mode_ans = answers[0]\n",
    "            new_conf = sum(new_df[attr_conf[attr]].tolist()) / 3\n",
    "        else:\n",
    "            mode_ans = mode(answers)\n",
    "            new_df[new_df[attr_map[attr]] != mode_ans][attr_conf[attr]] = 1 - new_df[new_df[attr_map[attr]] != mode_ans][attr_conf[attr]]\n",
    "            new_conf = sum(new_df[attr_conf[attr]].tolist()) / 3\n",
    "\n",
    "        sample[attr_map[attr]] = mode_ans\n",
    "        sample[attr_conf[attr]] = new_conf\n",
    "\n",
    "    data_dev_reduced = data_dev_reduced.append(sample)\n",
    "data_dev_reduced.to_csv('noun_data_dev.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3584"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_dev_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
