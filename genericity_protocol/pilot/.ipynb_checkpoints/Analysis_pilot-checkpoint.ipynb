{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 1,
>>>>>>> 84611510b2b0e9e0a55dd83d778b4984526fb08d
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "from nltk.metrics.agreement import AnnotationTask\n",
    "from sklearn.metrics import cohen_kappa_score as kappa\n",
    "from sklearn.metrics import accuracy_score as accuracy\n",
    "from scipy.stats import pearsonr as pearson\n",
    "from scipy.stats import spearmanr as spearman\n",
    "from math import isnan\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pilot_file = \"test_devtest.csv\"\n",
    "data = pd.read_csv(pilot_file)\n",
    "\n",
    "#Delete the final HIIT and replace with the new final HIIT\n",
    "data = data[data['HITId']!=\"3J9L0X0VDFRB44GUZ4TSV31037Z9WH\"]\n",
    "final_hit = \"results_pilot_april_26.csv\"\n",
    "data_final = pd.read_csv(final_hit)\n",
    "data = data.append(data_final, ignore_index=True)\n",
    "data.columns = [c.replace('.', '_') for c in data.columns]\n"
=======
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pilot_file = \"pilot_results_May5.csv\"\n",
    "data = pd.read_csv(pilot_file)\n",
    "data.columns = [c.replace('.', '_') for c in data.columns]"
>>>>>>> 84611510b2b0e9e0a55dd83d778b4984526fb08d
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 12,
>>>>>>> 84611510b2b0e9e0a55dd83d778b4984526fb08d
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dataframe(data):\n",
    "    '''\n",
    "    Input: Pandas csv dataframe obtained from MTurk\n",
    "    \n",
    "    Output: Pandas dataframe levelled by (User x Sentenced_ID)\n",
    "    '''\n",
    "    data[\"dicts\"] = data[\"Input_var_arrays\"].map(lambda x: json.loads(x))\n",
    "    global_list = []\n",
    "    \n",
    "    for row in data.itertuples():\n",
    "        for idx, local_dict in enumerate(row.dicts):\n",
    "            temp_dict = local_dict.copy()\n",
<<<<<<< HEAD
    "            var_part = \"Answer_noun_part\" + str(idx + 1)\n",
    "            var_part_c = \"Answer_noun_part_certainty\" + str(idx + 1)\n",
    "            var_kind = \"Answer_noun_class\" + str(idx + 1)\n",
    "            var_kind_c = \"Answer_noun_class_certainty\" + str(idx + 1)\n",
    "            var_abs = \"Answer_noun_abs\" + str(idx + 1)\n",
    "            var_abs_c = \"Answer_noun_abs_certainty\" + str(idx + 1)\n",
    "            temp_dict['part'] = getattr(row, var_part)\n",
    "            temp_dict['part_conf'] = getattr(row, var_part_c)\n",
    "            temp_dict['kind'] = getattr(row, var_kind)\n",
    "            temp_dict['kind_conf'] = getattr(row, var_kind_c)\n",
    "            temp_dict['abs'] = getattr(row, var_abs)\n",
    "            temp_dict['abs_conf'] = getattr(row, var_abs_c)\n",
=======
    "            var_dyn = \"Answer_pred_dyn\" + str(idx + 1)\n",
    "            var_dyn_c = \"Answer_dyn_conf\" + str(idx + 1)\n",
    "            var_part = \"Answer_pred_part\" + str(idx + 1)\n",
    "            var_part_c = \"Answer_part_conf\" + str(idx + 1)\n",
    "            var_hyp = \"Answer_pred_hyp\" + str(idx + 1)\n",
    "            var_hyp_c = \"Answer_hyp_conf\" + str(idx + 1)\n",
    "            temp_dict['part'] = getattr(row, var_part)\n",
    "            temp_dict['part_conf'] = getattr(row, var_part_c)\n",
    "            temp_dict['dyn'] = getattr(row, var_dyn)\n",
    "            temp_dict['dyn_conf'] = getattr(row, var_dyn_c)\n",
    "            temp_dict['hyp'] = getattr(row, var_hyp)\n",
    "            temp_dict['hyp_conf'] = getattr(row, var_hyp_c)\n",
>>>>>>> 84611510b2b0e9e0a55dd83d778b4984526fb08d
    "            temp_dict['worker_id'] = row.WorkerId\n",
    "            temp_dict['hit_id'] = row.HITId\n",
    "            global_list.append(temp_dict)\n",
    "    \n",
    "    return pd.DataFrame(global_list)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', -1)\n",
    "pilot_data = extract_dataframe(data)\n",
    "# Rearrange the columns\n",
    "cols = ['hit_id', 'worker_id','sent_id', 'noun_token', 'raw_sentence', 'noun_token','part','part_conf',\n",
    "        'kind','kind_conf','abs','abs_conf']\n",
    "\n",
    "pilot_data['sent_noun'] = pilot_data['sent_id'].map(lambda x : x) + \"_\" +\\\n",
    "                           pilot_data['noun_token'].map(lambda x: str(x))\n",
    "pilot_data = pilot_data[cols]"
=======
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A366MTQY0JG0EM', 420), ('AFZ0FCH5BDWOM', 370), ('A3E87M4OADYNII', 360), ('A323RBOHUTRW53', 310), ('A2DC9KLE5M9G1A', 290), ('A215S35FQFQYJ1', 280), ('ALJGAZOKIZ9E4', 280), ('A1IZQH072IRQND', 270), ('A2BVVBUTQ4AFH1', 270), ('A25K3EVWC6UAV5', 270), ('A30UE6DWFNUCWX', 260), ('A1AJG4BGSRPC93', 260), ('A36MOV8E6SYHCR', 260), ('A219VCQZADQ45W', 260), ('A31JM9RECQGYEX', 260)]\n",
      "(310, 11)\n",
      "(21650, 11)\n"
     ]
    }
   ],
   "source": [
    "raw_data = extract_dataframe(data)\n",
    "raw_data['sent_pred'] = raw_data['sent_id'].map(lambda x : x) + \"_\" +\\\n",
    "                           raw_data['pred_token'].map(lambda x: str(x))\n",
    "# Rearrange the columns\n",
    "cols = ['hit_id', 'worker_id','sent_pred','sent_id','pred_token','part','part_conf',\n",
    "        'dyn','dyn_conf','hyp','hyp_conf']\n",
    "pilot_data = raw_data[cols]"
>>>>>>> 84611510b2b0e9e0a55dd83d778b4984526fb08d
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "print(pilot_data.pivot_table(index=['part', 'kind', 'abs'], \n",
    "                                  columns='abs_conf', \n",
    "                                  values='worker_id', aggfunc=len))\n",
    "sns.heatmap(pilot_data.pivot_table(index=['part', 'kind', 'abs'], \n",
    "                                  columns='abs_conf', \n",
    "                                  values='worker_id', aggfunc=len).fillna(0))\n",
    "plt.show()"
=======
    "%matplotlib inline\n",
    "\n",
    "print(pilot_data.pivot_table(index=['hyp', 'part', 'dyn'], \n",
    "                                  columns='dyn_conf', \n",
    "                                  values='worker_id', aggfunc=len))\n",
    "sns.heatmap(pilot_data.pivot_table(index=['hyp', 'part', 'dyn'], \n",
    "                                  columns='dyn_conf', \n",
    "                                  values='worker_id', aggfunc=len).fillna(0))"
>>>>>>> 84611510b2b0e9e0a55dd83d778b4984526fb08d
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=2, nrows=3, figsize=(15, 10))\n",
    "sns.countplot(x='part', data=pilot_data, ax=axs[0][0])\n",
    "sns.countplot(x='part_conf', data=pilot_data, ax=axs[0][1])\n",
<<<<<<< HEAD
    "sns.countplot(x='kind', data=pilot_data, ax=axs[1][0])\n",
    "sns.countplot(x='kind_conf', data=pilot_data, ax=axs[1][1])\n",
    "sns.countplot(x='abs', data=pilot_data, ax=axs[2][0])\n",
    "sns.countplot(x='abs_conf', data=pilot_data, ax=axs[2][1])\n",
=======
    "sns.countplot(x='dyn', data=pilot_data, ax=axs[1][0])\n",
    "sns.countplot(x='dyn_conf', data=pilot_data, ax=axs[1][1])\n",
    "sns.countplot(x='hyp', data=pilot_data, ax=axs[2][0])\n",
    "sns.countplot(x='hyp_conf', data=pilot_data, ax=axs[2][1])\n",
>>>>>>> 84611510b2b0e9e0a55dd83d778b4984526fb08d
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inter Annotator agreement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_raw_agreement(data, key_var, check_var):\n",
    "    '''\n",
    "    Input: \n",
    "    1. data: Pandas dataframe\n",
    "    2. key_var: variable based on which raw agreement is to be calculated\n",
    "    3. check_var: vaiable on which raw agreement is calculated\n",
    "    \n",
    "    '''\n",
    "    print(\"####### Raw Count for {} ###########\".format(check_var))\n",
    "    ids = set(list(data[key_var].values))\n",
    "\n",
    "    total_count = len(ids)\n",
    "    raw_count = 0\n",
    "    keys = []\n",
    "    \n",
    "    for iden in ids:\n",
    "        temp = list(data[data[key_var] == iden][check_var].values)\n",
    "        if temp.count(temp[0]) == len(temp):\n",
    "            raw_count += 1\n",
    "            keys.append(iden)\n",
    "     \n",
    "    agreement = (raw_count/total_count)*100\n",
    "    \n",
    "    print(\"Total count of unique {} is {}\".format(key_var, total_count))\n",
    "    print(\"Raw count of matched for {} is {}\".format(check_var, raw_count))\n",
    "    print(\"Inter-annotator agreement for {} is {}%\".format(check_var, agreement))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    return agreement, keys\n",
<<<<<<< HEAD
    "\n",
    "part_agreement, key_part = calc_raw_agreement(pilot_data, 'sent_noun', 'part')\n",
    "kind_agreement, key_kind = calc_raw_agreement(pilot_data, 'sent_noun', 'kind')\n",
    "abs_agreement, key_abs = calc_raw_agreement(pilot_data, 'sent_noun', 'abs')"
=======
    "start_agreement, key_start = calc_raw_agreement(pilot_data, 'sent_pred', 'hyp')\n",
    "instant_agreement, key_inst = calc_raw_agreement(pilot_data, 'sent_pred', 'part')\n",
    "start_agreement, key_start = calc_raw_agreement(pilot_data, 'sent_pred', 'dyn')"
>>>>>>> 84611510b2b0e9e0a55dd83d778b4984526fb08d
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average of accuracy and kappa for each pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pairs_of_workers(data, worker_id):\n",
    "    '''\n",
    "    Given a pandas dataframe, and worker_id variable,\n",
    "    extracts a list of pairs of worker_ids\n",
    "    '''\n",
    "    workers = list(set(data[worker_id].values))\n",
    "    \n",
    "    return list(itertools.combinations(workers, 2))\n",
    "\n",
    "def extract_worker_sent_dict(data, worker_id, sent_id):\n",
    "    '''\n",
    "    Given a pandas dataframe, worker_id variable, and sentence_id variable,\n",
    "    extracts a dict where key is worker_id and value is set(sentences_ids annotated by that worker)\n",
    "    \n",
    "    '''\n",
    "    workers = list(set(data[worker_id].values))\n",
    "    \n",
    "    ans = {}\n",
    "    \n",
    "    for worker in workers:\n",
    "        sents = set(list(data[data[worker_id] == worker][sent_id].values))\n",
    "        ans[worker] = sents\n",
    "        \n",
    "    return ans\n",
    "\n",
    "def average_kappa_acc(data, worker_id, key_var, check_var):\n",
    "    '''\n",
    "    Input: 1. data: pandas dataframe\n",
    "           2. worker_id: Annotator id variable\n",
    "           3. key_var: level of the data (sentence-predicate id)\n",
    "           4. check_var: variable to be checked for kappa score\n",
    "    \n",
    "    Output: kappa score and average accuracy for (pairs of annotators) in the dataset\n",
    "\n",
    "    '''\n",
    "    worker_pairs = extract_pairs_of_workers(data, worker_id)\n",
    "    \n",
    "    worker_key_dict = extract_worker_sent_dict(data, worker_id, key_var)\n",
    "    \n",
    "    kappas = []\n",
    "    accuracies = []\n",
    "    lens = []\n",
    "    for (w1, w2) in worker_pairs:\n",
    "        \n",
    "        common_set = worker_key_dict[w1].intersection(worker_key_dict[w2])\n",
    "        temp1 = []\n",
    "        temp2 = []\n",
    "        \n",
    "        if common_set == set():\n",
    "            continue\n",
<<<<<<< HEAD
    "\n",
=======
    "        if len(common_set) == 150:\n",
    "            print(w1, w2)\n",
>>>>>>> 84611510b2b0e9e0a55dd83d778b4984526fb08d
    "        for key in common_set:\n",
    "            val1 = data[(data[key_var] == key) & \n",
    "                        (data[worker_id] == w1)][check_var].values\n",
    "            val2 = data[(data[key_var] == key) & \n",
    "                        (data[worker_id] == w2)][check_var].values\n",
    "\n",
    "            temp1.append(val1[0])\n",
    "            temp2.append(val2[0])\n",
<<<<<<< HEAD
    "        if not isnan(kappa(temp1, temp2)):\n",
    "            kappas.append(kappa(temp1, temp2))\n",
    "        else:\n",
    "            kappas.append(0)\n",
    "        accuracies.append(accuracy(temp1, temp2))\n",
    "        lens.append(len(temp1))\n",
    "    return kappas, accuracies, lens\n",
=======
    "\n",
    "        kappas.append(kappa(temp1, temp2))\n",
    "        accuracies.append(accuracy(temp1, temp2))\n",
    "        lens.append(len(temp1))\n",
    "    return kappas, accuracies\n",
>>>>>>> 84611510b2b0e9e0a55dd83d778b4984526fb08d
    "\n",
    "def rank_correlation(data, worker_id, key_var, check_var):\n",
    "    '''\n",
    "    Input: 1. data: pandas dataframe\n",
    "           2. worker_id: Annotator id variable\n",
    "           3. key_var: level of the data (sentence-predicate id)\n",
    "           4. check_var: variable to be checked for kappa score\n",
    "    \n",
    "    Output: pearson rank correlation\n",
    "\n",
    "    '''\n",
    "    worker_pairs = extract_pairs_of_workers(data, worker_id)\n",
    "    \n",
    "    worker_key_dict = extract_worker_sent_dict(data, worker_id, key_var)\n",
    "    \n",
    "    corrs = []\n",
    "    accuracies = []\n",
    "    for (w1, w2) in worker_pairs:\n",
    "        \n",
    "        common_set = worker_key_dict[w1].intersection(worker_key_dict[w2])\n",
    "        temp1 = []\n",
    "        temp2 = []\n",
    "        \n",
    "        if common_set == set():\n",
    "            continue\n",
    "\n",
    "        for key in common_set:\n",
    "            val1 = data[(data[key_var] == key) & \n",
    "                        (data[worker_id] == w1)][check_var].values\n",
    "            val2 = data[(data[key_var] == key) & \n",
    "                        (data[worker_id] == w2)][check_var].values\n",
    "\n",
    "            temp1.append(val1[0])\n",
    "            temp2.append(val2[0])\n",
    "\n",
    "        corrs.append(spearman(temp1, temp2)[0])\n",
<<<<<<< HEAD
    "#         accuracies.append(accuracy(temp1, temp2))\n",
=======
    "        accuracies.append(accuracy(temp1, temp2))\n",
>>>>>>> 84611510b2b0e9e0a55dd83d778b4984526fb08d
    "        \n",
    "    return corrs, accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "# ridit scoring\n",
    "# pilot_data['abs_conf']=pilot_data.groupby('worker_id').abs_conf.apply(lambda x: x.rank()/len(x))\n",
    "# pilot_data['part_conf']=pilot_data.groupby('worker_id').part_conf.apply(lambda x: x.rank()/len(x))\n",
    "# pilot_data['kind_conf']=pilot_data.groupby('worker_id').kind_conf.apply(lambda x: x.rank()/len(x))\n",
    "kappas = {}\n",
    "corrs = {}\n",
    "accs = {}\n",
    "lens_d = {}\n",
    "variables = ['part', 'kind', 'abs']\n",
    "variables_ord = ['part_conf', 'kind_conf', 'abs_conf']\n",
    "for var in variables:\n",
    "    kappas[var], accs[var], lens_d[var] = average_kappa_acc(pilot_data, 'worker_id', 'sent_noun', var)\n",
    "\n",
    "for var in variables_ord:\n",
    "    corrs[var], accs[var] = rank_correlation(pilot_data, 'worker_id', 'sent_noun', var)"
=======
    "kappas = {}\n",
    "corrs = {}\n",
    "accs = {}\n",
    "variables = ['hyp', 'part', 'dyn']\n",
    "variables_ord = ['hyp_conf', 'part_conf', 'dyn_conf']\n",
    "for var in variables:\n",
    "    kappas[var], accs[var] = average_kappa_acc(pilot_data, 'worker_id', 'sent_pred', var)\n",
    "\n",
    "for var in variables_ord:\n",
    "    corrs[var], accs[var] = rank_correlation(pilot_data, 'worker_id', 'sent_pred', var)"
>>>>>>> 84611510b2b0e9e0a55dd83d778b4984526fb08d
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa_data = pd.DataFrame.from_dict(kappas)\n",
<<<<<<< HEAD
    "# acc_data = pd.DataFrame.from_dict(accs)\n",
    "corr_data = pd.DataFrame.from_dict(corrs)\n",
    "\n",
    "ax = sns.boxplot(data=kappa_data)\n",
    "ax.set(ylabel='Kappa Score', title=\"Kappa\")\n",
    "plt.show()\n",
    "\n",
    "# ax = sns.boxplot(data=acc_data)\n",
    "# ax.set(ylabel='Accuracy', title=\"Accuracy\")\n",
    "# plt.show()\n",
=======
    "acc_data = pd.DataFrame.from_dict(accs)\n",
    "corr_data = pd.DataFrame.from_dict(corrs)\n",
    "\n",
    "ax = sns.boxplot(data=kappa_data)\n",
    "ax.set(ylabel='Kappa Score using builtin function', title=\"Kappa\")\n",
    "plt.show()\n",
    "print(np.mean(kappa_data))\n",
    "\n",
    "ax = sns.boxplot(data=acc_data)\n",
    "ax.set(ylabel='Accuracy(raw agreement)', title=\"Accuracy\")\n",
    "plt.show()\n",
    "print(np.mean(acc_data))\n",
>>>>>>> 84611510b2b0e9e0a55dd83d778b4984526fb08d
    "\n",
    "ax = sns.boxplot(data=corr_data)\n",
    "ax.set(ylabel='Corr coeff', title=\"Pearsons/Spearman Rank Correlation\")\n",
    "plt.show()\n",
<<<<<<< HEAD
    "\n",
    "kappa_mean = {'part':0, 'kind':0, 'abs':0}\n",
    "for var in variables:\n",
    "    total = sum(lens_d[var])\n",
    "    kappa_mean[var] += sum([kappas[var][i] * lens_d[var][i] / total for i in range(len(lens_d[var]))])\n",
    "print(np.mean(kappa_data))\n",
    "print(kappa_mean) \n",
=======
>>>>>>> 84611510b2b0e9e0a55dd83d778b4984526fb08d
    "print(np.mean(corr_data))"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pilot_data['abs_conf']=pilot_data.groupby('worker_id').abs_conf.apply(lambda x: x.rank()/len(x))\n",
    "pilot_data['part_conf']=pilot_data.groupby('worker_id').part_conf.apply(lambda x: x.rank()/len(x))\n",
    "pilot_data['kind_conf']=pilot_data.groupby('worker_id').kind_conf.apply(lambda x: x.rank()/len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pairs_of_workers(data, worker_id):\n",
    "    '''\n",
    "    Given a pandas dataframe, and worker_id variable,\n",
    "    extracts a list of pairs of worker_ids\n",
    "    '''\n",
    "    workers = list(set(data[worker_id].values))\n",
    "    \n",
    "    return list(itertools.combinations(workers, 2))\n",
    "\n",
    "def extract_worker_sent_dict(data, worker_id, sent_id):\n",
    "    '''\n",
    "    Given a pandas dataframe, worker_id variable, and sentence_id variable,\n",
    "    extracts a dict where key is worker_id and value is \n",
    "    set(sentences_ids annotated by that worker)\n",
    "    \n",
    "    '''\n",
    "    workers = list(set(data[worker_id].values))\n",
    "    \n",
    "    ans = {}\n",
    "    for worker in workers:\n",
    "        sents = set(list(data[data[worker_id] == worker][sent_id].values))\n",
    "        ans[worker] = sents\n",
    "        \n",
    "    return ans\n",
    "\n",
    "def ridit_dataframe(data, worker_id, key_var, check_var):\n",
    "    '''\n",
    "    Input: 1. data: pandas dataframe\n",
    "           2. worker_id: Annotator id variable\n",
    "           3. key_var: level of the data (sentence-predicate id)\n",
    "           4. check_var: variable to be checked for kappa score\n",
    "    \n",
    "    Output: kappa score and average accuracy for (pairs of annotators) in the dataset\n",
    "\n",
    "    '''\n",
    "    worker_pairs = extract_pairs_of_workers(data, worker_id)\n",
    "    \n",
    "    worker_key_dict = extract_worker_sent_dict(data, worker_id, key_var)\n",
    "    \n",
    "    cols = ['hit_id','sent_noun','worker1_id','worker2_id','agreement']\n",
    "    ridit_df = pd.DataFrame(columns=cols)\n",
    "    kappas = []\n",
    "    accuracies = []\n",
    "    for (w1, w2) in worker_pairs:\n",
    "        \n",
    "        common_set = worker_key_dict[w1].intersection(worker_key_dict[w2])\n",
    "        \n",
    "        if common_set == set():\n",
    "            continue\n",
    "\n",
    "        for key in common_set:\n",
    "            val1 = data[(data[key_var] == key) & \n",
    "                        (data[worker_id] == w1)][check_var].values[0]\n",
    "            val2 = data[(data[key_var] == key) & \n",
    "                        (data[worker_id] == w2)][check_var].values[0]\n",
    "            val3 = data[(data[key_var] == key) & \n",
    "                        (data[worker_id] == w2)]['hit_id'].values[0]\n",
    "            agreement = (val1 == val2)\n",
    "            temp_row = {'hit_id': val3, 'sent_noun': key, 'worker1_id': w1, \n",
    "                        'worker2_id': w2, 'agreement': agreement}\n",
    "            ridit_df.append(temp_row, ignore_index=True)\n",
    "            reversed_temp_row = {'hit_id': val3, 'sent_noun': key, 'worker1_id': w2, \n",
    "                                 'worker2_id': w1, 'agreement': agreement}\n",
    "            ridit_df.append(reversed_temp_row, ignore_index=True)\n",
    "        \n",
    "    return ridit_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The flipping happens here\n",
    "# ridit_df = ridit_dataframe(pilot_data,'worker_id','sent_noun','part')\n",
    "# ridit_df.head()"
=======
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run mixed effects model in R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyp"
>>>>>>> 84611510b2b0e9e0a55dd83d778b4984526fb08d
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enables the %%R magic \n",
    "%load_ext rpy2.ipython\n",
    "%R require(ggplot2); require(tidyr); require(lme4)"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Mixed effects model in R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part"
   ]
  },
  {
=======
>>>>>>> 84611510b2b0e9e0a55dd83d778b4984526fb08d
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "%%R -i pilot_data -o df_part\n",
    "\n",
    "#Convert to factors\n",
    "pilot_data$part = as.factor(pilot_data$part)\n",
    "pilot_data$kind = as.factor(pilot_data$kind)\n",
    "pilot_data$abs = as.factor(pilot_data$abs)\n",
    "\n",
    "#Mixed Effects Model\n",
    "model = glmer(part ~ 1 + (1|worker_id) + (1|sent_noun) + (1|hit_id), data=pilot_data,  family=binomial)\n",
    "\n",
    "#Model intercepts:\n",
    "df_part = ranef(model)$worker_id\n",
    "colnames(df_part) <- c('intercept')\n",
    "\n",
    "df_part$glmer_intercept_part = df_part$intercept #constant added manually\n",
    "df_part$worker_id <- rownames(df_part)\n",
=======
    "%%R -i pilot_data -o df_hyp\n",
    "\n",
    "#Convert to factors\n",
    "pilot_data$part = as.factor(pilot_data$part)\n",
    "pilot_data$dyn = as.factor(pilot_data$dyn)\n",
    "pilot_data$hyp = as.factor(pilot_data$hyp)\n",
    "\n",
    "#Mixed Effects Model\n",
    "model = glmer(part ~ 1 + (1|worker_id) + (1|sent_pred) + (1|hit_id), data=pilot_data,  family=binomial)\n",
    "\n",
    "#Model intercepts:\n",
    "df_hyp = ranef(model)$worker_id\n",
    "colnames(df_hyp) <- c('intercept')\n",
    "\n",
    "df_hyp$glmer_intercept_hyp = df_hyp$intercept + 0.4795 #constant added manually\n",
    "df_hyp$worker_id <- rownames(df_hyp)\n",
>>>>>>> 84611510b2b0e9e0a55dd83d778b4984526fb08d
    "print(summary(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "df_part['glmer_intercept_part'] = df_part['glmer_intercept_part'].apply(lambda x: 1/(1+np.exp(-x)))"
=======
    "df_hyp['glmer_intercept_hyp'] = df_hyp['glmer_intercept_hyp'].apply(lambda x: 1/(1+np.exp(-x)))"
>>>>>>> 84611510b2b0e9e0a55dd83d778b4984526fb08d
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "df_part.glmer_intercept_part.plot(kind='density')\n",
=======
    "df_hyp.glmer_intercept_hyp.plot(kind='density')\n",
>>>>>>> 84611510b2b0e9e0a55dd83d778b4984526fb08d
    "plt.title(\"Annotator probability density of saying part = true\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "## Kind"
=======
    "## Part"
>>>>>>> 84611510b2b0e9e0a55dd83d778b4984526fb08d
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "%%R -i pilot_data -o df_kind\n",
    "\n",
    "#Convert to factors\n",
    "pilot_data$part = as.factor(pilot_data$part)\n",
    "pilot_data$kind = as.factor(pilot_data$kind)\n",
    "pilot_data$abs = as.factor(pilot_data$abs)\n",
    "\n",
    "#Mixed Effects Model\n",
    "model = glmer(kind ~ 1 + (1|worker_id) + (1|sent_noun) + (1|hit_id), data=pilot_data,  family=\"binomial\")\n",
    "\n",
    "#Model intercepts:\n",
    "df_kind = ranef(model)$worker_id\n",
    "colnames(df_kind) <- c('intercept')\n",
    "\n",
    "df_kind$glmer_intercept_kind = df_kind$intercept #constant added manually\n",
    "df_kind$worker_id <- rownames(df_kind)\n",
    "\n",
=======
    "%%R -i pilot_data -o df_part\n",
    "\n",
    "#Convert to factors\n",
    "pilot_data$part = as.factor(pilot_data$part)\n",
    "pilot_data$dyn = as.factor(pilot_data$dyn)\n",
    "pilot_data$hyp = as.factor(pilot_data$hyp)\n",
    "\n",
    "#Mixed Effects Model\n",
    "model = glmer(part ~ 1 + (1|worker_id) + (1|sent_pred) + (1|hit_id), data=pilot_data,  family=binomial)\n",
    "\n",
    "#Model intercepts:\n",
    "df_part = ranef(model)$worker_id\n",
    "colnames(df_part) <- c('intercept')\n",
    "\n",
    "df_part$glmer_intercept_part = df_part$intercept + 0.4795 #constant added manually\n",
    "df_part$worker_id <- rownames(df_part)\n",
>>>>>>> 84611510b2b0e9e0a55dd83d778b4984526fb08d
    "print(summary(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "df_kind['glmer_intercept_kind'] = df_kind['glmer_intercept_kind'].apply(lambda x: 1/(1+np.exp(-x)))"
=======
    "df_part['glmer_intercept_part'] = df_part['glmer_intercept_part'].apply(lambda x: 1/(1+np.exp(-x)))"
>>>>>>> 84611510b2b0e9e0a55dd83d778b4984526fb08d
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "df_kind.glmer_intercept_kind.plot(kind='density')\n",
    "plt.title(\"Annotator probability density of saying kind = true\")\n",
=======
    "df_part.glmer_intercept_part.plot(kind='density')\n",
    "plt.title(\"Annotator probability density of saying part = true\")\n",
>>>>>>> 84611510b2b0e9e0a55dd83d778b4984526fb08d
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "## Abs"
=======
    "## Dynamic"
>>>>>>> 84611510b2b0e9e0a55dd83d778b4984526fb08d
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "%%R -i pilot_data -o df_abs\n",
    "\n",
    "#Convert to factors\n",
    "pilot_data$part = as.factor(pilot_data$part)\n",
    "pilot_data$kind = as.factor(pilot_data$kind)\n",
    "pilot_data$abs = as.factor(pilot_data$abs)\n",
    "\n",
    "#Mixed Effects Model\n",
    "model = glmer(abs ~ 1 + (1|worker_id) + (1|sent_noun) + (1|hit_id), data=pilot_data,  family=\"binomial\")\n",
    "\n",
    "#Model intercepts:\n",
    "df_abs = ranef(model)$worker_id\n",
    "colnames(df_abs) <- c('intercept')\n",
    "\n",
    "df_abs$glmer_intercept_abs = df_abs$intercept #constant added manually\n",
    "df_abs$worker_id <- rownames(df_abs)\n",
=======
    "%%R -i pilot_data -o df_dyn\n",
    "\n",
    "#Convert to factors\n",
    "pilot_data$part = as.factor(pilot_data$part)\n",
    "pilot_data$dyn = as.factor(pilot_data$dyn)\n",
    "pilot_data$hyp = as.factor(pilot_data$hyp)\n",
    "#Mixed Effects Model\n",
    "model = glmer(dyn ~ 1 + (1|worker_id) + (1|sent_pred) + (1|hit_id), data=pilot_data,  family=\"binomial\")\n",
    "\n",
    "#Model intercepts:\n",
    "df_dyn = ranef(model)$worker_id\n",
    "colnames(df_dyn) <- c('intercept')\n",
    "\n",
    "df_dyn$glmer_intercept_dyn = df_dyn$intercept - 0.2052 #constant added manually\n",
    "df_dyn$worker_id <- rownames(df_dyn)\n",
>>>>>>> 84611510b2b0e9e0a55dd83d778b4984526fb08d
    "\n",
    "print(summary(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "df_abs['glmer_intercept_abs'] = df_abs['glmer_intercept_abs'].apply(lambda x: 1/(1+np.exp(-x)))"
=======
    "df_dyn['glmer_intercept_dyn'] = df_dyn['glmer_intercept_dyn'].apply(lambda x: 1/(1+np.exp(-x)))"
>>>>>>> 84611510b2b0e9e0a55dd83d778b4984526fb08d
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "df_abs.glmer_intercept_abs.plot(kind='density')\n",
    "plt.title(\"Annotator probability density of saying abstract = true\")\n",
=======
    "df_dyn.glmer_intercept_dyn.plot(kind='density')\n",
    "plt.title(\"Annotator probability density of saying dynamic = true\")\n",
>>>>>>> 84611510b2b0e9e0a55dd83d778b4984526fb08d
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "def extract_pairs_of_workers(data, worker_id):\n",
=======
    "def extract_pairs_of_workers1(data, worker_id):\n",
>>>>>>> 84611510b2b0e9e0a55dd83d778b4984526fb08d
    "    '''\n",
    "    Given a pandas dataframe, and worker_id variable,\n",
    "    extracts a list of pairs of worker_ids\n",
    "    '''\n",
    "    workers = list(set(data[worker_id].values))\n",
    "    \n",
    "    return list(itertools.combinations(workers, 2))\n",
    "\n",
<<<<<<< HEAD
    "def extract_worker_sent_dict(data, worker_id, sent_id):\n",
=======
    "def extract_worker_sent_dict1(data, worker_id, sent_id):\n",
>>>>>>> 84611510b2b0e9e0a55dd83d778b4984526fb08d
    "    '''\n",
    "    Given a pandas dataframe, worker_id variable, and sentence_id variable,\n",
    "    extracts a dict where key is worker_id and value is set(sentences_ids annotated by that worker)\n",
    "    \n",
    "    '''\n",
    "    workers = list(set(data[worker_id].values))\n",
    "    \n",
    "    ans = {}\n",
    "    \n",
    "    for worker in workers:\n",
    "        sents = set(list(data[data[worker_id] == worker][sent_id].values))\n",
    "        ans[worker] = sents\n",
    "        \n",
    "    return ans\n",
    "\n",
<<<<<<< HEAD
    "def average_kappa_acc(data, worker_id, key_var, check_var):\n",
=======
    "def average_kappa_acc1(data, worker_id, key_var, check_var):\n",
>>>>>>> 84611510b2b0e9e0a55dd83d778b4984526fb08d
    "    '''\n",
    "    Input: 1. data: pandas dataframe\n",
    "           2. worker_id: Annotator id variable\n",
    "           3. key_var: level of the data (sentence-predicate id)\n",
    "           4. check_var: variable to be checked for kappa score\n",
    "    \n",
    "    Output: kappa score and average accuracy for (pairs of annotators) in the dataset\n",
    "\n",
    "    '''\n",
<<<<<<< HEAD
    "    worker_pairs = extract_pairs_of_workers(data, worker_id)\n",
    "    \n",
    "    worker_key_dict = extract_worker_sent_dict(data, worker_id, key_var)\n",
    "    if check_var == \"part\":\n",
    "        df = df_part\n",
    "        int_prob = \"glmer_intercept_part\"\n",
    "    elif check_var == \"abs\":\n",
    "        df = df_abs\n",
    "        int_prob = \"glmer_intercept_abs\"\n",
    "    else:\n",
    "        df = df_kind\n",
    "        int_prob = \"glmer_intercept_kind\"\n",
=======
    "    worker_pairs = extract_pairs_of_workers1(data, worker_id)\n",
    "    \n",
    "    worker_key_dict = extract_worker_sent_dict1(data, worker_id, key_var)\n",
    "    if check_var == \"part\":\n",
    "        df = df_part\n",
    "        int_prob = \"glmer_intercept_part\"\n",
    "    elif check_var == \"dyn\":\n",
    "        df = df_dyn\n",
    "        int_prob = \"glmer_intercept_dyn\"\n",
    "    else:\n",
    "        df = df_hyp\n",
    "        int_prob = \"glmer_intercept_hyp\"\n",
>>>>>>> 84611510b2b0e9e0a55dd83d778b4984526fb08d
    "\n",
    "    kappas = []\n",
    "    accuracies = []\n",
    "    for (w1, w2) in worker_pairs:\n",
    "        \n",
    "        common_set = worker_key_dict[w1].intersection(worker_key_dict[w2])\n",
    "        temp1 = []\n",
    "        temp2 = []\n",
    "        \n",
    "        if common_set == set():\n",
    "            continue\n",
    "\n",
    "        for key in common_set:\n",
    "            val1 = data[(data[key_var] == key) & \n",
    "                        (data[worker_id] == w1)][check_var].values\n",
    "            val2 = data[(data[key_var] == key) & \n",
    "                        (data[worker_id] == w2)][check_var].values\n",
    "\n",
    "            temp1.append(val1[0])\n",
    "            temp2.append(val2[0])\n",
    "        accuracies.append(accuracy(temp1, temp2))\n",
    "        # Now for modified kappa calculation\n",
    "        p_e = (df[df[worker_id] == w1][int_prob][0] * df[df[worker_id] == w2][int_prob][0]) + ((1 - df[df[worker_id] == w1][int_prob][0]) * (1 - df[df[worker_id] == w2][int_prob][0]))\n",
    "        kappas.append((accuracies[-1] - p_e) / (1 - p_e))\n",
    "        \n",
<<<<<<< HEAD
    "    return kappas, accuracies"
=======
    "    return kappas"
>>>>>>> 84611510b2b0e9e0a55dd83d778b4984526fb08d
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappas = {}\n",
    "corrs = {}\n",
    "accs = {}\n",
<<<<<<< HEAD
    "variables = ['part', 'kind', 'abs']\n",
    "# variables_ord = ['part_conf', 'kind_conf', 'abs_conf']\n",
    "for var in variables:\n",
    "    kappas[var], accs[var] = average_kappa_acc(pilot_data, 'worker_id', 'sent_noun', var)\n",
    "# for var in variables_ord:\n",
    "#     corrs[var], accs[var] = rank_correlation(pilot_data, 'worker_id', 'sent_noun', var)"
=======
    "variables = ['hyp', 'part', 'dyn']\n",
    "# variables_ord = ['part_conf', 'kind_conf', 'abs_conf']\n",
    "for var in variables:\n",
    "    kappas[var] = average_kappa_acc1(pilot_data, 'worker_id', 'sent_pred', var)"
>>>>>>> 84611510b2b0e9e0a55dd83d778b4984526fb08d
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa_data = pd.DataFrame.from_dict(kappas)\n",
<<<<<<< HEAD
    "acc_data = pd.DataFrame.from_dict(accs)\n",
    "corr_data = pd.DataFrame.from_dict(corrs)\n",
    "\n",
    "ax = sns.boxplot(data=kappa_data)\n",
    "ax.set(ylabel='Kappa Score', title=\"Kappa\")\n",
    "plt.show()\n",
    "\n",
    "ax = sns.boxplot(data=acc_data)\n",
    "ax.set(ylabel='Accuracy', title=\"Accuracy\")\n",
    "plt.show()\n",
    "print(np.mean(kappa_data))\n",
    "print(np.mean(acc_data))"
=======
    "\n",
    "ax = sns.boxplot(data=kappa_data)\n",
    "ax.set(ylabel='Kappa Score', title=\"Kappa from mixed effects mode\")\n",
    "plt.show()\n",
    "\n",
    "print(\"KAPPA\", np.mean(kappa_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if average confidence correlates with agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xyz = list(set(raw_data['sent_pred'].values))\n",
    "raw_data[raw_data['sent_pred'] == \"en-ud-train.conllu sent_5978_3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_list=[]\n",
    "\n",
    "for question in list(set(pilot_data.sent_pred)):\n",
    "    temp = {}\n",
    "    temp['sent_pred'] = question\n",
    "    temp['part_avg_confidence'] = np.mean(list(pilot_data[pilot_data[\"sent_pred\"] == question]['part_conf']))\n",
    "    temp['dyn_avg_confidence'] = np.mean(list(pilot_data[pilot_data[\"sent_pred\"] == question]['dyn_conf']))\n",
    "    p = sum(list(pilot_data[pilot_data[\"sent_pred\"] == question]['part'].astype(int))) / 5\n",
    "    q = sum(list(pilot_data[pilot_data[\"sent_pred\"] == question]['dyn'].astype(int))) / 5\n",
    "    temp['part_agreement'] = p * p + (1 - p) * (1 - p)\n",
    "    temp['dyn_agreement'] = q * q + (1 - q) * (1 - q)\n",
    "    all_list.append(temp)\n",
    "analyse_data = pd.DataFrame(all_list)\n",
    "print(spearman(list(analyse_data['dyn_agreement']), list(analyse_data['dyn_avg_confidence'])))\n",
    "print(spearman(list(analyse_data['part_agreement']), list(analyse_data['part_avg_confidence'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['raw_sentence', 'part', 'part_conf', 'dyn','dyn_conf','hyp','hyp_conf']\n",
    "# check_data = raw_data[cols]\n",
    "for sen in xyz:\n",
    "#     print(check_data[raw_data['sent_pred'] == sen])\n",
    "    print(str(set(raw_data[raw_data['sent_pred'] == sen]['raw_sentence'].values)), str(set(raw_data[raw_data['sent_pred'] == sen]['pred'].values)))\n",
    "    print(\"PART\", sum(list(raw_data[raw_data['sent_pred'] == sen]['part'].astype(int)))/5, \"CONF\", sum(list(raw_data[raw_data['sent_pred'] == sen]['part_conf'].values))/5, \"\\t\\t\",\n",
    "          \"HYP\", sum(list(raw_data[raw_data['sent_pred'] == sen]['hyp'].astype(int)))/5, \"CONF\", sum(list(raw_data[raw_data['sent_pred'] == sen]['hyp_conf'].values))/5, \"\\t\\t\"\n",
    "          \"DYN\", sum(list(raw_data[raw_data['sent_pred'] == sen]['dyn'].astype(int)))/5, \"CONF\", sum(list(raw_data[raw_data['sent_pred'] == sen]['dyn_conf'].values))/5,\"\\n\\n\")"
>>>>>>> 84611510b2b0e9e0a55dd83d778b4984526fb08d
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
