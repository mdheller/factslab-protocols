{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "from nltk.metrics.agreement import AnnotationTask\n",
    "from sklearn.metrics import cohen_kappa_score as kappa\n",
    "from sklearn.metrics import accuracy_score as accuracy\n",
    "from scipy.stats import pearsonr as pearson\n",
    "from scipy.stats import spearmanr as spearman\n",
    "from math import isnan\n",
    "from collections import Counter\n",
    "pd.set_option('max_colwidth', -1)                # shows entire contents of pandas row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_file = \"noun_devtest.csv\"\n",
    "data_file = \"noun_train.csv\"\n",
    "raw_data_file = pd.read_csv(data_file)\n",
    "raw_data_file.columns = [c.replace('.', '_') for c in raw_data_file.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dataframe(data):\n",
    "    '''\n",
    "    Input: Pandas csv dataframe obtained from MTurk\n",
    "    \n",
    "    Output: Pandas dataframe levelled by (User x Sentenced_ID)\n",
    "    '''\n",
    "    data[\"dicts\"] = data[\"Input_var_arrays\"].map(lambda x: json.loads(x))\n",
    "    global_list = []\n",
    "    \n",
    "    for row in data.itertuples():\n",
    "        for idx, local_dict in enumerate(row.dicts):\n",
    "            temp_dict = local_dict.copy()\n",
    "            var_part = \"Answer_noun_part\" + str(idx + 1)\n",
    "            var_part_c = \"Answer_noun_part_certainty\" + str(idx + 1)\n",
    "            var_kind = \"Answer_noun_class\" + str(idx + 1)\n",
    "            var_kind_c = \"Answer_noun_class_certainty\" + str(idx + 1)\n",
    "            var_abs = \"Answer_noun_abs\" + str(idx + 1)\n",
    "            var_abs_c = \"Answer_noun_abs_certainty\" + str(idx + 1)\n",
    "            temp_dict['part'] = getattr(row, var_part)\n",
    "            temp_dict['part_conf'] = getattr(row, var_part_c)\n",
    "            temp_dict['kind'] = getattr(row, var_kind)\n",
    "            temp_dict['kind_conf'] = getattr(row, var_kind_c)\n",
    "            temp_dict['abs'] = getattr(row, var_abs)\n",
    "            temp_dict['abs_conf'] = getattr(row, var_abs_c)\n",
    "            temp_dict['worker_id'] = row.WorkerId\n",
    "            temp_dict['hit_id'] = row.HITId\n",
    "            temp_dict['status'] = row.AssignmentStatus\n",
    "            global_list.append(temp_dict)\n",
    "    \n",
    "    return pd.DataFrame(global_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A3IGFK1YNA4H9O', 400), ('A1P3Z24Y6GRNVA', 330), ('A1JLQNPXN3NHVP', 260), ('AZ5PVV9LDYF26', 260), ('A3CUCMYK88L67N', 260), ('A3DEG7PE4RFCWB', 260), ('A1YW8LOGOAQ7ER', 250), ('A366MTQY0JG0EM', 250), ('AO81QRI85XLIJ', 250), ('A22SF4E1TDPAY6', 250), ('AUT1MH3OQ6T1', 250), ('AMU20U0IG3253', 250), ('AHIJACUG7ZL9B', 250), ('A3EUTYDDTBP4WK', 250), ('A3V83GH53AXO9P', 250)]\n",
      "(29790, 12)\n",
      "(0, 12)\n"
     ]
    }
   ],
   "source": [
    "raw_data = extract_dataframe(raw_data_file)\n",
    "raw_data = raw_data[raw_data['status']!='Rejected']\n",
    "raw_data['sent_noun'] = raw_data['sent_id'].map(lambda x : x) + \"_\" + raw_data['noun_token'].map(lambda x: str(x))\n",
    "\n",
    "cols = ['worker_id', 'sent_id', 'sent_noun', 'noun_token', 'raw_sentence', 'noun',\n",
    "        'part', 'part_conf', 'kind', 'kind_conf', 'abs', 'abs_conf']\n",
    "data = raw_data[cols]\n",
    "\n",
    "x=Counter(list(data['worker_id'].values))\n",
    "print(x.most_common()[:15])\n",
    "print(data.shape)\n",
    "ann_data = data[data['worker_id']=='APHNYDGTCRN3O']\n",
    "# xdata = data[data['worker_id']!='A3IGFK1YNA4H9O']\n",
    "print(ann_data.shape)\n",
    "# print(xdata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=2, nrows=6, figsize=(15, 15))\n",
    "sns.countplot(x='part', data=data, ax=axs[0][0])\n",
    "sns.countplot(x='part', data=ann_data, ax=axs[0][1])\n",
    "sns.countplot(x='kind', data=data, ax=axs[1][0])\n",
    "sns.countplot(x='kind', data=ann_data, ax=axs[1][1])\n",
    "sns.countplot(x='abs', data=data, ax=axs[2][0])\n",
    "sns.countplot(x='abs', data=ann_data, ax=axs[2][1])\n",
    "\n",
    "sns.countplot(x='part_conf', data=data, ax=axs[3][0])\n",
    "sns.countplot(x='part_conf', data=ann_data, ax=axs[3][1])\n",
    "sns.countplot(x='kind_conf', data=data, ax=axs[4][0])\n",
    "sns.countplot(x='kind_conf', data=ann_data, ax=axs[4][1])\n",
    "sns.countplot(x='abs_conf', data=data, ax=axs[5][0])\n",
    "sns.countplot(x='abs_conf', data=ann_data, ax=axs[5][1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.pivot_table(index=['part', 'kind', 'abs'], \n",
    "                                  columns='kind_conf', \n",
    "                                  values='worker_id', aggfunc=len))\n",
    "sns.heatmap(data.pivot_table(index=['part', 'kind', 'abs'], \n",
    "                                  columns='kind_conf', \n",
    "                                  values='worker_id', aggfunc=len).fillna(0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=2, nrows=3, figsize=(15, 10))\n",
    "sns.countplot(x='part', data=data, ax=axs[0][0])\n",
    "sns.countplot(x='part_conf', data=data, ax=axs[0][1])\n",
    "sns.countplot(x='kind', data=data, ax=axs[1][0])\n",
    "sns.countplot(x='kind_conf', data=data, ax=axs[1][1])\n",
    "sns.countplot(x='abs', data=data, ax=axs[2][0])\n",
    "sns.countplot(x='abs_conf', data=data, ax=axs[2][1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inter Annotator agreement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_raw_agreement(data, key_var, check_var):\n",
    "    '''\n",
    "    Input: \n",
    "    1. data: Pandas dataframe\n",
    "    2. key_var: variable based on which raw agreement is to be calculated\n",
    "    3. check_var: vaiable on which raw agreement is calculated\n",
    "    \n",
    "    '''\n",
    "    print(\"####### Raw Count for {} ###########\".format(check_var))\n",
    "    ids = set(list(data[key_var].values))\n",
    "\n",
    "    total_count = len(ids)\n",
    "    raw_count = 0\n",
    "    keys = []\n",
    "    \n",
    "    for iden in ids:\n",
    "        temp = list(data[data[key_var] == iden][check_var].values)\n",
    "        if temp.count(temp[0]) == len(temp):\n",
    "            raw_count += 1\n",
    "            keys.append(iden)\n",
    "     \n",
    "    agreement = (raw_count/total_count)*100\n",
    "    \n",
    "    print(\"Total count of unique {} is {}\".format(key_var, total_count))\n",
    "    print(\"Raw count of matched for {} is {}\".format(check_var, raw_count))\n",
    "    print(\"Inter-annotator agreement for {} is {}%\".format(check_var, agreement))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    return agreement, keys\n",
    "\n",
    "part_agreement, key_part = calc_raw_agreement(data, 'sent_noun', 'part')\n",
    "kind_agreement, key_kind = calc_raw_agreement(data, 'sent_noun', 'kind')\n",
    "abs_agreement, key_abs = calc_raw_agreement(data, 'sent_noun', 'abs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average of accuracy and kappa for each pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pairs_of_workers(data, worker_id):\n",
    "    '''\n",
    "    Given a pandas dataframe, and worker_id variable,\n",
    "    extracts a list of pairs of worker_ids\n",
    "    '''\n",
    "    workers = list(set(data[worker_id].values))\n",
    "    \n",
    "    return list(itertools.combinations(workers, 2))\n",
    "\n",
    "def extract_worker_sent_dict(data, worker_id, sent_id):\n",
    "    '''\n",
    "    Given a pandas dataframe, worker_id variable, and sentence_id variable,\n",
    "    extracts a dict where key is worker_id and value is set(sentences_ids annotated by that worker)\n",
    "    \n",
    "    '''\n",
    "    workers = list(set(data[worker_id].values))\n",
    "    \n",
    "    ans = {}\n",
    "    \n",
    "    for worker in workers:\n",
    "        sents = set(list(data[data[worker_id] == worker][sent_id].values))\n",
    "        ans[worker] = sents\n",
    "        \n",
    "    return ans\n",
    "\n",
    "def average_kappa_acc(data, worker_id, key_var, check_var):\n",
    "    '''\n",
    "    Input: 1. data: pandas dataframe\n",
    "           2. worker_id: Annotator id variable\n",
    "           3. key_var: level of the data (sentence-predicate id)\n",
    "           4. check_var: variable to be checked for kappa score\n",
    "    \n",
    "    Output: kappa score and average accuracy for (pairs of annotators) in the dataset\n",
    "\n",
    "    '''\n",
    "    worker_pairs = extract_pairs_of_workers(data, worker_id)\n",
    "    \n",
    "    worker_key_dict = extract_worker_sent_dict(data, worker_id, key_var)\n",
    "    \n",
    "    kappas = []\n",
    "    accuracies = []\n",
    "    lens = []\n",
    "    for (w1, w2) in worker_pairs:\n",
    "        \n",
    "        common_set = worker_key_dict[w1].intersection(worker_key_dict[w2])\n",
    "        temp1 = []\n",
    "        temp2 = []\n",
    "        \n",
    "        if common_set == set():\n",
    "            continue\n",
    "\n",
    "        for key in common_set:\n",
    "            val1 = data[(data[key_var] == key) & \n",
    "                        (data[worker_id] == w1)][check_var].values\n",
    "            val2 = data[(data[key_var] == key) & \n",
    "                        (data[worker_id] == w2)][check_var].values\n",
    "\n",
    "            temp1.append(val1[0])\n",
    "            temp2.append(val2[0])\n",
    "        if not isnan(kappa(temp1, temp2)):\n",
    "            kappas.append(kappa(temp1, temp2))\n",
    "        else:\n",
    "            kappas.append(0)\n",
    "        accuracies.append(accuracy(temp1, temp2))\n",
    "        lens.append(len(temp1))\n",
    "    return kappas, accuracies, lens\n",
    "\n",
    "def rank_correlation(data, worker_id, key_var, check_var):\n",
    "    '''\n",
    "    Input: 1. data: pandas dataframe\n",
    "           2. worker_id: Annotator id variable\n",
    "           3. key_var: level of the data (sentence-predicate id)\n",
    "           4. check_var: variable to be checked for kappa score\n",
    "    \n",
    "    Output: pearson rank correlation\n",
    "\n",
    "    '''\n",
    "    worker_pairs = extract_pairs_of_workers(data, worker_id)\n",
    "    \n",
    "    worker_key_dict = extract_worker_sent_dict(data, worker_id, key_var)\n",
    "    \n",
    "    corrs = []\n",
    "    accuracies = []\n",
    "    for (w1, w2) in worker_pairs:\n",
    "        \n",
    "        common_set = worker_key_dict[w1].intersection(worker_key_dict[w2])\n",
    "        temp1 = []\n",
    "        temp2 = []\n",
    "        \n",
    "        if common_set == set():\n",
    "            continue\n",
    "\n",
    "        for key in common_set:\n",
    "            val1 = data[(data[key_var] == key) & \n",
    "                        (data[worker_id] == w1)][check_var].values\n",
    "            val2 = data[(data[key_var] == key) & \n",
    "                        (data[worker_id] == w2)][check_var].values\n",
    "\n",
    "            temp1.append(val1[0])\n",
    "            temp2.append(val2[0])\n",
    "\n",
    "        corrs.append(spearman(temp1, temp2)[0])\n",
    "#         accuracies.append(accuracy(temp1, temp2))\n",
    "        \n",
    "    return corrs, accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ridit scoring\n",
    "# data['abs_conf']=data.groupby('worker_id').abs_conf.apply(lambda x: x.rank()/len(x))\n",
    "# data['part_conf']=data.groupby('worker_id').part_conf.apply(lambda x: x.rank()/len(x))\n",
    "# data['kind_conf']=data.groupby('worker_id').kind_conf.apply(lambda x: x.rank()/len(x))\n",
    "kappas = {}\n",
    "corrs = {}\n",
    "accs = {}\n",
    "lens_d = {}\n",
    "variables = ['part', 'kind', 'abs']\n",
    "variables_ord = ['part_conf', 'kind_conf', 'abs_conf']\n",
    "for var in variables:\n",
    "    kappas[var], accs[var], lens_d[var] = average_kappa_acc(data, 'worker_id', 'sent_noun', var)\n",
    "\n",
    "for var in variables_ord:\n",
    "    corrs[var], accs[var] = rank_correlation(data, 'worker_id', 'sent_noun', var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa_data = pd.DataFrame.from_dict(kappas)\n",
    "# acc_data = pd.DataFrame.from_dict(accs)\n",
    "corr_data = pd.DataFrame.from_dict(corrs)\n",
    "\n",
    "ax = sns.boxplot(data=kappa_data)\n",
    "ax.set(ylabel='Kappa Score', title=\"Kappa\")\n",
    "plt.show()\n",
    "\n",
    "# ax = sns.boxplot(data=acc_data)\n",
    "# ax.set(ylabel='Accuracy', title=\"Accuracy\")\n",
    "# plt.show()\n",
    "\n",
    "ax = sns.boxplot(data=corr_data)\n",
    "ax.set(ylabel='Corr coeff', title=\"Pearsons/Spearman Rank Correlation\")\n",
    "plt.show()\n",
    "\n",
    "kappa_mean = {'part':0, 'kind':0, 'abs':0}\n",
    "for var in variables:\n",
    "    total = sum(lens_d[var])\n",
    "    kappa_mean[var] += sum([kappas[var][i] * lens_d[var][i] / total for i in range(len(lens_d[var]))])\n",
    "print(np.mean(kappa_data))\n",
    "print(kappa_mean) \n",
    "print(np.mean(corr_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['abs_conf']=data.groupby('worker_id').abs_conf.apply(lambda x: x.rank()/len(x))\n",
    "data['part_conf']=data.groupby('worker_id').part_conf.apply(lambda x: x.rank()/len(x))\n",
    "data['kind_conf']=data.groupby('worker_id').kind_conf.apply(lambda x: x.rank()/len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pairs_of_workers(data, worker_id):\n",
    "    '''\n",
    "    Given a pandas dataframe, and worker_id variable,\n",
    "    extracts a list of pairs of worker_ids\n",
    "    '''\n",
    "    workers = list(set(data[worker_id].values))\n",
    "    \n",
    "    return list(itertools.combinations(workers, 2))\n",
    "\n",
    "def extract_worker_sent_dict(data, worker_id, sent_id):\n",
    "    '''\n",
    "    Given a pandas dataframe, worker_id variable, and sentence_id variable,\n",
    "    extracts a dict where key is worker_id and value is \n",
    "    set(sentences_ids annotated by that worker)\n",
    "    \n",
    "    '''\n",
    "    workers = list(set(data[worker_id].values))\n",
    "    \n",
    "    ans = {}\n",
    "    for worker in workers:\n",
    "        sents = set(list(data[data[worker_id] == worker][sent_id].values))\n",
    "        ans[worker] = sents\n",
    "        \n",
    "    return ans\n",
    "\n",
    "def ridit_dataframe(data, worker_id, key_var, check_var):\n",
    "    '''\n",
    "    Input: 1. data: pandas dataframe\n",
    "           2. worker_id: Annotator id variable\n",
    "           3. key_var: level of the data (sentence-predicate id)\n",
    "           4. check_var: variable to be checked for kappa score\n",
    "    \n",
    "    Output: kappa score and average accuracy for (pairs of annotators) in the dataset\n",
    "\n",
    "    '''\n",
    "    worker_pairs = extract_pairs_of_workers(data, worker_id)\n",
    "    \n",
    "    worker_key_dict = extract_worker_sent_dict(data, worker_id, key_var)\n",
    "    \n",
    "    cols = ['hit_id','sent_noun','worker1_id','worker2_id','agreement']\n",
    "    ridit_df = pd.DataFrame(columns=cols)\n",
    "    kappas = []\n",
    "    accuracies = []\n",
    "    for (w1, w2) in worker_pairs:\n",
    "        \n",
    "        common_set = worker_key_dict[w1].intersection(worker_key_dict[w2])\n",
    "        \n",
    "        if common_set == set():\n",
    "            continue\n",
    "\n",
    "        for key in common_set:\n",
    "            val1 = data[(data[key_var] == key) & \n",
    "                        (data[worker_id] == w1)][check_var].values[0]\n",
    "            val2 = data[(data[key_var] == key) & \n",
    "                        (data[worker_id] == w2)][check_var].values[0]\n",
    "            val3 = data[(data[key_var] == key) & \n",
    "                        (data[worker_id] == w2)]['hit_id'].values[0]\n",
    "            agreement = (val1 == val2)\n",
    "            temp_row = {'hit_id': val3, 'sent_noun': key, 'worker1_id': w1, \n",
    "                        'worker2_id': w2, 'agreement': agreement}\n",
    "            ridit_df.append(temp_row, ignore_index=True)\n",
    "            reversed_temp_row = {'hit_id': val3, 'sent_noun': key, 'worker1_id': w2, \n",
    "                                 'worker2_id': w1, 'agreement': agreement}\n",
    "            ridit_df.append(reversed_temp_row, ignore_index=True)\n",
    "        \n",
    "    return ridit_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The flipping happens here\n",
    "# ridit_df = ridit_dataframe(data,'worker_id','sent_noun','part')\n",
    "# ridit_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enables the %%R magic \n",
    "%load_ext rpy2.ipython\n",
    "%R require(ggplot2); require(tidyr); require(lme4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Mixed effects model in R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i data -o df_part\n",
    "\n",
    "#Convert to factors\n",
    "data$part = as.factor(data$part)\n",
    "data$kind = as.factor(data$kind)\n",
    "data$abs = as.factor(data$abs)\n",
    "\n",
    "#Mixed Effects Model\n",
    "model = glmer(part ~ 1 + (1|worker_id) + (1|sent_noun) + (1|hit_id), data=data,  family=binomial)\n",
    "\n",
    "#Model intercepts:\n",
    "df_part = ranef(model)$worker_id\n",
    "colnames(df_part) <- c('intercept')\n",
    "\n",
    "df_part$glmer_intercept_part = df_part$intercept #constant added manually\n",
    "df_part$worker_id <- rownames(df_part)\n",
    "print(summary(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_part['glmer_intercept_part'] = df_part['glmer_intercept_part'].apply(lambda x: 1/(1+np.exp(-x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_part.glmer_intercept_part.plot(kind='density')\n",
    "plt.title(\"Annotator probability density of saying part = true\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i data -o df_kind\n",
    "\n",
    "#Convert to factors\n",
    "data$part = as.factor(data$part)\n",
    "data$kind = as.factor(data$kind)\n",
    "data$abs = as.factor(data$abs)\n",
    "\n",
    "#Mixed Effects Model\n",
    "model = glmer(kind ~ 1 + (1|worker_id) + (1|sent_noun) + (1|hit_id), data=data,  family=\"binomial\")\n",
    "\n",
    "#Model intercepts:\n",
    "df_kind = ranef(model)$worker_id\n",
    "colnames(df_kind) <- c('intercept')\n",
    "\n",
    "df_kind$glmer_intercept_kind = df_kind$intercept #constant added manually\n",
    "df_kind$worker_id <- rownames(df_kind)\n",
    "\n",
    "print(summary(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kind['glmer_intercept_kind'] = df_kind['glmer_intercept_kind'].apply(lambda x: 1/(1+np.exp(-x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kind.glmer_intercept_kind.plot(kind='density')\n",
    "plt.title(\"Annotator probability density of saying kind = true\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i data -o df_abs\n",
    "\n",
    "#Convert to factors\n",
    "data$part = as.factor(data$part)\n",
    "data$kind = as.factor(data$kind)\n",
    "data$abs = as.factor(data$abs)\n",
    "\n",
    "#Mixed Effects Model\n",
    "model = glmer(abs ~ 1 + (1|worker_id) + (1|sent_noun) + (1|hit_id), data=data,  family=\"binomial\")\n",
    "\n",
    "#Model intercepts:\n",
    "df_abs = ranef(model)$worker_id\n",
    "colnames(df_abs) <- c('intercept')\n",
    "\n",
    "df_abs$glmer_intercept_abs = df_abs$intercept #constant added manually\n",
    "df_abs$worker_id <- rownames(df_abs)\n",
    "\n",
    "print(summary(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_abs['glmer_intercept_abs'] = df_abs['glmer_intercept_abs'].apply(lambda x: 1/(1+np.exp(-x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_abs.glmer_intercept_abs.plot(kind='density')\n",
    "plt.title(\"Annotator probability density of saying abstract = true\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pairs_of_workers(data, worker_id):\n",
    "    '''\n",
    "    Given a pandas dataframe, and worker_id variable,\n",
    "    extracts a list of pairs of worker_ids\n",
    "    '''\n",
    "    workers = list(set(data[worker_id].values))\n",
    "    \n",
    "    return list(itertools.combinations(workers, 2))\n",
    "\n",
    "def extract_worker_sent_dict(data, worker_id, sent_id):\n",
    "    '''\n",
    "    Given a pandas dataframe, worker_id variable, and sentence_id variable,\n",
    "    extracts a dict where key is worker_id and value is set(sentences_ids annotated by that worker)\n",
    "    \n",
    "    '''\n",
    "    workers = list(set(data[worker_id].values))\n",
    "    \n",
    "    ans = {}\n",
    "    \n",
    "    for worker in workers:\n",
    "        sents = set(list(data[data[worker_id] == worker][sent_id].values))\n",
    "        ans[worker] = sents\n",
    "        \n",
    "    return ans\n",
    "\n",
    "def average_kappa_acc(data, worker_id, key_var, check_var):\n",
    "    '''\n",
    "    Input: 1. data: pandas dataframe\n",
    "           2. worker_id: Annotator id variable\n",
    "           3. key_var: level of the data (sentence-predicate id)\n",
    "           4. check_var: variable to be checked for kappa score\n",
    "    \n",
    "    Output: kappa score and average accuracy for (pairs of annotators) in the dataset\n",
    "\n",
    "    '''\n",
    "    worker_pairs = extract_pairs_of_workers(data, worker_id)\n",
    "    \n",
    "    worker_key_dict = extract_worker_sent_dict(data, worker_id, key_var)\n",
    "    if check_var == \"part\":\n",
    "        df = df_part\n",
    "        int_prob = \"glmer_intercept_part\"\n",
    "    elif check_var == \"abs\":\n",
    "        df = df_abs\n",
    "        int_prob = \"glmer_intercept_abs\"\n",
    "    else:\n",
    "        df = df_kind\n",
    "        int_prob = \"glmer_intercept_kind\"\n",
    "\n",
    "    kappas = []\n",
    "    accuracies = []\n",
    "    for (w1, w2) in worker_pairs:\n",
    "        \n",
    "        common_set = worker_key_dict[w1].intersection(worker_key_dict[w2])\n",
    "        temp1 = []\n",
    "        temp2 = []\n",
    "        \n",
    "        if common_set == set():\n",
    "            continue\n",
    "\n",
    "        for key in common_set:\n",
    "            val1 = data[(data[key_var] == key) & \n",
    "                        (data[worker_id] == w1)][check_var].values\n",
    "            val2 = data[(data[key_var] == key) & \n",
    "                        (data[worker_id] == w2)][check_var].values\n",
    "\n",
    "            temp1.append(val1[0])\n",
    "            temp2.append(val2[0])\n",
    "        accuracies.append(accuracy(temp1, temp2))\n",
    "        # Now for modified kappa calculation\n",
    "        p_e = (df[df[worker_id] == w1][int_prob][0] * df[df[worker_id] == w2][int_prob][0]) + ((1 - df[df[worker_id] == w1][int_prob][0]) * (1 - df[df[worker_id] == w2][int_prob][0]))\n",
    "        kappas.append((accuracies[-1] - p_e) / (1 - p_e))\n",
    "        \n",
    "    return kappas, accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappas = {}\n",
    "corrs = {}\n",
    "accs = {}\n",
    "variables = ['part', 'kind', 'abs']\n",
    "# variables_ord = ['part_conf', 'kind_conf', 'abs_conf']\n",
    "for var in variables:\n",
    "    kappas[var], accs[var] = average_kappa_acc(data, 'worker_id', 'sent_noun', var)\n",
    "# for var in variables_ord:\n",
    "#     corrs[var], accs[var] = rank_correlation(data, 'worker_id', 'sent_noun', var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa_data = pd.DataFrame.from_dict(kappas)\n",
    "acc_data = pd.DataFrame.from_dict(accs)\n",
    "corr_data = pd.DataFrame.from_dict(corrs)\n",
    "\n",
    "ax = sns.boxplot(data=kappa_data)\n",
    "ax.set(ylabel='Kappa Score', title=\"Kappa\")\n",
    "plt.show()\n",
    "\n",
    "ax = sns.boxplot(data=acc_data)\n",
    "ax.set(ylabel='Accuracy', title=\"Accuracy\")\n",
    "plt.show()\n",
    "print(np.mean(kappa_data))\n",
    "print(np.mean(acc_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
