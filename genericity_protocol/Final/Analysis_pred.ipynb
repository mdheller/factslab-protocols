{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "from nltk.metrics.agreement import AnnotationTask\n",
    "from sklearn.metrics import cohen_kappa_score as kappa\n",
    "from sklearn.metrics import accuracy_score as accuracy\n",
    "from scipy.stats import pearsonr as pearson\n",
    "from scipy.stats import spearmanr as spearman\n",
    "from math import isnan\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"pred_devtest.csv\"\n",
    "# data_file = \"pred_train.csv\"\n",
    "raw_data_file = pd.read_csv(data_file)\n",
    "raw_data_file.columns = [c.replace('.', '_') for c in raw_data_file.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dataframe(data):\n",
    "    '''\n",
    "    Input: Pandas csv dataframe obtained from MTurk\n",
    "    \n",
    "    Output: Pandas dataframe levelled by (User x Sentenced_ID)\n",
    "    '''\n",
    "    data[\"dicts\"] = data[\"Input_var_arrays\"].map(lambda x: json.loads(x))\n",
    "    global_list = []\n",
    "    \n",
    "    for row in data.itertuples():\n",
    "        for idx, local_dict in enumerate(row.dicts):\n",
    "            temp_dict = local_dict.copy()\n",
    "            var_dyn = \"Answer_pred_dyn\" + str(idx + 1)\n",
    "            var_dyn_c = \"Answer_dyn_conf\" + str(idx + 1)\n",
    "            var_part = \"Answer_pred_part\" + str(idx + 1)\n",
    "            var_part_c = \"Answer_part_conf\" + str(idx + 1)\n",
    "            var_hyp = \"Answer_pred_hyp\" + str(idx + 1)\n",
    "            var_hyp_c = \"Answer_hyp_conf\" + str(idx + 1)\n",
    "            temp_dict['part'] = getattr(row, var_part)\n",
    "            temp_dict['part_conf'] = getattr(row, var_part_c)\n",
    "            temp_dict['dyn'] = getattr(row, var_dyn)\n",
    "            temp_dict['dyn_conf'] = getattr(row, var_dyn_c)\n",
    "            temp_dict['hyp'] = getattr(row, var_hyp)\n",
    "            temp_dict['hyp_conf'] = getattr(row, var_hyp_c)\n",
    "            temp_dict['worker_id'] = row.WorkerId\n",
    "            temp_dict['hit_id'] = row.HITId\n",
    "            temp_dict['status'] = row.AssignmentStatus\n",
    "            global_list.append(temp_dict)\n",
    "    \n",
    "    return pd.DataFrame(global_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A2UF2FRGVW4T89', 290), ('A3OPHHRV96Y2UH', 280), ('A215S35FQFQYJ1', 260), ('A1JLQNPXN3NHVP', 260), ('A2HHOH92TCP7WA', 260), ('A14OPFM8OFA4WF', 250), ('A30GPAEVFFIAIW', 250), ('A3MQ8BS6EYO2WW', 250), ('A30UE6DWFNUCWX', 250), ('A1U5YW7RKHBOFP', 250), ('A2BVVBUTQ4AFH1', 250), ('A1BPXI87NVUA0R', 250), ('A3M01W8KKZF99X', 250), ('AOUZBLGN7NVT0', 250), ('AN4D1WRTKLUYZ', 250)]\n",
      "(16920, 11)\n"
     ]
    }
   ],
   "source": [
    "raw_data = extract_dataframe(raw_data_file)\n",
    "raw_data = raw_data[raw_data['status']!='Rejected']\n",
    "raw_data['sent_pred'] = raw_data['sent_id'].map(lambda x : x) + \"_\" +\\\n",
    "                           raw_data['pred_token'].map(lambda x: str(x))\n",
    "# Rearrange the columns\n",
    "cols = ['hit_id', 'worker_id','sent_pred','sent_id','pred_token','part','part_conf',\n",
    "        'dyn','dyn_conf','hyp','hyp_conf']\n",
    "data = raw_data[cols]\n",
    "\n",
    "x=Counter(list(data['worker_id'].values))\n",
    "print(x.most_common()[:15])\n",
    "print(data.shape)\n",
    "# ann_data = data[data['worker_id']=='A323RBOHUTRW53']\n",
    "# xdata = data[data['worker_id']!='A323RBOHUTRW53']\n",
    "# print(ann_data.shape)\n",
    "# print(xdata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=2, nrows=6, figsize=(15, 15))\n",
    "sns.countplot(x='part', data=data, ax=axs[0][0])\n",
    "sns.countplot(x='part', data=ann_data, ax=axs[0][1])\n",
    "sns.countplot(x='dyn', data=data, ax=axs[1][0])\n",
    "sns.countplot(x='dyn', data=ann_data, ax=axs[1][1])\n",
    "sns.countplot(x='hyp', data=data, ax=axs[2][0])\n",
    "sns.countplot(x='hyp', data=ann_data, ax=axs[2][1])\n",
    "\n",
    "sns.countplot(x='part_conf', data=data, ax=axs[3][0])\n",
    "sns.countplot(x='part_conf', data=ann_data, ax=axs[3][1])\n",
    "sns.countplot(x='dyn_conf', data=data, ax=axs[4][0])\n",
    "sns.countplot(x='dyn_conf', data=ann_data, ax=axs[4][1])\n",
    "sns.countplot(x='hyp_conf', data=data, ax=axs[5][0])\n",
    "sns.countplot(x='hyp_conf', data=ann_data, ax=axs[5][1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "print(data.pivot_table(index=['hyp', 'part', 'dyn'], \n",
    "                                  columns='dyn_conf', \n",
    "                                  values='worker_id', aggfunc=len))\n",
    "sns.heatmap(data.pivot_table(index=['hyp', 'part', 'dyn'], \n",
    "                                  columns='dyn_conf', \n",
    "                                  values='worker_id', aggfunc=len).fillna(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=2, nrows=3, figsize=(15, 10))\n",
    "sns.countplot(x='part', data=data, ax=axs[0][0])\n",
    "sns.countplot(x='part_conf', data=data, ax=axs[0][1])\n",
    "sns.countplot(x='dyn', data=data, ax=axs[1][0])\n",
    "sns.countplot(x='dyn_conf', data=data, ax=axs[1][1])\n",
    "sns.countplot(x='hyp', data=data, ax=axs[2][0])\n",
    "sns.countplot(x='hyp_conf', data=data, ax=axs[2][1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inter Annotator agreement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_raw_agreement(data, key_var, check_var):\n",
    "    '''\n",
    "    Input: \n",
    "    1. data: Pandas dataframe\n",
    "    2. key_var: variable based on which raw agreement is to be calculated\n",
    "    3. check_var: vaiable on which raw agreement is calculated\n",
    "    \n",
    "    '''\n",
    "    print(\"####### Raw Count for {} ###########\".format(check_var))\n",
    "    ids = set(list(data[key_var].values))\n",
    "\n",
    "    total_count = len(ids)\n",
    "    raw_count = 0\n",
    "    keys = []\n",
    "    \n",
    "    for iden in ids:\n",
    "        temp = list(data[data[key_var] == iden][check_var].values)\n",
    "        if temp.count(temp[0]) == len(temp):\n",
    "            raw_count += 1\n",
    "            keys.append(iden)\n",
    "     \n",
    "    agreement = (raw_count/total_count)*100\n",
    "    \n",
    "    print(\"Total count of unique {} is {}\".format(key_var, total_count))\n",
    "    print(\"Raw count of matched for {} is {}\".format(check_var, raw_count))\n",
    "    print(\"Inter-annotator agreement for {} is {}%\".format(check_var, agreement))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    return agreement, keys\n",
    "start_agreement, key_start = calc_raw_agreement(data, 'sent_pred', 'hyp')\n",
    "instant_agreement, key_inst = calc_raw_agreement(data, 'sent_pred', 'part')\n",
    "start_agreement, key_start = calc_raw_agreement(data, 'sent_pred', 'dyn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average of accuracy and kappa for each pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pairs_of_workers(data, worker_id):\n",
    "    '''\n",
    "    Given a pandas dataframe, and worker_id variable,\n",
    "    extracts a list of pairs of worker_ids\n",
    "    '''\n",
    "    workers = list(set(data[worker_id].values))\n",
    "    \n",
    "    return list(itertools.combinations(workers, 2))\n",
    "\n",
    "def extract_worker_sent_dict(data, worker_id, sent_id):\n",
    "    '''\n",
    "    Given a pandas dataframe, worker_id variable, and sentence_id variable,\n",
    "    extracts a dict where key is worker_id and value is set(sentences_ids annotated by that worker)\n",
    "    \n",
    "    '''\n",
    "    workers = list(set(data[worker_id].values))\n",
    "    \n",
    "    ans = {}\n",
    "    \n",
    "    for worker in workers:\n",
    "        sents = set(list(data[data[worker_id] == worker][sent_id].values))\n",
    "        ans[worker] = sents\n",
    "        \n",
    "    return ans\n",
    "\n",
    "def average_kappa_acc(data, worker_id, key_var, check_var):\n",
    "    '''\n",
    "    Input: 1. data: pandas dataframe\n",
    "           2. worker_id: Annotator id variable\n",
    "           3. key_var: level of the data (sentence-predicate id)\n",
    "           4. check_var: variable to be checked for kappa score\n",
    "    \n",
    "    Output: kappa score and average accuracy for (pairs of annotators) in the dataset\n",
    "\n",
    "    '''\n",
    "    worker_pairs = extract_pairs_of_workers(data, worker_id)\n",
    "    \n",
    "    worker_key_dict = extract_worker_sent_dict(data, worker_id, key_var)\n",
    "    \n",
    "    kappas = []\n",
    "    accuracies = []\n",
    "    lens = []\n",
    "    for (w1, w2) in worker_pairs:\n",
    "        \n",
    "        common_set = worker_key_dict[w1].intersection(worker_key_dict[w2])\n",
    "        temp1 = []\n",
    "        temp2 = []\n",
    "        \n",
    "        if common_set == set():\n",
    "            continue\n",
    "        if len(common_set) == 150:\n",
    "            print(w1, w2)\n",
    "        for key in common_set:\n",
    "            val1 = data[(data[key_var] == key) & \n",
    "                        (data[worker_id] == w1)][check_var].values\n",
    "            val2 = data[(data[key_var] == key) & \n",
    "                        (data[worker_id] == w2)][check_var].values\n",
    "\n",
    "            temp1.append(val1[0])\n",
    "            temp2.append(val2[0])\n",
    "\n",
    "        kappas.append(kappa(temp1, temp2))\n",
    "        accuracies.append(accuracy(temp1, temp2))\n",
    "        lens.append(len(temp1))\n",
    "    return kappas, accuracies\n",
    "\n",
    "def rank_correlation(data, worker_id, key_var, check_var):\n",
    "    '''\n",
    "    Input: 1. data: pandas dataframe\n",
    "           2. worker_id: Annotator id variable\n",
    "           3. key_var: level of the data (sentence-predicate id)\n",
    "           4. check_var: variable to be checked for kappa score\n",
    "    \n",
    "    Output: pearson rank correlation\n",
    "\n",
    "    '''\n",
    "    worker_pairs = extract_pairs_of_workers(data, worker_id)\n",
    "    \n",
    "    worker_key_dict = extract_worker_sent_dict(data, worker_id, key_var)\n",
    "    \n",
    "    corrs = []\n",
    "    accuracies = []\n",
    "    for (w1, w2) in worker_pairs:\n",
    "        \n",
    "        common_set = worker_key_dict[w1].intersection(worker_key_dict[w2])\n",
    "        temp1 = []\n",
    "        temp2 = []\n",
    "        \n",
    "        if common_set == set():\n",
    "            continue\n",
    "\n",
    "        for key in common_set:\n",
    "            val1 = data[(data[key_var] == key) & \n",
    "                        (data[worker_id] == w1)][check_var].values\n",
    "            val2 = data[(data[key_var] == key) & \n",
    "                        (data[worker_id] == w2)][check_var].values\n",
    "\n",
    "            temp1.append(val1[0])\n",
    "            temp2.append(val2[0])\n",
    "\n",
    "        corrs.append(spearman(temp1, temp2)[0])\n",
    "        accuracies.append(accuracy(temp1, temp2))\n",
    "        \n",
    "    return corrs, accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "kappas = {}\n",
    "corrs = {}\n",
    "accs = {}\n",
    "variables = ['hyp', 'part', 'dyn']\n",
    "variables_ord = ['hyp_conf', 'part_conf', 'dyn_conf']\n",
    "for var in variables:\n",
    "    kappas[var], accs[var] = average_kappa_acc(data, 'worker_id', 'sent_pred', var)\n",
    "\n",
    "for var in variables_ord:\n",
    "    corrs[var], accs[var] = rank_correlation(data, 'worker_id', 'sent_pred', var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa_data = pd.DataFrame.from_dict(kappas)\n",
    "acc_data = pd.DataFrame.from_dict(accs)\n",
    "corr_data = pd.DataFrame.from_dict(corrs)\n",
    "\n",
    "ax = sns.boxplot(data=kappa_data)\n",
    "ax.set(ylabel='Kappa Score using builtin function', title=\"Kappa\")\n",
    "plt.show()\n",
    "print(np.mean(kappa_data))\n",
    "\n",
    "ax = sns.boxplot(data=acc_data)\n",
    "ax.set(ylabel='Accuracy(raw agreement)', title=\"Accuracy\")\n",
    "plt.show()\n",
    "print(np.mean(acc_data))\n",
    "\n",
    "ax = sns.boxplot(data=corr_data)\n",
    "ax.set(ylabel='Corr coeff', title=\"Pearsons/Spearman Rank Correlation\")\n",
    "plt.show()\n",
    "print(np.mean(corr_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run mixed effects model in R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enables the %%R magic \n",
    "%load_ext rpy2.ipython\n",
    "%R require(ggplot2); require(tidyr); require(lme4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i data -o df_hyp\n",
    "\n",
    "#Convert to factors\n",
    "data$part = as.factor(data$part)\n",
    "data$dyn = as.factor(data$dyn)\n",
    "data$hyp = as.factor(data$hyp)\n",
    "\n",
    "#Mixed Effects Model\n",
    "model = glmer(part ~ 1 + (1|worker_id) + (1|sent_pred) + (1|hit_id), data=data,  family=binomial)\n",
    "\n",
    "#Model intercepts:\n",
    "df_hyp = ranef(model)$worker_id\n",
    "colnames(df_hyp) <- c('intercept')\n",
    "\n",
    "df_hyp$glmer_intercept_hyp = df_hyp$intercept + 0.4795 #constant added manually\n",
    "df_hyp$worker_id <- rownames(df_hyp)\n",
    "print(summary(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hyp['glmer_intercept_hyp'] = df_hyp['glmer_intercept_hyp'].apply(lambda x: 1/(1+np.exp(-x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hyp.glmer_intercept_hyp.plot(kind='density')\n",
    "plt.title(\"Annotator probability density of saying part = true\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i data -o df_part\n",
    "\n",
    "#Convert to factors\n",
    "data$part = as.factor(data$part)\n",
    "data$dyn = as.factor(data$dyn)\n",
    "data$hyp = as.factor(data$hyp)\n",
    "\n",
    "#Mixed Effects Model\n",
    "model = glmer(part ~ 1 + (1|worker_id) + (1|sent_pred) + (1|hit_id), data=data,  family=binomial)\n",
    "\n",
    "#Model intercepts:\n",
    "df_part = ranef(model)$worker_id\n",
    "colnames(df_part) <- c('intercept')\n",
    "\n",
    "df_part$glmer_intercept_part = df_part$intercept + 0.4795 #constant added manually\n",
    "df_part$worker_id <- rownames(df_part)\n",
    "print(summary(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_part['glmer_intercept_part'] = df_part['glmer_intercept_part'].apply(lambda x: 1/(1+np.exp(-x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_part.glmer_intercept_part.plot(kind='density')\n",
    "plt.title(\"Annotator probability density of saying part = true\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i data -o df_dyn\n",
    "\n",
    "#Convert to factors\n",
    "data$part = as.factor(data$part)\n",
    "data$dyn = as.factor(data$dyn)\n",
    "data$hyp = as.factor(data$hyp)\n",
    "#Mixed Effects Model\n",
    "model = glmer(dyn ~ 1 + (1|worker_id) + (1|sent_pred) + (1|hit_id), data=data,  family=\"binomial\")\n",
    "\n",
    "#Model intercepts:\n",
    "df_dyn = ranef(model)$worker_id\n",
    "colnames(df_dyn) <- c('intercept')\n",
    "\n",
    "df_dyn$glmer_intercept_dyn = df_dyn$intercept - 0.2052 #constant added manually\n",
    "df_dyn$worker_id <- rownames(df_dyn)\n",
    "\n",
    "print(summary(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dyn['glmer_intercept_dyn'] = df_dyn['glmer_intercept_dyn'].apply(lambda x: 1/(1+np.exp(-x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dyn.glmer_intercept_dyn.plot(kind='density')\n",
    "plt.title(\"Annotator probability density of saying dynamic = true\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pairs_of_workers1(data, worker_id):\n",
    "    '''\n",
    "    Given a pandas dataframe, and worker_id variable,\n",
    "    extracts a list of pairs of worker_ids\n",
    "    '''\n",
    "    workers = list(set(data[worker_id].values))\n",
    "    \n",
    "    return list(itertools.combinations(workers, 2))\n",
    "\n",
    "def extract_worker_sent_dict1(data, worker_id, sent_id):\n",
    "    '''\n",
    "    Given a pandas dataframe, worker_id variable, and sentence_id variable,\n",
    "    extracts a dict where key is worker_id and value is set(sentences_ids annotated by that worker)\n",
    "    \n",
    "    '''\n",
    "    workers = list(set(data[worker_id].values))\n",
    "    \n",
    "    ans = {}\n",
    "    \n",
    "    for worker in workers:\n",
    "        sents = set(list(data[data[worker_id] == worker][sent_id].values))\n",
    "        ans[worker] = sents\n",
    "        \n",
    "    return ans\n",
    "\n",
    "def average_kappa_acc1(data, worker_id, key_var, check_var):\n",
    "    '''\n",
    "    Input: 1. data: pandas dataframe\n",
    "           2. worker_id: Annotator id variable\n",
    "           3. key_var: level of the data (sentence-predicate id)\n",
    "           4. check_var: variable to be checked for kappa score\n",
    "    \n",
    "    Output: kappa score and average accuracy for (pairs of annotators) in the dataset\n",
    "\n",
    "    '''\n",
    "    worker_pairs = extract_pairs_of_workers1(data, worker_id)\n",
    "    \n",
    "    worker_key_dict = extract_worker_sent_dict1(data, worker_id, key_var)\n",
    "    if check_var == \"part\":\n",
    "        df = df_part\n",
    "        int_prob = \"glmer_intercept_part\"\n",
    "    elif check_var == \"dyn\":\n",
    "        df = df_dyn\n",
    "        int_prob = \"glmer_intercept_dyn\"\n",
    "    else:\n",
    "        df = df_hyp\n",
    "        int_prob = \"glmer_intercept_hyp\"\n",
    "\n",
    "    kappas = []\n",
    "    accuracies = []\n",
    "    for (w1, w2) in worker_pairs:\n",
    "        \n",
    "        common_set = worker_key_dict[w1].intersection(worker_key_dict[w2])\n",
    "        temp1 = []\n",
    "        temp2 = []\n",
    "        \n",
    "        if common_set == set():\n",
    "            continue\n",
    "\n",
    "        for key in common_set:\n",
    "            val1 = data[(data[key_var] == key) & \n",
    "                        (data[worker_id] == w1)][check_var].values\n",
    "            val2 = data[(data[key_var] == key) & \n",
    "                        (data[worker_id] == w2)][check_var].values\n",
    "\n",
    "            temp1.append(val1[0])\n",
    "            temp2.append(val2[0])\n",
    "        accuracies.append(accuracy(temp1, temp2))\n",
    "        # Now for modified kappa calculation\n",
    "        p_e = (df[df[worker_id] == w1][int_prob][0] * df[df[worker_id] == w2][int_prob][0]) + ((1 - df[df[worker_id] == w1][int_prob][0]) * (1 - df[df[worker_id] == w2][int_prob][0]))\n",
    "        kappas.append((accuracies[-1] - p_e) / (1 - p_e))\n",
    "        \n",
    "    return kappas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappas = {}\n",
    "corrs = {}\n",
    "accs = {}\n",
    "variables = ['hyp', 'part', 'dyn']\n",
    "# variables_ord = ['part_conf', 'kind_conf', 'abs_conf']\n",
    "for var in variables:\n",
    "    kappas[var] = average_kappa_acc1(data, 'worker_id', 'sent_pred', var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa_data = pd.DataFrame.from_dict(kappas)\n",
    "\n",
    "ax = sns.boxplot(data=kappa_data)\n",
    "ax.set(ylabel='Kappa Score', title=\"Kappa from mixed effects mode\")\n",
    "plt.show()\n",
    "\n",
    "print(\"KAPPA\", np.mean(kappa_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if average confidence correlates with agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xyz = list(set(raw_data['sent_pred'].values))\n",
    "raw_data[raw_data['sent_pred'] == \"en-ud-train.conllu sent_5978_3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_list=[]\n",
    "\n",
    "for question in list(set(data.sent_pred)):\n",
    "    temp = {}\n",
    "    temp['sent_pred'] = question\n",
    "    temp['part_avg_confidence'] = np.mean(list(data[data[\"sent_pred\"] == question]['part_conf']))\n",
    "    temp['dyn_avg_confidence'] = np.mean(list(data[data[\"sent_pred\"] == question]['dyn_conf']))\n",
    "    p = sum(list(data[data[\"sent_pred\"] == question]['part'].astype(int))) / 5\n",
    "    q = sum(list(data[data[\"sent_pred\"] == question]['dyn'].astype(int))) / 5\n",
    "    temp['part_agreement'] = p * p + (1 - p) * (1 - p)\n",
    "    temp['dyn_agreement'] = q * q + (1 - q) * (1 - q)\n",
    "    all_list.append(temp)\n",
    "analyse_data = pd.DataFrame(all_list)\n",
    "print(spearman(list(analyse_data['dyn_agreement']), list(analyse_data['dyn_avg_confidence'])))\n",
    "print(spearman(list(analyse_data['part_agreement']), list(analyse_data['part_avg_confidence'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['raw_sentence', 'part', 'part_conf', 'dyn','dyn_conf','hyp','hyp_conf']\n",
    "# check_data = raw_data[cols]\n",
    "for sen in xyz:\n",
    "#     print(check_data[raw_data['sent_pred'] == sen])\n",
    "    print(str(set(raw_data[raw_data['sent_pred'] == sen]['raw_sentence'].values)), str(set(raw_data[raw_data['sent_pred'] == sen]['pred'].values)))\n",
    "    print(\"PART\", sum(list(raw_data[raw_data['sent_pred'] == sen]['part'].astype(int)))/5, \"CONF\", sum(list(raw_data[raw_data['sent_pred'] == sen]['part_conf'].values))/5, \"\\t\\t\",\n",
    "          \"HYP\", sum(list(raw_data[raw_data['sent_pred'] == sen]['hyp'].astype(int)))/5, \"CONF\", sum(list(raw_data[raw_data['sent_pred'] == sen]['hyp_conf'].values))/5, \"\\t\\t\"\n",
    "          \"DYN\", sum(list(raw_data[raw_data['sent_pred'] == sen]['dyn'].astype(int)))/5, \"CONF\", sum(list(raw_data[raw_data['sent_pred'] == sen]['dyn_conf'].values))/5,\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
